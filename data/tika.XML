<?xml version="1.0" encoding="utf-8"?>
<bugrepository name="tika">
	<bug fixdate="Tue, 14 Jan 2014 10:28:56 +0000" id="1" opendate="Tue, 5 Feb 2013 13:21:59 +0000">
		<buginformation>
			<summary>TikaCLI: invalid characters in embedded document name causes FNFE when trying to save</summary>
			<description>&lt;p&gt;Attached document hits this on Windows:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;C:\&amp;gt;java.exe -jar tika-app-1.3.jar -z -x c:\data\idit\T-DS_Excel2003-PPT2003_1.xls
Extracting 'file0.png' (image/png) to .\file0.png
Extracting 'file1.emf' (application/x-emf) to .\file1.emf
Extracting 'file2.jpg' (image/jpeg) to .\file2.jpg
Extracting 'file3.emf' (application/x-emf) to .\file3.emf
Extracting 'file4.wmf' (application/x-msmetafile) to .\file4.wmf
Extracting 'MBD0016BDE4/?£☺.bin' (application/octet-stream) to .\MBD0016BDE4\?£☺.bin
Exception in thread &quot;main&quot; org.apache.tika.exception.TikaException: TIKA-198: Illegal IOException from org.apache.tika.parser.microsoft.OfficeParser@75f875f8
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:248)
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)
        at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)
        at org.apache.tika.cli.TikaCLI$OutputType.process(TikaCLI.java:139)
        at org.apache.tika.cli.TikaCLI.process(TikaCLI.java:415)
        at org.apache.tika.cli.TikaCLI.main(TikaCLI.java:109)
Caused by: java.io.FileNotFoundException: .\MBD0016BDE4\?£☺.bin (The filename, directory name, or volume label syntax is incorrect.)
        at java.io.FileOutputStream.&amp;lt;init&amp;gt;(FileOutputStream.java:205)
        at java.io.FileOutputStream.&amp;lt;init&amp;gt;(FileOutputStream.java:156)
        at org.apache.tika.cli.TikaCLI$FileEmbeddedDocumentExtractor.parseEmbedded(TikaCLI.java:722)
        at org.apache.tika.parser.microsoft.AbstractPOIFSExtractor.handleEmbeddedOfficeDoc(AbstractPOIFSExtractor.java:201)
        at org.apache.tika.parser.microsoft.ExcelExtractor.parse(ExcelExtractor.java:158)
        at org.apache.tika.parser.microsoft.OfficeParser.parse(OfficeParser.java:194)
        at org.apache.tika.parser.microsoft.OfficeParser.parse(OfficeParser.java:161)
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)
        ... 5 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;TikaCLI manages to create the sub-directory, but because the embedded fileName has invalid (for Windows) characters, it fails.&lt;/p&gt;

&lt;p&gt;On Linux it runs fine.&lt;/p&gt;

&lt;p&gt;I think somehow ... we have to sanitize the embedded file name ...&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 11 Apr 2013 07:12:13 +0000" id="2" opendate="Thu, 11 Apr 2013 06:58:46 +0000">
		<buginformation>
			<summary>Upgrade to PDFBox 1.8.1</summary>
			<description>&lt;p&gt;The PDFBox project has just announced version 1.8.1 of the PDFBox library containing many fixes and improvements that will be useful to Tika. Therefore, it will be useful to upgrade once available in the Maven Repo.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 27 Dec 2013 03:32:30 +0000" id="3" opendate="Mon, 22 Apr 2013 20:33:28 +0000">
		<buginformation>
			<summary>Incorrectly declared SUPPORTED_TYPES in ChmParser.</summary>
			<description>&lt;p&gt;&lt;a href=&quot;http://www.iana.org/assignments/media-types/application/vnd.ms-htmlhelp&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;This link&lt;/a&gt; assigns the official mime type for these files to &quot;application/vnd.ms-htmlhelp&quot;. In the wild there are also two other types used:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;application/chm&lt;/li&gt;
	&lt;li&gt;application/x-chm&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;tika-mimetypes.xml uses the correct official mime type, but ChmParser declares that it supports only &quot;application/chm&quot;. For this reason content that uses the official mime type (e.g. coming via Detector or parsed using AutoDetectParser, or simply declared in metadata) fails to parse due to unknown mime type.&lt;/p&gt;

&lt;p&gt;The fix seems simple - ChmParser should declare also all of the above types in its SUPPORTED_TYPES.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/chm/ChmParser.java</file>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/chm/CHM2XHTML.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 26 May 2013 11:36:56 +0000" id="4" opendate="Fri, 24 May 2013 13:24:05 +0000">
		<buginformation>
			<summary>text/html procuder for tika-server</summary>
			<description>&lt;p&gt;the /tika resource handler of tika-server can only produce text/plain. This patch adds support for producing text/html.&lt;/p&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-server/src/main/java/org/apache/tika/server/TikaResource.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 19 Jul 2013 13:54:43 +0000" id="5" opendate="Thu, 11 Jul 2013 11:00:20 +0000">
		<buginformation>
			<summary>MagicDetector don't work for all RFC882 message Types </summary>
			<description>&lt;p&gt;I am trying to use Tika to extract metadata from eml's created via Novell Groupwise. By this I ran into  a problem with the dedection of &quot;message/rfc822&quot;. The MagicDetector (working with the default tika-mimetypes.xml) compares the &quot;match&quot; values binary. RFC822 describes the header attributes are case independent (see &lt;a href=&quot;http://www.ietf.org/rfc/rfc0822.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.ietf.org/rfc/rfc0822.txt&lt;/a&gt; 3.4.7). So MIME-Version is the same than Mime-Version.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/detect/MagicDetector.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 17 Jul 2013 22:08:47 +0000" id="6" opendate="Wed, 17 Jul 2013 20:31:12 +0000">
		<buginformation>
			<summary>File-Based TikaInputStreams are Deleted by ExternalEmbedder.embed</summary>
			<description>&lt;p&gt;When an application using Tika passes &lt;tt&gt;InputStream&lt;/tt&gt; objects to &lt;tt&gt;ExternalEmbedder.embed&lt;/tt&gt; the stream is usually read into a temporary file which is then deleted after embedding takes place.&lt;/p&gt;

&lt;p&gt;However, if the application passes in a file-based &lt;tt&gt;TikaInputStream&lt;/tt&gt; the embedder ends up dealing with directly with the original source file, which is then deleted after embedding takes place.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/embedder/ExternalEmbedder.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 27 Dec 2013 03:48:19 +0000" id="7" opendate="Tue, 23 Jul 2013 13:45:11 +0000">
		<buginformation>
			<summary>Process loops infinitely on parsing of a CHM file</summary>
			<description>&lt;p&gt;By parsing &lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12593696/12593696_eventcombmt.chm&quot; title=&quot;eventcombmt.chm attached to TIKA-1152&quot;&gt;the attachment CHM file&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt; (MS Microsoft Help Files), Java process stuck.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;[main,5,main]

	org.apache.tika.parser.chm.lzx.ChmLzxBlock.extractContent(ChmLzxBlock.java:203)
	org.apache.tika.parser.chm.lzx.ChmLzxBlock.&amp;lt;init&amp;gt;(ChmLzxBlock.java:77)
	org.apache.tika.parser.chm.core.ChmExtractor.extractChmEntry(ChmExtractor.java:338)
	org.apache.tika.parser.chm.CHMDocumentInformation.getContent(CHMDocumentInformation.java:72)
	org.apache.tika.parser.chm.CHMDocumentInformation.getText(CHMDocumentInformation.java:141)
	org.apache.tika.parser.chm.CHM2XHTML.process(CHM2XHTML.java:34)
	org.apache.tika.parser.chm.ChmParser.parse(ChmParser.java:51)
	org.apache.tika.parser.ParserDecorator.parse(ParserDecorator.java:91)
	org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)
	org.apache.tika.parser.AbstractParser.parse(AbstractParser.java:53)
	com.polyspot.document.converter.DocumentConverter.realizeConversion(DocumentConverter.java:192)
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/chm/lzx/ChmLzxBlock.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 27 Apr 2018 19:24:19 +0000" id="8" opendate="Tue, 23 Jul 2013 14:06:47 +0000">
		<buginformation>
			<summary>Upgrade pdfbox to latest 1.8.2 version</summary>
			<description>&lt;p&gt;Current version is 1.8.1&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 5 Aug 2013 11:55:43 +0000" id="9" opendate="Fri, 2 Aug 2013 13:58:10 +0000">
		<buginformation>
			<summary>AMR file not correctly detected</summary>
			<description>&lt;p&gt;Attached file is not correctly detected as an AMR file.&lt;/p&gt;

&lt;p&gt;Expected result:&lt;br/&gt;
$ java -jar tika-app-1.4.jar -d test.amr&lt;br/&gt;
audio/amr&lt;/p&gt;

&lt;p&gt;Actual result:&lt;br/&gt;
$ java -jar tika-app-1.4.jar -d test.amr&lt;br/&gt;
application/octet-stream&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 13 Aug 2013 22:25:19 +0000" id="10" opendate="Tue, 13 Aug 2013 19:39:09 +0000">
		<buginformation>
			<summary>Incorrect Mime Type Detected for Adobe InDesign Files</summary>
			<description>&lt;p&gt;Tika detects &quot;application/octet-stream&quot; mime type for Adobe InDesign files. I resolved this via creating the following entry in custom-mimetypes.xml :&lt;/p&gt;

&lt;p&gt;&amp;lt;mime-type type=&quot;application/x-adobe-indesign&quot;&amp;gt;&lt;br/&gt;
	&amp;lt;acronym&amp;gt;INDD&amp;lt;/acronym&amp;gt;&lt;br/&gt;
	&amp;lt;_comment&amp;gt;Adobe InDesign document&amp;lt;/_comment&amp;gt;&lt;br/&gt;
	&amp;lt;glob pattern=&quot;*.indd&quot; /&amp;gt;&lt;br/&gt;
	&amp;lt;magic priority=&quot;50&quot;&amp;gt;&lt;br/&gt;
		&amp;lt;match value=&quot;0x0606edf5d81d46e5bd31efe7fe74b71d&quot; type=&quot;string&quot;&lt;br/&gt; offset=&quot;0&quot; /&amp;gt;&lt;br/&gt;
	&amp;lt;/magic&amp;gt;&lt;br/&gt;
&amp;lt;/mime-type&amp;gt;&lt;/p&gt;

&lt;p&gt;Can we include this magic number pattern in the main tika-mimetypes.xml?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 28 Dec 2013 01:39:18 +0000" id="11" opendate="Wed, 14 Aug 2013 09:15:12 +0000">
		<buginformation>
			<summary>Add support for SolidWorks files</summary>
			<description>&lt;p&gt;It would be an advantage if the mime type for SolidWorks files could be detected by tika. File extensions include *slddrw, *sldasm, *.sldasm.&lt;br/&gt;
Standard properties are store in office alike format.&lt;br/&gt;
Custom properties are not detected.&lt;br/&gt;
I will include a custom-mimetypes.xml&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/test/java/org/apache/tika/TikaDetectionTest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 28 Aug 2013 18:34:01 +0000" id="12" opendate="Wed, 21 Aug 2013 16:43:27 +0000">
		<buginformation>
			<summary>FLVParser NullPointerException</summary>
			<description>&lt;p&gt;On certain video files, the FLV parser throws an NPE on line 242.&lt;/p&gt;

&lt;p&gt;The piece of code causing this is the following:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/tika/blob/1.4/tika-parsers/src/main/java/org/apache/tika/parser/video/FLVParser.java#L242&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/tika/blob/1.4/tika-parsers/src/main/java/org/apache/tika/parser/video/FLVParser.java#L242&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;241: for (Entry&amp;lt;String, Object&amp;gt; entry : extractedMetadata.entrySet()) {
242:   metadata.set(entry.getKey(), entry.getValue().toString());
243: }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt; 

&lt;p&gt;Which should probably be replaced by something like this:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;241: for (Entry&amp;lt;String, Object&amp;gt; entry : extractedMetadata.entrySet()) {
242:   if (entry.getValue() == null) continue;
243:   metadata.set(entry.getKey(), entry.getValue().toString());
244: }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt; 

&lt;p&gt;Exception trace :&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;[root@hermes backend]# java -jar bin/tika-app-1.1.jar -j ./data.mp4
Exception in thread &quot;main&quot; org.apache.tika.exception.TikaException: Unexpected RuntimeException from org.apache.tika.parser.video.FLVParser@58d9660d
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:244)
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)
        at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)
        at org.apache.tika.cli.TikaCLI$OutputType.process(TikaCLI.java:130)
        at org.apache.tika.cli.TikaCLI.process(TikaCLI.java:397)
        at org.apache.tika.cli.TikaCLI.main(TikaCLI.java:101)
Caused by: java.lang.NullPointerException
        at org.apache.tika.parser.video.FLVParser.parse(FLVParser.java:242)
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)
        ... 5 more
org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)
        at org.apache.tika.cli.TikaCLI$OutputType.process(TikaCLI.java:130)
        at org.apache.tika.cli.TikaCLI.process(TikaCLI.java:397)
        at org.apache.tika.cli.TikaCLI.main(TikaCLI.java:101)
Caused by: java.lang.NullPointerException
        at org.apache.tika.parser.video.FLVParser.parse(FLVParser.java:242)
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)
        ... 5 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt; </description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/video/FLVParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 3 Sep 2013 18:47:40 +0000" id="13" opendate="Mon, 2 Sep 2013 12:16:04 +0000">
		<buginformation>
			<summary>Insufficiently specific magic for binary image/cgm files</summary>
			<description>&lt;p&gt;I've been running Tika against a large corpus of web archives files, and I'm seeing a number of false positives for image/cgm. The Tika magic is&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;      &amp;lt;match value=&lt;span class=&quot;code-quote&quot;&gt;&quot;BEGMF&quot;&lt;/span&gt; type=&lt;span class=&quot;code-quote&quot;&gt;&quot;string&quot;&lt;/span&gt; offset=&lt;span class=&quot;code-quote&quot;&gt;&quot;0&quot;&lt;/span&gt;/&amp;gt;
      &amp;lt;match value=&lt;span class=&quot;code-quote&quot;&gt;&quot;0x0020&quot;&lt;/span&gt; mask=&lt;span class=&quot;code-quote&quot;&gt;&quot;0xffe0&quot;&lt;/span&gt; type=&lt;span class=&quot;code-quote&quot;&gt;&quot;string&quot;&lt;/span&gt; offset=&lt;span class=&quot;code-quote&quot;&gt;&quot;0&quot;&lt;/span&gt;/&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The issue seems to be that the second magic matcher is not very specific, e.g. matching files that start 0x002a. To be fair, this is only c.700 false matches out of &amp;gt;300 million resources, but it would be nice if this could be tightened up. &lt;/p&gt;

&lt;p&gt;Looking at the PRONOM signatures&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;&lt;a href=&quot;http://www.nationalarchives.gov.uk/PRONOM/Format/proFormatSearch.aspx?status=detailReport&amp;amp;id=1048&amp;amp;strPageToDisplay=signatures&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.nationalarchives.gov.uk/PRONOM/Format/proFormatSearch.aspx?status=detailReport&amp;amp;id=1048&amp;amp;strPageToDisplay=signatures&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;http://www.nationalarchives.gov.uk/PRONOM/Format/proFormatSearch.aspx?status=detailReport&amp;amp;id=1049&amp;amp;strPageToDisplay=signatures&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.nationalarchives.gov.uk/PRONOM/Format/proFormatSearch.aspx?status=detailReport&amp;amp;id=1049&amp;amp;strPageToDisplay=signatures&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;http://www.nationalarchives.gov.uk/PRONOM/Format/proFormatSearch.aspx?status=detailReport&amp;amp;id=1050&amp;amp;strPageToDisplay=signatures&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.nationalarchives.gov.uk/PRONOM/Format/proFormatSearch.aspx?status=detailReport&amp;amp;id=1050&amp;amp;strPageToDisplay=signatures&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;http://www.nationalarchives.gov.uk/PRONOM/Format/proFormatSearch.aspx?status=detailReport&amp;amp;id=1051&amp;amp;strPageToDisplay=signatures&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.nationalarchives.gov.uk/PRONOM/Format/proFormatSearch.aspx?status=detailReport&amp;amp;id=1051&amp;amp;strPageToDisplay=signatures&lt;/a&gt;&lt;br/&gt;
it seems we have a variable position marker that changes slightly for each version. Therefore, a more robust signature should be:&lt;/li&gt;
&lt;/ul&gt;


&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;      &amp;lt;match value=&lt;span class=&quot;code-quote&quot;&gt;&quot;BEGMF&quot;&lt;/span&gt; type=&lt;span class=&quot;code-quote&quot;&gt;&quot;string&quot;&lt;/span&gt; offset=&lt;span class=&quot;code-quote&quot;&gt;&quot;0&quot;&lt;/span&gt;/&amp;gt;
      &amp;lt;match value=&lt;span class=&quot;code-quote&quot;&gt;&quot;0x0020&quot;&lt;/span&gt; mask=&lt;span class=&quot;code-quote&quot;&gt;&quot;0xffe0&quot;&lt;/span&gt; type=&lt;span class=&quot;code-quote&quot;&gt;&quot;string&quot;&lt;/span&gt; offset=&lt;span class=&quot;code-quote&quot;&gt;&quot;0&quot;&lt;/span&gt;&amp;gt;
        &amp;lt;match value=&lt;span class=&quot;code-quote&quot;&gt;&quot;0x10220001&quot;&lt;/span&gt; type=&lt;span class=&quot;code-quote&quot;&gt;&quot;string&quot;&lt;/span&gt; offset=&lt;span class=&quot;code-quote&quot;&gt;&quot;2:64&quot;&lt;/span&gt;/&amp;gt;
        &amp;lt;match value=&lt;span class=&quot;code-quote&quot;&gt;&quot;0x10220002&quot;&lt;/span&gt; type=&lt;span class=&quot;code-quote&quot;&gt;&quot;string&quot;&lt;/span&gt; offset=&lt;span class=&quot;code-quote&quot;&gt;&quot;2:64&quot;&lt;/span&gt;/&amp;gt;
        &amp;lt;match value=&lt;span class=&quot;code-quote&quot;&gt;&quot;0x10220003&quot;&lt;/span&gt; type=&lt;span class=&quot;code-quote&quot;&gt;&quot;string&quot;&lt;/span&gt; offset=&lt;span class=&quot;code-quote&quot;&gt;&quot;2:64&quot;&lt;/span&gt;/&amp;gt;
        &amp;lt;match value=&lt;span class=&quot;code-quote&quot;&gt;&quot;0x10220004&quot;&lt;/span&gt; type=&lt;span class=&quot;code-quote&quot;&gt;&quot;string&quot;&lt;/span&gt; offset=&lt;span class=&quot;code-quote&quot;&gt;&quot;2:64&quot;&lt;/span&gt;/&amp;gt;
      &amp;lt;/match&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Where I have assumed the filename part of the CGM file will be less that 64 characters long.&lt;/p&gt;

&lt;p&gt;Could this magic be considered for inclusion?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 27 Sep 2013 18:56:08 +0000" id="14" opendate="Thu, 12 Sep 2013 07:04:53 +0000">
		<buginformation>
			<summary>Invalid characters in text extracted from *.ppt files</summary>
			<description>&lt;p&gt;Since tika 1.3 in text extracted from *.ppt files some unwanted asterisks occurs. I'm attaching simple sample project that reproduces that bug.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/HSLFExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 8 May 2014 15:34:29 +0000" id="15" opendate="Tue, 1 Oct 2013 09:44:36 +0000">
		<buginformation>
			<summary>MS Money files wrongly detected as True Type Font</summary>
			<description>&lt;p&gt;TTF magic is probably not specific enough, because it incorrectly detect MS Money files as TTF files, and then the parsing generates an Exception.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Caused by: ! java.io.IOException: head is mandatory&lt;br/&gt;
! at org.apache.fontbox.ttf.AbstractTTFParser.parseTables(AbstractTTFParser.java:107) &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Here is the magic detection code that I added to &lt;tt&gt;custom-mimetypes.xml&lt;/tt&gt;, and solves it:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-xml&quot;&gt;&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;mime-info&amp;gt;&lt;/span&gt;
	&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;mime-type type=&lt;span class=&quot;code-quote&quot;&gt;&quot;application/x-msmoney&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;
		&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;glob pattern=&lt;span class=&quot;code-quote&quot;&gt;&quot;*.mny&quot;&lt;/span&gt; /&amp;gt;&lt;/span&gt;
		&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;magic priority=&lt;span class=&quot;code-quote&quot;&gt;&quot;60&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;
			&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;match value=&lt;span class=&quot;code-quote&quot;&gt;&quot;0x000100004D534953414D204461746162617365&quot;&lt;/span&gt; type=&lt;span class=&quot;code-quote&quot;&gt;&quot;string&quot;&lt;/span&gt; offset=&lt;span class=&quot;code-quote&quot;&gt;&quot;0&quot;&lt;/span&gt; /&amp;gt;&lt;/span&gt;
		&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/magic&amp;gt;&lt;/span&gt;
	&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/mime-type&amp;gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;It can replace the existing &lt;tt&gt;application/x-msmoney&lt;/tt&gt; empty mime-type in &lt;tt&gt;tika-mimetypes.xml&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;magic comes from&lt;br/&gt;
&lt;a href=&quot;http://filesignatures.net/index.php?search=mny&amp;amp;mode=EXT&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://filesignatures.net/index.php?search=mny&amp;amp;mode=EXT&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 4 Oct 2013 19:03:13 +0000" id="16" opendate="Wed, 2 Oct 2013 15:47:13 +0000">
		<buginformation>
			<summary>Add Matroska (mkv, mka) format detection</summary>
			<description>&lt;p&gt;There's no mimetype detection for Matroska format, although it's a popular video format.&lt;br/&gt;
Here is some code I added in my custom mimetypes to detect them:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;	&amp;lt;mime-type type=&lt;span class=&quot;code-quote&quot;&gt;&quot;video/x-matroska&quot;&lt;/span&gt;&amp;gt;
		&amp;lt;glob pattern=&lt;span class=&quot;code-quote&quot;&gt;&quot;*.mkv&quot;&lt;/span&gt; /&amp;gt;
		&amp;lt;magic priority=&lt;span class=&quot;code-quote&quot;&gt;&quot;40&quot;&lt;/span&gt;&amp;gt;
			&amp;lt;match value=&lt;span class=&quot;code-quote&quot;&gt;&quot;0x1A45DFA3934282886d6174726f736b61&quot;&lt;/span&gt; type=&lt;span class=&quot;code-quote&quot;&gt;&quot;string&quot;&lt;/span&gt; offset=&lt;span class=&quot;code-quote&quot;&gt;&quot;0&quot;&lt;/span&gt; /&amp;gt;
		&amp;lt;/magic&amp;gt;
	&amp;lt;/mime-type&amp;gt;
	&amp;lt;mime-type type=&lt;span class=&quot;code-quote&quot;&gt;&quot;audio/x-matroska&quot;&lt;/span&gt;&amp;gt;
		&amp;lt;glob pattern=&lt;span class=&quot;code-quote&quot;&gt;&quot;*.mka&quot;&lt;/span&gt; /&amp;gt;
	&amp;lt;/mime-type&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;I found the signature for the mkv on: &lt;br/&gt;
&lt;a href=&quot;http://www.garykessler.net/library/file_sigs.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.garykessler.net/library/file_sigs.html&lt;/a&gt;&lt;br/&gt;
I was not able to find it clearly for mka, but detection by filename is still useful.&lt;/p&gt;

&lt;p&gt;Although, the full spec is available here:&lt;br/&gt;
&lt;a href=&quot;http://matroska.org/technical/specs/index.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://matroska.org/technical/specs/index.html&lt;/a&gt;&lt;br/&gt;
Maybe it's a bit more complex than this constant magic, but it works on my tests files.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 9 Nov 2013 11:29:04 +0000" id="17" opendate="Wed, 6 Nov 2013 21:31:41 +0000">
		<buginformation>
			<summary>ArrayIndexOutOfBoundsException: 9 parsing RTF</summary>
			<description>&lt;p&gt;When trying to parse an RTF file I'm getting the following exception. I am not able to attach the file for privacy reasons:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;java.lang.ArrayIndexOutOfBoundsException: 9
                           TextExtractor.java:872 org.apache.tika.parser.rtf.TextExtractor.processControlWord
                           TextExtractor.java:566 org.apache.tika.parser.rtf.TextExtractor.parseControlWord
                           TextExtractor.java:492 org.apache.tika.parser.rtf.TextExtractor.parseControlToken
                           TextExtractor.java:459 org.apache.tika.parser.rtf.TextExtractor.extract
                           TextExtractor.java:448 org.apache.tika.parser.rtf.TextExtractor.extract
                                RTFParser.java:56 org.apache.tika.parser.rtf.RTFParser.parse
                                 (Unknown Source) sun.reflect.NativeMethodAccessorImpl.invoke0
                 NativeMethodAccessorImpl.java:57 sun.reflect.NativeMethodAccessorImpl.invoke
             DelegatingMethodAccessorImpl.java:43 sun.reflect.DelegatingMethodAccessorImpl.invoke
                                  Method.java:606 java.lang.reflect.Method.invoke
                                Reflector.java:93 clojure.lang.Reflector.invokeMatchingMethod
                                Reflector.java:28 clojure.lang.Reflector.invokeInstanceMethod
                               tika_parser.clj:20 rtf-parser.tika-parser/parse
               form-init2921349737948661927.clj:1 rtf-parser.tika-parser/eval4200
                               Compiler.java:6619 clojure.lang.Compiler.eval
                               Compiler.java:6582 clojure.lang.Compiler.eval
                                    core.clj:2852 clojure.core/eval
                                     main.clj:259 clojure.main/repl[fn]
                                     main.clj:259 clojure.main/repl[fn]
                                     main.clj:277 clojure.main/repl[fn]
                                     main.clj:277 clojure.main/repl
                                 RestFn.java:1096 clojure.lang.RestFn.invoke
                        interruptible_eval.clj:56 clojure.tools.nrepl.middleware.interruptible-eval/evaluate[fn]
                                     AFn.java:159 clojure.lang.AFn.applyToHelper
                                     AFn.java:151 clojure.lang.AFn.applyTo
                                     core.clj:617 clojure.core/apply
                                    core.clj:1788 clojure.core/with-bindings*
                                  RestFn.java:425 clojure.lang.RestFn.invoke
                        interruptible_eval.clj:41 clojure.tools.nrepl.middleware.interruptible-eval/evaluate
                       interruptible_eval.clj:171 clojure.tools.nrepl.middleware.interruptible-eval/interruptible-eval[fn]
                                    core.clj:2330 clojure.core/comp[fn]
                       interruptible_eval.clj:138 clojure.tools.nrepl.middleware.interruptible-eval/run-next[fn]
                                      AFn.java:24 clojure.lang.AFn.run
                     ThreadPoolExecutor.java:1145 java.util.concurrent.ThreadPoolExecutor.runWorker
                      ThreadPoolExecutor.java:615 java.util.concurrent.ThreadPoolExecutor$Worker.run
                                  Thread.java:724 java.lang.Thread.run
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 28 Dec 2013 01:14:49 +0000" id="18" opendate="Mon, 11 Nov 2013 16:47:50 +0000">
		<buginformation>
			<summary>Allow access to HtmlParser's HtmlSchema</summary>
			<description>&lt;p&gt;TagSoup's HTMLSchema is not really well suited for HTML5 nor is it capable of correctly handling some very strange quirks, e.g. table inside anchors. By allowing access to the schema applications can modify the schema to suit their needs on the fly.&lt;/p&gt;

&lt;p&gt;This would also mean that we don't have to rely on &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-985&quot; title=&quot;Support for HTML5 elements&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-985&quot;&gt;TIKA-985&lt;/a&gt; getting committed, we can change it from our own applications.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/html/HtmlParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 19 Apr 2017 14:54:51 +0000" id="19" opendate="Thu, 14 Nov 2013 12:33:06 +0000">
		<buginformation>
			<summary>XLSB support</summary>
			<description>&lt;p&gt;We use Manifoldcf 1.3 and Solr 4.4 to index a shared network drive, works fine for most of our Office filetypes ( docx, xlsx,.... ) but we also have a lot of files with filetype xlsb which are not in the supported filetypes. &lt;br/&gt;
In order to keep using this solution it is essential to us that there will be a solution provided in the future&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 29 Jan 2014 18:13:44 +0000" id="20" opendate="Sat, 16 Nov 2013 14:27:25 +0000">
		<buginformation>
			<summary>JAX-RS server only responds to queries to/from http://localhost</summary>
			<description>&lt;p&gt;I'm not sure if this is a problem with the Tika JAX-RS server, or with how it uses CXF under the hood. Anyway:&lt;/p&gt;

&lt;p&gt;I have a large text extraction job (10-15 million documents) that I'm using the web service for. It would be nice to be able to distribute this horizontally across multiple nodes to speed up the processing. I had thought to have a job queue with a couple consumers, farming out PUT requests across several Tika web service endpoints.&lt;/p&gt;

&lt;p&gt;But the JAX-RS web service will only respond to queries made to &lt;tt&gt;&lt;a href=&quot;http://localhost:9998/tika&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://localhost:9998/tika&lt;/a&gt;&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;I can't call &lt;tt&gt;&lt;a href=&quot;http://hostname:9998/tika&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hostname:9998/tika&lt;/a&gt;&lt;/tt&gt; &amp;#8211; even if it's still a local operation.&lt;/p&gt;

&lt;p&gt;Here is a list of things I've tried:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;I changed line 89 of TikaServerCLI.java to compute the name of the host at runtime. No go: the server starts up, and immediately terminates.&lt;/li&gt;
	&lt;li&gt;I changed line 89 of TikaServerCLI.java to be a hostname (not a FQDN), and re-compiled:
	&lt;ul&gt;
		&lt;li&gt;&lt;tt&gt;mvn compile -rf :tika-server&lt;/tt&gt; compiles successfully. Start up the server, and it terminates, just like when I tried to compute the hostname at runtime&lt;/li&gt;
		&lt;li&gt;&lt;tt&gt;mvn install&lt;/tt&gt; from the topmost Tika directory gets the service responding to both &lt;tt&gt;&lt;a href=&quot;http://hostname:9998/tika&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hostname:9998/tika&lt;/a&gt;&lt;/tt&gt; and &lt;tt&gt;&lt;a href=&quot;http://hostname.domain.net:9998/tika&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://hostname.domain.net:9998/tika&lt;/a&gt;&lt;/tt&gt; (Seemed weird, this is why I was thinking it was further up the chain in CXF?)&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;In a perfect world:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;The server should respond to any valid calls that make sense:
	&lt;ul&gt;
		&lt;li&gt;127.0.0.1&lt;/li&gt;
		&lt;li&gt;localhost&lt;/li&gt;
		&lt;li&gt;hostname&lt;/li&gt;
		&lt;li&gt;host.domain.tld&lt;/li&gt;
		&lt;li&gt;ip_address&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;A &lt;tt&gt;hostname&lt;/tt&gt; invocation parameter could be used to limit what the service responds to when it's started up. (A very optional, nice-to-have.)&lt;/li&gt;
&lt;/ol&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-server/src/main/java/org/apache/tika/server/TikaServerCli.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 2 Dec 2013 15:02:04 +0000" id="21" opendate="Mon, 2 Dec 2013 09:26:59 +0000">
		<buginformation>
			<summary>Upgrade pdfbox 1.8.3</summary>
			<description>&lt;p&gt;pdfbox just released new 1.8.3 version&lt;br/&gt;
&lt;a href=&quot;http://www.apache.org/dist/pdfbox/1.8.3/RELEASE-NOTES.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.apache.org/dist/pdfbox/1.8.3/RELEASE-NOTES.txt&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 3 Dec 2013 00:56:18 +0000" id="22" opendate="Mon, 2 Dec 2013 15:26:33 +0000">
		<buginformation>
			<summary>Add possibility for switching to pdfbox NonSequentialPDFParser</summary>
			<description>&lt;p&gt;As discussing, we can improve PDF extraction by 45% with this new NonSequentialPDFParser and fit more with PDF specification. This parser will be integrated by default in pdfbox 2.0.&lt;/p&gt;

&lt;p&gt;ref.: &lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/PDFBOX-1104&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/PDFBOX-1104&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;http://pdfbox.apache.org/ideas.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://pdfbox.apache.org/ideas.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We should provide an extended parser or parameter current PDFParser to call:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;PDDocument.loadNonSeq(file, scratchFile);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDFParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 19 Dec 2013 19:47:52 +0000" id="23" opendate="Fri, 13 Dec 2013 18:31:47 +0000">
		<buginformation>
			<summary>Upgrade Tika tests to JUnit 4.X</summary>
			<description>&lt;p&gt;Annotations in the 4.X API are much much cleaner.&lt;br/&gt;
The upgrade is usually trivial.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 27 Dec 2013 04:03:32 +0000" id="24" opendate="Sat, 14 Dec 2013 15:10:09 +0000">
		<buginformation>
			<summary>Address tika-parsers o.a.t.mime.TestMimeTypes TODO: Need a test flash file</summary>
			<description>&lt;p&gt;AFAICS the TODO relates to the absence of suitable .swf/.SWF files to use within the assertTypeByData(String st, String st) method.&lt;br/&gt;
Over in Nutch we currently run some tests and have available .swf files which can be used within Tika.&lt;br/&gt;
&lt;a href=&quot;https://svn.apache.org/repos/asf/nutch/branches/2.x/src/plugin/parse-swf/sample/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://svn.apache.org/repos/asf/nutch/branches/2.x/src/plugin/parse-swf/sample/&lt;/a&gt;  &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/mime/TestMimeTypes.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 13 Jan 2014 14:50:13 +0000" id="25" opendate="Wed, 8 Jan 2014 23:37:46 +0000">
		<buginformation>
			<summary>Integrate with Java-7 FileTypeDetector API</summary>
			<description>&lt;p&gt;It would be useful if Tika natively provided Java-7 FileTypeDetector &lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; implementations. Adding the corresponding META-INF/services/java.nio.file.spi.FileTypeDetector files would allow the use of Files.probeContentType &lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; without any specific links to Tika for this functionality.&lt;/p&gt;

&lt;p&gt;If you do not want to rely on Java-7 for the core, then this could be added as an extension module.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; &lt;a href=&quot;http://docs.oracle.com/javase/7/docs/api/java/nio/file/spi/FileTypeDetector.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://docs.oracle.com/javase/7/docs/api/java/nio/file/spi/FileTypeDetector.html&lt;/a&gt;&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; &lt;a href=&quot;http://docs.oracle.com/javase/7/docs/api/java/nio/file/Files.html#probeContentType(java.nio.file.Path&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://docs.oracle.com/javase/7/docs/api/java/nio/file/Files.html#probeContentType(java.nio.file.Path&lt;/a&gt;)&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 8 May 2014 16:43:02 +0000" id="26" opendate="Wed, 15 Jan 2014 14:54:02 +0000">
		<buginformation>
			<summary>XPS detection</summary>
			<description>&lt;p&gt;Tika now detect xps files using only file extension.&lt;br/&gt;
Please modify XPS definition in tika-mimetypes.xml in the following way:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;	&amp;lt;mime-type type=&quot;application/vnd.ms-xpsdocument&quot;&amp;gt;
		&amp;lt;glob pattern=&quot;*.xps&quot;/&amp;gt;
		&amp;lt;sub-class-of type=&quot;application/zip&quot; /&amp;gt;
		&amp;lt;magic priority=&quot;50&quot;&amp;gt;
			&amp;lt;match value=&quot;PK\003\004&quot; type=&quot;string&quot; offset=&quot;0&quot;&amp;gt;
				&amp;lt;match value=&quot;Metadata/Job_PT.xml&quot; type=&quot;string&quot; offset=&quot;30&quot; /&amp;gt;
			&amp;lt;/match&amp;gt;
		&amp;lt;/magic&amp;gt;
	&amp;lt;/mime-type&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Thank you,&lt;br/&gt;
Marco&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 17 Feb 2014 12:17:16 +0000" id="27" opendate="Fri, 17 Jan 2014 14:02:16 +0000">
		<buginformation>
			<summary>Extract thumbnail of OOXML Office files</summary>
			<description>&lt;p&gt;From Microsoft Office 2007 file formats, thumbnail could be included in package. We can extract this embedded thumbnail for OOXML files.&lt;/p&gt;

&lt;p&gt;As discussed in mailing list, we should extract thumbnail as a attachment, not as metadata (&lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-90&quot; title=&quot;Allow thumbnails as document metadata&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-90&quot;&gt;TIKA-90&lt;/a&gt;).&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;embeddedRelationId format is thumbnail_{i}.{extension}.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/AbstractOOXMLExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 3 Feb 2014 20:14:34 +0000" id="28" opendate="Mon, 3 Feb 2014 15:34:30 +0000">
		<buginformation>
			<summary>Embedded files not extracted properly from PDF</summary>
			<description>&lt;p&gt;IAW pdfbox example here:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://svn.apache.org/repos/asf/pdfbox/trunk/examples/src/main/java/org/apache/pdfbox/examples/pdmodel/ExtractEmbeddedFiles.java&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://svn.apache.org/repos/asf/pdfbox/trunk/examples/src/main/java/org/apache/pdfbox/examples/pdmodel/ExtractEmbeddedFiles.java&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;the PDF parser does not check for additional entries under Kids node when Names node does not exist.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDF2XHTML.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 4 Feb 2014 22:33:37 +0000" id="29" opendate="Tue, 4 Feb 2014 10:59:04 +0000">
		<buginformation>
			<summary>Hyperlink in .doc page header broken</summary>
			<description>&lt;p&gt;If you have a hyperlink to a webpage or mailto in the page header (german: Kopfzeile) of your .doc document the import is defaced like this:&lt;br/&gt;
 �HYPERLINK &quot;http://tw-systemhaus.de&quot; �&lt;a href=&quot;http://tw-systemhaus.de&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://tw-systemhaus.de&lt;/a&gt;�&lt;/p&gt;

&lt;p&gt;It's however not an issue in text.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/WordExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 26 Feb 2014 01:30:48 +0000" id="30" opendate="Tue, 25 Feb 2014 16:15:07 +0000">
		<buginformation>
			<summary>CharsetDetector.getReader method doesn't support empty/null declaredEncoding</summary>
			<description>&lt;p&gt;The javadoc of the CharsetDetector.getReader method states that the declaredEncoding parameter can be null or empty. However, providing such values throws an exception (IllegalArgumentException or IllegalCharsetNameException).&lt;br/&gt;
Either the javadoc should be updated to match this behavior, or the bug should be fixed (even if the method seems not to be used internally).&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/txt/CharsetDetector.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 6 Mar 2014 17:14:38 +0000" id="31" opendate="Mon, 3 Mar 2014 02:49:13 +0000">
		<buginformation>
			<summary>Tika is not indexing all authors of a PDF</summary>
			<description>&lt;p&gt;When submitting a PDF with this information in its XMP metadata:&lt;br/&gt;
...&lt;br/&gt;
      &amp;lt;dc:creator&amp;gt;&lt;br/&gt;
        &amp;lt;rdf:Bag&amp;gt;&lt;br/&gt;
          &amp;lt;rdf:li&amp;gt;Author 1&amp;lt;/rdf:li&amp;gt;&lt;br/&gt;
          &amp;lt;rdf:li&amp;gt;Author 2&amp;lt;/rdf:li&amp;gt;&lt;br/&gt;
        &amp;lt;/rdf:Bag&amp;gt;&lt;br/&gt;
      &amp;lt;/dc:creator&amp;gt;&lt;br/&gt;
...&lt;br/&gt;
Only the first one appears in the collection:&lt;br/&gt;
...&lt;br/&gt;
        &quot;author&quot;:&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;quot;Author 1&amp;quot;&amp;#93;&lt;/span&gt;,&lt;br/&gt;
        &quot;author_s&quot;:&quot;Author 1&quot;,&lt;br/&gt;
...&lt;/p&gt;

&lt;p&gt;In spite of having set the field to multiValued in the Solr schema:&lt;/p&gt;

&lt;p&gt;&amp;lt;field name=&quot;author&quot; type=&quot;text_general&quot; indexed=&quot;true&quot; stored=&quot;true&quot; multiValued=&quot;true&quot;/&amp;gt;&lt;/p&gt;

&lt;p&gt;Let me know if there's any further specific information I could provide.&lt;/p&gt;

&lt;p&gt;Thanks in advance! &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDFParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 6 Mar 2014 13:25:53 +0000" id="32" opendate="Thu, 6 Mar 2014 13:22:41 +0000">
		<buginformation>
			<summary>MS Word Filter out control characters on ouput</summary>
			<description>&lt;p&gt;Control characters present mostly in table of index and un-visualizable. We should filter out them.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/WordExtractor.java</file>
			<file>/tika-parsers/src/test/resources/test-documents/testControlCharacters.doc</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 17 Jul 2014 16:43:30 +0000" id="33" opendate="Fri, 16 Apr 2010 08:35:49 +0000">
		<buginformation>
			<summary>Generate list of supported and detected types automatically</summary>
			<description>&lt;p&gt;Currently we edit the list of supported types (&lt;a href=&quot;http://lucene.apache.org/tika/0.7/formats.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.apache.org/tika/0.7/formats.html&lt;/a&gt;) manually, which is bound to leave the list outdated and incomplete. It would be better if the list was automatically generated from the tika-mimetypes.xml file and the getSupportedTypes() response of the AutoDetectParser class.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 4 Apr 2014 09:24:28 +0000" id="34" opendate="Tue, 29 Mar 2011 21:36:07 +0000">
		<buginformation>
			<summary>Add support for Outlook PST</summary>
			<description>&lt;p&gt;Hello everyone,&lt;/p&gt;

&lt;p&gt;As you might know, Outlook stores its mails and other stuff in a single PST file. There's a relatively new Java library called java-libpst for reading Outlook PST files. It is licensed under the LGPL and available over here: &lt;a href=&quot;http://code.google.com/p/java-libpst/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://code.google.com/p/java-libpst/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I have tested the library on Outlook 2000 and Outlook 2003, with good results. It would be great if the library could be integrated into Tika.&lt;/p&gt;

&lt;p&gt;Best regards&lt;br/&gt;
Tran Nam Quang&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/mbox/OutlookPSTParser.java</file>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 24 Mar 2014 15:42:18 +0000" id="35" opendate="Mon, 22 Jul 2013 19:31:34 +0000">
		<buginformation>
			<summary>Maven Build Should Automatically Produce test-jar Artifacts</summary>
			<description>&lt;p&gt;The Maven build should be updated to produce test jar artifacts for appropriate sub-projects (see below) such that developers can extend test classes by adding the &lt;tt&gt;test-jar&lt;/tt&gt; artifact as a dependency, i.e.:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    &amp;lt;dependency&amp;gt;
      &amp;lt;groupId&amp;gt;org.apache.tika&amp;lt;/groupId&amp;gt;
      &amp;lt;artifactId&amp;gt;tika-parsers&amp;lt;/artifactId&amp;gt;
      &amp;lt;version&amp;gt;1.6-SNAPSHOT&amp;lt;/version&amp;gt;
      &amp;lt;type&amp;gt;test-jar&amp;lt;/type&amp;gt;
      &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;
    &amp;lt;/dependency&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following sub-projects contain tests that developers might want to extend and their corresponding &lt;tt&gt;pom.xml&lt;/tt&gt; should have the &lt;a href=&quot;http://maven.apache.org/guides/mini/guide-attached-tests.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;attached tests&lt;/a&gt; added:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;tika-app&lt;/li&gt;
	&lt;li&gt;tika-core&lt;/li&gt;
	&lt;li&gt;tika-parsers&lt;/li&gt;
	&lt;li&gt;tika-server&lt;/li&gt;
	&lt;li&gt;tika-xmp&lt;/li&gt;
&lt;/ul&gt;

			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 21 Feb 2015 06:05:16 +0000" id="36" opendate="Thu, 24 Oct 2013 14:47:21 +0000">
		<buginformation>
			<summary>Microsoft Project Support</summary>
			<description>&lt;p&gt;It would be good for Tika to support the Microsoft Project file format. Unfortunately, the only Java library for MPP is under the LGPL, so can't be used directly from within Tika.&lt;/p&gt;

&lt;p&gt;This issue is to track changes needed to Tika itself, in order to be able to support a Third Party Plugin for Microsoft Project files, based on the LGPL licensed MPXJ&lt;/p&gt;

&lt;p&gt;Note - details of the external plugin are available on the wiki, in &lt;a href=&quot;http://wiki.apache.org/tika/3rd%20party%20parser%20plugins&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://wiki.apache.org/tika/3rd%20party%20parser%20plugins&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/SummaryExtractor.java</file>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 9 Dec 2013 19:04:04 +0000" id="37" opendate="Tue, 3 Dec 2013 01:01:15 +0000">
		<buginformation>
			<summary>Refactor PDFParser to enable easier parameter setting</summary>
			<description>&lt;p&gt;It would be handy to be able to set PDFParser parameters (extractAnnotationText, etc) in a config file and via ParseContext.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDF2XHTML.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 3 Feb 2014 14:04:40 +0000" id="38" opendate="Mon, 20 Jan 2014 09:17:53 +0000">
		<buginformation>
			<summary>Adding Source code (Java, Groovy, C) parser</summary>
			<description>&lt;p&gt;We can parser some source code file formats:&lt;br/&gt;
text/x-java-source&lt;br/&gt;
text/x-groovy&lt;br/&gt;
text/x-c&lt;/p&gt;


&lt;p&gt;for HTML rendering from code, we can use jhightlight: &lt;a href=&quot;http://www.ohloh.net/p/jhighlight&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.ohloh.net/p/jhighlight&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 27 Jan 2014 13:27:21 +0000" id="39" opendate="Thu, 23 Jan 2014 10:59:57 +0000">
		<buginformation>
			<summary>PDFTextStripper fails while getting data of PDF form fields of type PDSignatureField</summary>
			<description>&lt;p&gt;I have a PDF document that contains a filled in form. Among the various fields of type text and radio button there are multiple fields for digital signatures. When I load this document into tika-app I get the following exception:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Caused by: java.lang.RuntimeException: Can't get signature as String, use getSignature() instead.
	at org.apache.pdfbox.pdmodel.interactive.form.PDSignatureField.getValue(PDSignatureField.java:131)
	at org.apache.tika.parser.pdf.PDF2XHTML.addFieldString(PDF2XHTML.java:467)
	at org.apache.tika.parser.pdf.PDF2XHTML.processAcroField(PDF2XHTML.java:425)
	at org.apache.tika.parser.pdf.PDF2XHTML.extractAcroForm(PDF2XHTML.java:411)
	at org.apache.tika.parser.pdf.PDF2XHTML.endDocument(PDF2XHTML.java:184)
	at org.apache.pdfbox.util.PDFTextStripper.writeText(PDFTextStripper.java:330)
	at org.apache.tika.parser.pdf.PDF2XHTML.process(PDF2XHTML.java:95)
	at org.apache.tika.parser.pdf.PDFParser.parse(PDFParser.java:143)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)
	... 43 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The problem seems to be that PDF2XHTML seems to expect that it can call getValue() on all PDField objects. According to the PDFBox 1.8.3 java doc this is not true for the sub class PDSignatureField:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://pdfbox.apache.org/docs/1.8.3/javadocs/org/apache/pdfbox/pdmodel/interactive/form/PDSignatureField.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://pdfbox.apache.org/docs/1.8.3/javadocs/org/apache/pdfbox/pdmodel/interactive/form/PDSignatureField.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The java doc says that getSignature() should be called instead. &lt;/p&gt;

&lt;p&gt;Assuming that the information inside the signature is not relevant for the extraction process and can be discarded the following patch helps:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Index: tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDF2XHTML.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP &amp;lt;+&amp;gt;UTF-8 ===================================================================
--- tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDF2XHTML.java	(revision 1560617)
+++ tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDF2XHTML.java	(revision )
@@ -40,6 +40,7 @@
 import org.apache.pdfbox.pdmodel.interactive.documentnavigation.outline.PDOutlineNode;
 import org.apache.pdfbox.pdmodel.interactive.form.PDAcroForm;
 import org.apache.pdfbox.pdmodel.interactive.form.PDField;
+import org.apache.pdfbox.pdmodel.interactive.form.PDSignatureField;
 import org.apache.pdfbox.util.PDFTextStripper;
 import org.apache.pdfbox.util.TextPosition;
 import org.apache.tika.exception.TikaException;
@@ -464,7 +465,9 @@
           }
           String value = &quot;&quot;;
           try {
+              if (!(field instanceof PDSignatureField)) {
-              value = field.getValue();
+                  value = field.getValue();
+              }
           } catch (IOException e) {
                //swallow
           }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 4 Feb 2014 15:13:41 +0000" id="40" opendate="Tue, 4 Feb 2014 15:10:09 +0000">
		<buginformation>
			<summary>Update PDFBox to v1.8.4</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 12 May 2014 15:15:14 +0000" id="41" opendate="Tue, 4 Feb 2014 20:19:08 +0000">
		<buginformation>
			<summary>Safely handle null embedded files in PDFs</summary>
			<description>&lt;p&gt;I filed a potential fix, unit test and test doc for this in &lt;a href=&quot;https://issues.apache.org/jira/browse/PDFBOX-1884&quot; title=&quot;Avoid NPE when encountering null PDComplexFileSpecification&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PDFBOX-1884&quot;&gt;&lt;del&gt;PDFBOX-1884&lt;/del&gt;&lt;/a&gt;.  We'll need to add one test for null in the Tika PDFParser to handle this change once it is fixed in PDFBox.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 6 Mar 2014 16:53:48 +0000" id="42" opendate="Wed, 5 Feb 2014 14:05:02 +0000">
		<buginformation>
			<summary>Add PDF version to PDFParser output</summary>
			<description>&lt;p&gt;I'd like to identify the PDF version of files, this is not currently reported by the PDFParser although the information is available via PDFBox.  I have attached a patch that adds the format version to the Metadata object.&lt;/p&gt;

&lt;p&gt;However, I am not familiar enough with the Tika source to know if an alternative metadata key should be used, or this new one added.&lt;/p&gt;

&lt;p&gt;Comments welcome.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDFParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 23 Jul 2015 01:09:44 +0000" id="43" opendate="Wed, 5 Feb 2014 17:07:00 +0000">
		<buginformation>
			<summary>PDFBox can throw StringIndexOutOfBoundsException on some dates</summary>
			<description>&lt;p&gt;PDFBOX's date parser can throw a StringIndexOutOfBoundsException if a date string for parsing is empty or contains only spaces.  A few of my test pdfs have this &quot;feature.&quot;&lt;/p&gt;

&lt;p&gt;Until &lt;a href=&quot;https://issues.apache.org/jira/browse/PDFBOX-1803&quot; title=&quot;StringIndexOutOfBound on DateConverter.toCalendar&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PDFBOX-1803&quot;&gt;&lt;del&gt;PDFBOX-1803&lt;/del&gt;&lt;/a&gt; is resolved, we can add an extra catch to prevent this from causing problems in TIKA&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;@@ -171,6 +171,9 @@
             addMetadata(metadata, TikaCoreProperties.CREATED, info.getCreationDate());
         } catch (IOException e) {
             // Invalid date format, just ignore
+        } catch (StringIndexOutOfBoundsException e){
+            //remove after PDFBOX-1883 is fixed
+            // Invalid date format, just ignore
         }
         try {
             Calendar modified = info.getModificationDate();
@@ -178,6 +181,9 @@
             addMetadata(metadata, TikaCoreProperties.MODIFIED, modified);
         } catch (IOException e) {
             // Invalid date format, just ignore
+        } catch (StringIndexOutOfBoundsException e){
+            //remove after PDFBOX-1883 is fixed
+            // Invalid date format, just ignore
         }

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDFParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 22 Feb 2014 16:17:51 +0000" id="44" opendate="Fri, 21 Feb 2014 14:59:40 +0000">
		<buginformation>
			<summary>Tika does not recognise empty nor spanning ZIP files magic</summary>
			<description>&lt;p&gt;As it turns out, magic differs for non-empty, empty and&lt;br/&gt;
spanning ZIP files. Tika recognizes only the non-empty ZIP files.&lt;/p&gt;

&lt;p&gt;Magic for empty ZIP file is validated with hexdump:&lt;br/&gt;
&lt;a href=&quot;https://gist.github.com/cstamas/6e90ae73f83c8e4a3f42&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://gist.github.com/cstamas/6e90ae73f83c8e4a3f42&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Also described on Wikipedia&lt;br/&gt;
&lt;a href=&quot;http://en.wikipedia.org/wiki/Zip_(file_format&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/Zip_(file_format&lt;/a&gt;)&lt;br/&gt;
(see sidebar with Magic Numbers)&lt;/p&gt;

&lt;p&gt;Proposed change:&lt;br/&gt;
add two more match entries to ZIP MIME definition:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/tika/pull/4&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/tika/pull/4&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 16 Mar 2015 02:25:42 +0000" id="45" opendate="Fri, 21 Feb 2014 21:49:21 +0000">
		<buginformation>
			<summary>Support for 7z archives</summary>
			<description>&lt;p&gt;I think upgrading to Apache Commons Compress 1.7 and adding &quot;application/x-7z-compressed&quot; to PackageParser supported types will fix this.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/resources/test-documents/test-documents.7z</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pkg/PackageParser.java</file>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 31 Mar 2014 11:59:03 +0000" id="46" opendate="Fri, 21 Feb 2014 23:13:56 +0000">
		<buginformation>
			<summary>Better parsing of Mbox files</summary>
			<description>&lt;p&gt;MboxParser currently looses metadata of all emails, except first. It does not extract/parse emails, nor decode parts. It should handle embedded emails like other container parsers do, so emails will be automatically parsed by RFC822Parser. I will try to add a patch for this.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/mbox/MboxParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 12 Sep 2016 21:44:53 +0000" id="47" opendate="Tue, 4 Mar 2014 14:54:33 +0000">
		<buginformation>
			<summary>WordExtractor - bold hyperlink not closed properly</summary>
			<description>&lt;p&gt;If a Word document contains a bold hyperlink, the resulting xhtml is:&lt;/p&gt;

&lt;p&gt;&amp;lt;a href=&quot;http://www.testdomain.com/support/workcentre-7232-7242/file-download/enus.html?operatingSystem=macosx108&amp;amp;amp;fileLanguage=en&amp;amp;amp;contentId=126220&amp;amp;amp;from=downloads&amp;amp;amp;viewArchived=false&quot;&amp;gt;&amp;lt;b&amp;gt;Test link&amp;lt;/a&amp;gt;&amp;lt;/b&amp;gt;&lt;/p&gt;

&lt;p&gt;The closing bold and anchor tags are transposed, which isn't valid XHTML.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/TIKA-2078</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 18 Mar 2014 13:08:30 +0000" id="48" opendate="Thu, 13 Mar 2014 09:11:50 +0000">
		<buginformation>
			<summary>More ogg based mime entries</summary>
			<description>&lt;p&gt;There are a few more ogg based formats for which we don't currently have mime entries or magic. Wikipedia is pretty good on giving the mime types and descriptions, while /liboggz/oggz_auto.h has the first packet identifier strings&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 21 Mar 2014 13:38:34 +0000" id="49" opendate="Thu, 20 Mar 2014 22:26:02 +0000">
		<buginformation>
			<summary>Atom feed failed to detect</summary>
			<description>&lt;p&gt;Atom feeds with namespace &lt;a href=&quot;http://www.w3.org/2005/Atom&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.w3.org/2005/Atom&lt;/a&gt; are not detected as &lt;tt&gt;application/atom+xml&lt;/tt&gt;. Trivial patch attached, sample feed taken from &lt;a href=&quot;http://en.wikipedia.org/wiki/Atom_%28standard%29&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;wikipedia&lt;/a&gt;.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 25 Mar 2014 16:38:44 +0000" id="50" opendate="Tue, 25 Mar 2014 14:13:07 +0000">
		<buginformation>
			<summary>Improve PST file detection</summary>
			<description>&lt;p&gt;Please update the PST mime-type definition to:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&amp;lt;mime-type type=&lt;span class=&quot;code-quote&quot;&gt;&quot;application/vnd.ms-outlook-pst&quot;&lt;/span&gt;&amp;gt;    
	&amp;lt;_comment&amp;gt;Outlook Personal Folders File Format&amp;lt;/_comment&amp;gt;
	&amp;lt;magic priority=&lt;span class=&quot;code-quote&quot;&gt;&quot;50&quot;&lt;/span&gt;&amp;gt;
		&amp;lt;match value=&lt;span class=&quot;code-quote&quot;&gt;&quot;!BDN....SM&quot;&lt;/span&gt; type=&lt;span class=&quot;code-quote&quot;&gt;&quot;string&quot;&lt;/span&gt; offset=&lt;span class=&quot;code-quote&quot;&gt;&quot;0&quot;&lt;/span&gt; mask=&lt;span class=&quot;code-quote&quot;&gt;&quot;0xFFFFFFFF00000000FFFF&quot;&lt;/span&gt;/&amp;gt;
	&amp;lt;/magic&amp;gt; 
	&amp;lt;glob pattern=&lt;span class=&quot;code-quote&quot;&gt;&quot;*.pst&quot;&lt;/span&gt;/&amp;gt;
	&amp;lt;glob pattern=&lt;span class=&quot;code-quote&quot;&gt;&quot;*.ost&quot;&lt;/span&gt;/&amp;gt;
	&amp;lt;sub-&lt;span class=&quot;code-keyword&quot;&gt;class-&lt;/span&gt;of type=&lt;span class=&quot;code-quote&quot;&gt;&quot;application/x-tika-msoffice&quot;&lt;/span&gt;/&amp;gt;   	
&amp;lt;/mime-type&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;File header description: &lt;a href=&quot;http://msdn.microsoft.com/en-us/library/ff387474(v=office.12).aspx&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://msdn.microsoft.com/en-us/library/ff387474(v=office.12).aspx&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 9 Apr 2014 21:54:57 +0000" id="51" opendate="Wed, 9 Apr 2014 21:09:16 +0000">
		<buginformation>
			<summary>Extract images from PDF documents</summary>
			<description>&lt;p&gt;It would be nice if images within PDF documents could be extracted much like embedded attachments are now being handled.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 11 Feb 2015 04:00:54 +0000" id="52" opendate="Wed, 9 Apr 2014 22:44:10 +0000">
		<buginformation>
			<summary>Self-hosted documentation for the JAX-RS Server</summary>
			<description>&lt;p&gt;Currently, if you fire up the JAX-RS Tika Server, and go to the root of the server in a web browser, you get an empty page back. You have to know to head over to &lt;a href=&quot;https://wiki.apache.org/tika/TikaJAXRS&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://wiki.apache.org/tika/TikaJAXRS&lt;/a&gt; find out what the available URLs are&lt;/p&gt;

&lt;p&gt;We should self-host some simple documentation on the server at the root of it, so that people can discover what it offers. Ideally, this should be largely auto-generated based on the endpoints, so that we don't risk missing things when we add new features&lt;/p&gt;

&lt;p&gt;This will also allow us to potentially offer a sample running version of the server for people to discover Tika with&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-server/pom.xml</file>
			<file>/tika-server/src/main/java/org/apache/tika/server/TikaServerCli.java</file>
			<file>/tika-server/src/main/java/org/apache/tika/server/TikaWelcome.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 7 May 2014 12:29:17 +0000" id="53" opendate="Wed, 9 Apr 2014 22:45:50 +0000">
		<buginformation>
			<summary>JAX-RS server should have endpoints which are like the &quot;--list-&lt;&gt;&quot; options to the CLI</summary>
			<description>&lt;p&gt;If you ask the Tika App CLI nicely, via the -&lt;del&gt;list&lt;/del&gt;&amp;lt;foo&amp;gt;, it'll tell you about the mimetypes it knows about, the parsers it supports, the detectors it supports etc&lt;/p&gt;

&lt;p&gt;The Tika Server should have a similar set of discovery endpoints&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-server/src/main/java/org/apache/tika/server/TikaParsers.java</file>
			<file>/tika-server/src/test/java/org/apache/tika/server/CXFTestBase.java</file>
			<file>/tika-server/src/main/java/org/apache/tika/server/CSVMessageBodyWriter.java</file>
			<file>/tika-server/src/main/java/org/apache/tika/server/TikaDetectors.java</file>
			<file>/tika-server/src/main/java/org/apache/tika/server/TikaMimeTypes.java</file>
			<file>/tika-server/src/main/java/org/apache/tika/server/HTMLHelper.java</file>
			<file>/tika-server/src/test/java/org/apache/tika/server/TikaDetectorsTest.java</file>
			<file>/tika-server/src/main/java/org/apache/tika/server/MetadataEP.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 11 Apr 2014 01:57:29 +0000" id="54" opendate="Thu, 10 Apr 2014 00:09:08 +0000">
		<buginformation>
			<summary>Move TrackingHandler into TikaTest and add a few other helper classes for embedded document tests</summary>
			<description>&lt;p&gt;This is a very minor refactoring to move TrackingHandler into TikaTest and add a few other classes to make testing embedded document extraction easier.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/TikaTest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 5 Aug 2014 13:17:00 +0000" id="55" opendate="Thu, 17 Apr 2014 06:25:34 +0000">
		<buginformation>
			<summary>Upgrade Commons compress to 1.8.1</summary>
			<description>&lt;p&gt;Hi,&lt;br/&gt;
I am using Tika to detect content also from archives. But because the raw input stream is a CipherInputStream I ran into &lt;a href=&quot;https://issues.apache.org/jira/browse/COMPRESS-277&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/COMPRESS-277&lt;/a&gt;&lt;br/&gt;
which compress kindly solved for me.&lt;br/&gt;
To be able to use Tika without patching my stack, I would like to see an upgrade of commons compress to 1.8.1 as soon as it is out.&lt;br/&gt;
This may, or may not be in 1.6 timeframe.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 24 Apr 2014 02:25:48 +0000" id="56" opendate="Wed, 23 Apr 2014 19:49:54 +0000">
		<buginformation>
			<summary>Magic bytes from Wikipedia</summary>
			<description>&lt;p&gt;Wikipedia has a (currently quite short) list of known byte signatures of various file formats (&lt;a href=&quot;http://en.wikipedia.org/wiki/List_of_file_signatures&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/List_of_file_signatures&lt;/a&gt;). It would be good to verify that Tika correctly recognizes all those formats.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 24 Apr 2014 13:28:52 +0000" id="57" opendate="Thu, 24 Apr 2014 10:40:20 +0000">
		<buginformation>
			<summary>Expose PDF Avg Char and Spacing Tolerance Config Params</summary>
			<description>&lt;p&gt;&lt;tt&gt;PDFParserConfig&lt;/tt&gt; should allow for override of PDFBox's &lt;tt&gt;averageCharTolerance&lt;/tt&gt; and &lt;tt&gt;spacingTolerance&lt;/tt&gt; settings as noted by a TODO comment in &lt;tt&gt;PDF2XHTML&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;Additionally, &lt;tt&gt;PDF2XHTML&lt;/tt&gt;'s use of &lt;tt&gt;PDFParserConfig&lt;/tt&gt; should be changed slightly to allow for extension of that config class and its configuration behavior.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDF2XHTML.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 24 Apr 2014 14:19:57 +0000" id="58" opendate="Thu, 24 Apr 2014 11:47:37 +0000">
		<buginformation>
			<summary>Missing return lines at output of SourceCodeParser</summary>
			<description>&lt;p&gt;xhtml output is on a single line.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/code/SourceCodeParserTest.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/code/SourceCodeParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 25 Apr 2014 09:21:20 +0000" id="59" opendate="Fri, 25 Apr 2014 09:03:52 +0000">
		<buginformation>
			<summary>GZip now has an official mimetype</summary>
			<description>&lt;p&gt;Since we added the gzip mimetype entry to Tika, an official one of application/gzip was added through RFC6713 &amp;lt;&lt;a href=&quot;https://tools.ietf.org/html/rfc6713&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://tools.ietf.org/html/rfc6713&lt;/a&gt;&amp;gt;. We should change to make that our default, and leave the old one as an alias&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 28 Apr 2014 15:10:08 +0000" id="60" opendate="Mon, 28 Apr 2014 14:54:03 +0000">
		<buginformation>
			<summary>Additional XML type: application/x-xml</summary>
			<description>&lt;p&gt;The following MediaType is not yet supported by Tika (not as a Media Type or an Alias): application/x-xml&lt;/p&gt;


&lt;p&gt;I am no Media-Type expert, but if someone here at Tika is, then I suggest looking into it and if he sees fit then add it to the Tika Registry.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 9 May 2014 16:31:47 +0000" id="61" opendate="Mon, 28 Apr 2014 15:03:37 +0000">
		<buginformation>
			<summary>Additional Gzip types: </summary>
			<description>&lt;p&gt;I found several GZip mime types (which were supported by our group till we began using Tika) which aren't listed in the Tika registry.&lt;/p&gt;


&lt;p&gt;Now, I am not sure if they are legit or not, and I think that a Tika member will be able to investigate and decide if they should enter as mime types or aliases to gzip.&lt;/p&gt;

&lt;p&gt;These are the types:&lt;br/&gt;
application/x-gunzip&lt;br/&gt;
application/gzipped&lt;br/&gt;
application/gzip-compressed&lt;br/&gt;
gzip/document&lt;/p&gt;



&lt;p&gt;They can be found listed here:&lt;br/&gt;
&lt;a href=&quot;http://mimeapplication.net/x-gunzip&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://mimeapplication.net/x-gunzip&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 11 Mar 2015 16:50:41 +0000" id="62" opendate="Wed, 30 Apr 2014 02:37:49 +0000">
		<buginformation>
			<summary>Adding MS Visio VSDX to mime-types detection</summary>
			<description>&lt;p&gt;Visio files under the Open Office XML (ooxml) format are not recognized by the mim-type detector and always returns the family mime-type instead: &lt;tt&gt;application/x-tika-ooxml&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;It turns out most Microsoft OOXML file formats are defined in the tika-mimetypes.xml, but not not Visio.  I have created the list for someone to add:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-xml&quot;&gt;  &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;mime-type type=&lt;span class=&quot;code-quote&quot;&gt;&quot;application/vnd.ms-visio.drawing.main+xml&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;_comment&amp;gt;&lt;/span&gt;Office Open XML Visio Drawing (macro-free)&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/_comment&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;glob pattern=&lt;span class=&quot;code-quote&quot;&gt;&quot;*.vsdx&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;sub-class-of type=&lt;span class=&quot;code-quote&quot;&gt;&quot;application/x-tika-ooxml&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/mime-type&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;mime-type type=&lt;span class=&quot;code-quote&quot;&gt;&quot;application/vnd.ms-visio.template.main+xml&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;_comment&amp;gt;&lt;/span&gt;Office Open XML Visio Template (macro-free)&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/_comment&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;glob pattern=&lt;span class=&quot;code-quote&quot;&gt;&quot;*.vstx&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;sub-class-of type=&lt;span class=&quot;code-quote&quot;&gt;&quot;application/x-tika-ooxml&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/mime-type&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;mime-type type=&lt;span class=&quot;code-quote&quot;&gt;&quot;application/vnd.ms-visio.stencil.main+xml&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;_comment&amp;gt;&lt;/span&gt;Office Open XML Visio Stencil (macro-free)&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/_comment&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;glob pattern=&lt;span class=&quot;code-quote&quot;&gt;&quot;*.vssx&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;sub-class-of type=&lt;span class=&quot;code-quote&quot;&gt;&quot;application/x-tika-ooxml&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/mime-type&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;mime-type type=&lt;span class=&quot;code-quote&quot;&gt;&quot;application/vnd.ms-visio.drawing.macroEnabled.main+xml&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;_comment&amp;gt;&lt;/span&gt;Office Open XML Visio Drawing (macro-enabled)&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/_comment&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;glob pattern=&lt;span class=&quot;code-quote&quot;&gt;&quot;*.vsdm&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;sub-class-of type=&lt;span class=&quot;code-quote&quot;&gt;&quot;application/x-tika-ooxml&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/mime-type&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;mime-type type=&lt;span class=&quot;code-quote&quot;&gt;&quot;application/vnd.ms-visio.template.macroEnabled.main+xml&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;_comment&amp;gt;&lt;/span&gt;Office Open XML Visio Template (macro-enabled)&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/_comment&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;glob pattern=&lt;span class=&quot;code-quote&quot;&gt;&quot;*.vstm&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;sub-class-of type=&lt;span class=&quot;code-quote&quot;&gt;&quot;application/x-tika-ooxml&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/mime-type&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;mime-type type=&lt;span class=&quot;code-quote&quot;&gt;&quot;application/vnd.ms-visio.stencil.macroEnabled.main+xml&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;_comment&amp;gt;&lt;/span&gt;Office Open XML Visio Stencil (macro-enabled)&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/_comment&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;glob pattern=&lt;span class=&quot;code-quote&quot;&gt;&quot;*.vssm&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;sub-class-of type=&lt;span class=&quot;code-quote&quot;&gt;&quot;application/x-tika-ooxml&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/mime-type&amp;gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/resources/test-documents/testVISIO.vsdm</file>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 6 May 2014 15:34:14 +0000" id="63" opendate="Fri, 2 May 2014 21:07:43 +0000">
		<buginformation>
			<summary>Upgrade to PDFBOX 1.8.5</summary>
			<description>&lt;p&gt;PDFBOX 1.8.5 has been released: &lt;a href=&quot;http://pdfbox.apache.org/downloads.html#recent&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://pdfbox.apache.org/downloads.html#recent&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We can update to this version, and eventually test &amp;amp; fix also &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1231&quot; title=&quot;Safely handle null embedded files in PDFs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1231&quot;&gt;&lt;del&gt;TIKA-1231&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 22 May 2014 16:08:21 +0000" id="64" opendate="Wed, 7 May 2014 10:06:38 +0000">
		<buginformation>
			<summary>Inconsistent priorities in bundled tika-mimetypes.xml</summary>
			<description>&lt;p&gt;It seems that mime-type priorities are a bit inconsistent in the tika-core bundled tika-mimetypes.xml&lt;/p&gt;

&lt;p&gt;Few examples:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;&lt;a href=&quot;https://github.com/apache/tika/blob/trunk/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml#L3497&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;application/zip&lt;/a&gt; vs &lt;a href=&quot;https://github.com/apache/tika/blob/trunk/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml#L3510&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;application/x-7z-compressed&lt;/a&gt;: both are similar &quot;containers&quot; archive formats (structured, having entries), having distinct file extensions (&quot;zip&quot; vs &quot;7z&quot; globs), still priorities are 40 and 50 respectively.&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://github.com/apache/tika/blob/trunk/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml#L3497&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;application/zip&lt;/a&gt; vs &lt;a href=&quot;https://github.com/apache/tika/blob/trunk/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml#L4713&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;text/html&lt;/a&gt;: not quite related MIME types, having same priority of 40. But ZIP files can be &quot;uncompressed&quot; (meaning entries are mostly &quot;concatenated&quot;, and their content, if plaintext, is readable). Hence, having an &quot;uncompressed&quot; ZIP (or any subclass like JAR) file that contains HTML files zipped up might/will be detected as HTML, which is wrong.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;And this is what happens in Nexus that uses Tika under the hud for &quot;content&quot; validation, basically using MIME magic detection provided by Tika Detector: the Java JAR &lt;tt&gt;com.intellij:annotations:7.0.3&lt;/tt&gt; (&lt;a href=&quot;http://repo1.maven.org/maven2/com/intellij/annotations/7.0.3/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;link&lt;/a&gt;) is being detected as &lt;tt&gt;text/html&lt;/tt&gt; instead of (expected) &lt;tt&gt;application/java-archive&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;Reason is following: the JAR file is zipped up in &quot;uncompressed&quot; zip format, and among few annotations it also contains one HTML file entry (the license I guess). Since both MIME types have same priority (40), I guess tika &quot;randomly&quot; chooses the &lt;tt&gt;text/html&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;Original Nexus issue&lt;br/&gt;
&lt;a href=&quot;https://issues.sonatype.org/browse/NEXUS-6560&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.sonatype.org/browse/NEXUS-6560&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;At Nexus issue there is a GH Pull Request that solves the problem for us (by raising &lt;tt&gt;application/zip&lt;/tt&gt; priority to 41.&lt;/p&gt;

&lt;p&gt;But by inspecting the bundled tike-mimetypes.xml we spotted other &amp;#8211; probably &amp;#8211; priority inconsistencies, like that of zip vs 7z mentioned above.&lt;/p&gt;

&lt;p&gt;Note: this happens when using tika-core solely on classpath and using it for MIME magic detection. Interestingly, when the tika-parsers (with it's all dependencies) are added to classpath, Tika will properly figure out that the artifact is &lt;tt&gt;application/java-archive&lt;/tt&gt;. Still, our use case in Nexus requires the MIME magic detection only, so we do not use tika-parsers, nor we would like to do so.&lt;/p&gt;

&lt;p&gt;Sample project to reproduce&lt;br/&gt;
&lt;a href=&quot;https://github.com/cstamas/tika-1292&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/cstamas/tika-1292&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/mime/MimeTypes.java</file>
			<file>/tika-parsers/src/test/resources/test-documents/testJAR_with_HTML.jar</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 27 May 2014 19:38:11 +0000" id="65" opendate="Mon, 12 May 2014 16:38:17 +0000">
		<buginformation>
			<summary>Add ability to turn off extraction of PDXObjectImages (TIKA-1268) from PDFs</summary>
			<description>&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1268&quot; title=&quot;Extract images from PDF documents&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1268&quot;&gt;&lt;del&gt;TIKA-1268&lt;/del&gt;&lt;/a&gt; added the capability to extract embedded images as regular embedded resources...a great feature!&lt;/p&gt;

&lt;p&gt;However, for some use cases, it might not be desirable to extract those types of embedded resources.  I see two ways of allowing the client to choose whether or not to extract those images:&lt;/p&gt;

&lt;p&gt;1) set a value in the metadata for the extracted images that identifies them as embedded PDXObjectImages vs regular image attachments.  The client can then choose not to process embedded resources with a given metadata value.&lt;/p&gt;

&lt;p&gt;2) allow the client to set a parameter in the PDFConfig object.&lt;/p&gt;

&lt;p&gt;My initial proposal is to go with option 2, and I'll attach a patch shortly.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 9 Jun 2014 14:47:32 +0000" id="66" opendate="Mon, 19 May 2014 11:21:45 +0000">
		<buginformation>
			<summary>Parsing Html page (not well formed) containing two title tags results in metadata (title) to be overwritten</summary>
			<description>&lt;p&gt;While crawling following web page, we came accross a strange issue where by title for page was not being extracted accurately:&lt;br/&gt;
&lt;a href=&quot;http://www.samsung.com/us/support/faq/FAQ00052677/61239/SM-C105AZWAATT&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.samsung.com/us/support/faq/FAQ00052677/61239/SM-C105AZWAATT&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This html page is not well formed and contains two title tags (one in head and one is body):&lt;br/&gt;
e.g. &quot;&amp;lt;html&amp;gt;&amp;lt;title&amp;gt;Simple Content&amp;lt;/title&amp;gt;&amp;lt;body&amp;gt;&amp;lt;h1&amp;gt;&amp;lt;/h1&amp;gt;&amp;lt;title&amp;gt;TitleToIgnore&amp;lt;/title&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;&quot;&lt;/p&gt;

&lt;p&gt;Now in this case a simple fix to htmlhandler could make sure that once title value has been set in metadata, it should not be overridden when another title tag is subsequently found.&lt;/p&gt;

&lt;p&gt;I am submitting fix for this issue as a path for review  (1.5) - hoping that this could be committed to latest?&lt;/p&gt;

&lt;p&gt;Can you please review and update kindly.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/html/HtmlHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 5 Jun 2014 01:48:54 +0000" id="67" opendate="Wed, 28 May 2014 14:39:42 +0000">
		<buginformation>
			<summary>Centralize JSON handling of Metadata</summary>
			<description>&lt;p&gt;When json was initially added to TIKA CLI (&lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-213&quot; title=&quot;JSON output from Tika CLI&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-213&quot;&gt;&lt;del&gt;TIKA-213&lt;/del&gt;&lt;/a&gt;), there was a recommendation to centralize JSON handling of Metadata, potentially putting it in core.  On a recent bug fix (&lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1291&quot; title=&quot;Invalid JSON output on CLI&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1291&quot;&gt;&lt;del&gt;TIKA-1291&lt;/del&gt;&lt;/a&gt;), the same recommendation was repeated especially noting that we now handle JSON/Metadata differently in CLI and server.&lt;/p&gt;

&lt;p&gt;Let's centralize JSON handling in core and use GSON.  We should add a serializer and a deserializer so that users don't have to reinvent that wheel.&lt;/p&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 29 May 2014 16:35:07 +0000" id="68" opendate="Thu, 29 May 2014 16:11:35 +0000">
		<buginformation>
			<summary>FDF files detection</summary>
			<description>&lt;p&gt;Please provide support for FDF magic detection.&lt;br/&gt;
FDF are Adobe Forms as described here: &lt;a href=&quot;http://www.adobe.com/devnet/acrobat/fdftoolkit.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.adobe.com/devnet/acrobat/fdftoolkit.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Magic mime looks like PDF:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-style: solid;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&amp;lt;mime-type type=&lt;span class=&quot;code-quote&quot;&gt;&quot;application/vnd.fdf&quot;&lt;/span&gt;&amp;gt;		
		&amp;lt;acronym&amp;gt;FDF&amp;lt;/acronym&amp;gt;
		&amp;lt;_comment&amp;gt;Forms Data Format&amp;lt;/_comment&amp;gt;
		&amp;lt;tika:link&amp;gt;http:&lt;span class=&quot;code-comment&quot;&gt;//en.wikipedia.org/wiki/Forms_Data_Format&amp;lt;/tika:link&amp;gt;
&lt;/span&gt;		&amp;lt;tika:link&amp;gt;http:&lt;span class=&quot;code-comment&quot;&gt;//www.adobe.com/devnet/acrobat/fdftoolkit.html&amp;lt;/tika:link&amp;gt;
&lt;/span&gt;		&amp;lt;tika:uti&amp;gt;com.adobe.fdf&amp;lt;/tika:uti&amp;gt;
		&amp;lt;magic priority=&lt;span class=&quot;code-quote&quot;&gt;&quot;50&quot;&lt;/span&gt;&amp;gt;
			&amp;lt;match value=&lt;span class=&quot;code-quote&quot;&gt;&quot;%FDF-&quot;&lt;/span&gt; type=&lt;span class=&quot;code-quote&quot;&gt;&quot;string&quot;&lt;/span&gt; offset=&lt;span class=&quot;code-quote&quot;&gt;&quot;0&quot;&lt;/span&gt; /&amp;gt;
		&amp;lt;/magic&amp;gt;
		&amp;lt;glob pattern=&lt;span class=&quot;code-quote&quot;&gt;&quot;*.fdf&quot;&lt;/span&gt; /&amp;gt;
	&amp;lt;/mime-type&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 4 Aug 2014 16:53:03 +0000" id="69" opendate="Tue, 3 Jun 2014 08:04:50 +0000">
		<buginformation>
			<summary>Tika does not read text from Headers, Cover Pages, and SDT components of DOCX documents</summary>
			<description>&lt;p&gt;Currently, Tika does not read text from Cover Pages and Tables Of Content of DOCX documents. Examples of documents are attached. &lt;/p&gt;

&lt;p&gt;To process documents, I used the standalone Tika-App utility, tika-app-1.5.jar. I tried both specifying files to be processed in the command line and selecting them from the utility menu.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/XWPFWordExtractorDecorator.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 20 Feb 2015 19:22:01 +0000" id="70" opendate="Thu, 5 Jun 2014 01:57:20 +0000">
		<buginformation>
			<summary>Improve exception reporting in JAX-RS server</summary>
			<description>&lt;p&gt;I'd like to use tika-server for &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1302&quot; title=&quot;Let&amp;#39;s run Tika against a large batch of docs nightly&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1302&quot;&gt;TIKA-1302&lt;/a&gt;.  As part of that, I'd like to record exception stacktraces per document.  I see two options: transmit the info back to the client (assuming a doc didn't bring the server down &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; ) along with the current error code or log the document id and stacktrace via the server.  Given my current design thoughts, I'd prefer the first option.&lt;/p&gt;

&lt;p&gt;Any objections or recommendations?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-server/src/main/java/org/apache/tika/server/TikaServerParseExceptionMapper.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 20 Mar 2015 20:49:16 +0000" id="71" opendate="Thu, 5 Jun 2014 23:11:10 +0000">
		<buginformation>
			<summary>Use a common path for the Tika Server unpacker resources</summary>
			<description>&lt;p&gt;Currently, the two different methods of the Tika Server unpacker endpoint don't share a common url prefix, which causes them to clash with the new welcome endpoint&lt;/p&gt;

&lt;p&gt;As discussed on the mailing list, we should change these two have a common prefix, so that the urls are then:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/unpack/
{id}&lt;br/&gt;
 * /unpack/all/{id}&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;After making the change, the changelog and release notes need to be updated for it, as it is a breaking change for the (handful of) users of the endpoint&lt;/p&gt;

&lt;p&gt;This will help with &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1269&quot; title=&quot;Self-hosted documentation for the JAX-RS Server&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1269&quot;&gt;&lt;del&gt;TIKA-1269&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 20 Mar 2015 20:49:16 +0000" id="72" opendate="Fri, 6 Jun 2014 13:11:52 +0000">
		<buginformation>
			<summary>MSI file detection</summary>
			<description>&lt;p&gt;Please remove *.msi extension from application/x-msdownload mime-type definition, incorrectly listed there, and add the following mime-type in tika-mimetypes.xml:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&amp;lt;mime-type type=&lt;span class=&quot;code-quote&quot;&gt;&quot;application/x-ms-installer&quot;&lt;/span&gt;&amp;gt;
    	&amp;lt;_comment&amp;gt;Microsoft Windows Installer&amp;lt;/_comment&amp;gt; 
    	&amp;lt;sub-&lt;span class=&quot;code-keyword&quot;&gt;class-&lt;/span&gt;of type=&lt;span class=&quot;code-quote&quot;&gt;&quot;application/x-tika-msoffice&quot;&lt;/span&gt;/&amp;gt;
    	&amp;lt;glob pattern=&lt;span class=&quot;code-quote&quot;&gt;&quot;*.msi&quot;&lt;/span&gt;/&amp;gt;
    	&amp;lt;glob pattern=&lt;span class=&quot;code-quote&quot;&gt;&quot;*.msp&quot;&lt;/span&gt;/&amp;gt;
    	&amp;lt;glob pattern=&lt;span class=&quot;code-quote&quot;&gt;&quot;*.mst&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;/mime-type&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 24 Jun 2014 01:23:24 +0000" id="73" opendate="Mon, 23 Jun 2014 16:26:09 +0000">
		<buginformation>
			<summary>Upgrade to PDFBox 1.8.6</summary>
			<description>&lt;p&gt;This is to track moving to PDFBox 1.8.6.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/pom.xml</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 24 Jun 2014 16:06:50 +0000" id="74" opendate="Mon, 23 Jun 2014 19:35:42 +0000">
		<buginformation>
			<summary>OpenDocumentParser doesn't correctly process metadata</summary>
			<description>&lt;p&gt;When using OpenDocumentParser, the metadata isn't set correctly. When using it to write an html file, the only metadata that it knows about is content type because it is set ahead of time.&lt;/p&gt;

&lt;p&gt;The problem is that when iterating over the zip contents, meta.xml isn't processed before content.xml. The metadata set on the parse object is correct after parse() returns, however the contents of the resulting html file is missing all of the metadata.&lt;/p&gt;

&lt;p&gt;Changing the code to be &lt;/p&gt;

&lt;p&gt;boolean parsedMetaData = false;&lt;br/&gt;
boolean delayLoadContent = false;&lt;br/&gt;
while (entry != null) {&lt;br/&gt;
...&lt;br/&gt;
} else if (entry.getName().equals(&quot;meta.xml&quot;)) {&lt;br/&gt;
                meta.parse(zip, new DefaultHandler(), metadata, context);&lt;br/&gt;
                parsedMetaData = true;&lt;/p&gt;

&lt;p&gt;                if (delayLoadContent) {&lt;br/&gt;
                    if (content instanceof OpenDocumentContentParser) &lt;/p&gt;
{
                        ((OpenDocumentContentParser) content).parseInternal(zip, handler, metadata, context);
                    } else {
                        // Foreign content parser was set:
                        content.parse(zip, handler, metadata, context);
                    }&lt;br/&gt;
                }&lt;br/&gt;
            } else if (entry.getName().endsWith(&quot;content.xml&quot;)) {&lt;br/&gt;
                if (!parsedMetaData) {
                    delayLoadContent = true;
                } else {&lt;br/&gt;
                    if (content instanceof OpenDocumentContentParser) {                        ((OpenDocumentContentParser) content).parseInternal(zip, handler, metadata, context);                    }
&lt;p&gt; else &lt;/p&gt;
{
                        // Foreign content parser was set:
                        content.parse(zip, handler, metadata, context);
                    }
&lt;p&gt;                }&lt;br/&gt;
            }&lt;/p&gt;

&lt;p&gt;works as expected.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/odf/OpenDocumentParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 21 Jul 2014 15:36:10 +0000" id="75" opendate="Thu, 26 Jun 2014 06:51:47 +0000">
		<buginformation>
			<summary>Add support for newer iWork file formats</summary>
			<description>&lt;p&gt;IWork 2013 uses a revised file format which replaces the xml files that hold the content by .iwa files (a binary format). This file format is becoming increasingly relevant as more and more people are using apple products. However, it does not appear to work with the current IWorkPackageParser (tested with several of the example .pages files one can get from the iCloud). &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/iwork/iwana/IWork13PackageParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 24 Jul 2014 09:51:02 +0000" id="76" opendate="Tue, 22 Jul 2014 21:09:27 +0000">
		<buginformation>
			<summary>AutoDetectParser extracts no text when SourceCodeParser is selected</summary>
			<description>&lt;p&gt;When using the AutoDetectParser in java code, and the SourceCodeParser is selected (i.e. java files), the handler gets no text:&lt;/p&gt;

&lt;p&gt;I have this test program:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; data = &lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;HelloWorld {}&quot;&lt;/span&gt;;
    ByteArrayInputStream bais = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ByteArrayInputStream(data.getBytes());
    Parser autoDetectParser = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; AutoDetectParser();
    BodyContentHandler bch = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; BodyContentHandler(50);
    ParseContext parseContext = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ParseContext();
    Metadata metadata = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Metadata();
    metadata.set(Metadata.CONTENT_TYPE, &lt;span class=&quot;code-quote&quot;&gt;&quot;text/x-java-source&quot;&lt;/span&gt;);
    &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
       autoDetectParser.parse(bais, bch, metadata, parseContext);
    } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (Exception e) {
       e.printStackTrace();
    }
    &lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.out.println(&lt;span class=&quot;code-quote&quot;&gt;&quot;Text extracted: &quot;&lt;/span&gt;+bch.toString())
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;It returns (using the SourceCodeParser): &lt;/p&gt;
    &lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt; &amp;gt; Text extracted: &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;But when I use this code:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; data = &lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;HelloWorld {}&quot;&lt;/span&gt;;
    ByteArrayInputStream bais = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ByteArrayInputStream(data.getBytes());
    Parser autoDetectParser = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; AutoDetectParser();
    BodyContentHandler bch = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; BodyContentHandler(50);
    ParseContext parseContext = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ParseContext();
    Metadata metadata = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Metadata();
    metadata.set(Metadata.CONTENT_TYPE, &lt;span class=&quot;code-quote&quot;&gt;&quot;text/plain&quot;&lt;/span&gt;);
    &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {  autoDetectParser.parse(bais, bch, metadata, parseContext);  } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (Exception e) {  e.printStackTrace();  }
    &lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.out.println(&lt;span class=&quot;code-quote&quot;&gt;&quot;Text extracted: &quot;&lt;/span&gt;+bch.toString())
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;





&lt;p&gt;The Text Parser is used and I get:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt; &amp;gt; Text extracted: &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;HelloWorld {} &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;I have also tested this command: &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&amp;gt; java -jar tika-app-1.5.jar -t D:\text.java
  (no text)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/code/SourceCodeParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 25 Jul 2014 19:16:01 +0000" id="77" opendate="Thu, 24 Jul 2014 12:55:37 +0000">
		<buginformation>
			<summary>Need to add code to look for OS-specific keys for embedded files within PDFs</summary>
			<description>&lt;p&gt;Embedded files in PDFs can be found by the general all purpose key we  currently use via PDFBox:  &quot;F&quot;.  However, embedded documents can also be stored under OS specific keys: &quot;DOS&quot;, &quot;Mac&quot; and &quot;Unix&quot;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lehmi&quot; class=&quot;user-hover&quot; rel=&quot;lehmi&quot;&gt;Andreas Lehmkühler&lt;/a&gt; confirmed on the PDFBox users &lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/pdfbox-users/201407.mbox/%3c1572548479.2099779.1406198761475.open-xchange@patina.store%3e&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;list&lt;/a&gt; that we might be missing embedded documents if we're not trying the OS specific keys as well.  As Andreas points out, according to the spec the OS specific keys shouldn't be used any more, but I think we should support extraction for them.&lt;/p&gt;

&lt;p&gt;My proposal is to pull all documents that are available by any of the four keys (well, via getEmbeddedFile&amp;lt;OS&amp;gt;() in PDFBox).  This has the downside of potentially extracting basically duplicate documents, but I'd prefer to err on the side of extracting everything.&lt;/p&gt;

&lt;p&gt;The code fix is trivial, and I'll try to commit it today.  However, it will take me a bit of time to generate a test file that stores files under the OS specific keys.  So, if anyone has an ASF-friendly file available or wants to take the task of generating one, please do.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDF2XHTML.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 25 Jul 2014 11:50:46 +0000" id="78" opendate="Thu, 24 Jul 2014 13:47:49 +0000">
		<buginformation>
			<summary>Decrease memory consumption when extracting images from PDFs</summary>
			<description>&lt;p&gt;This patch applies changes made in &lt;a href=&quot;https://issues.apache.org/jira/browse/PDFBOX-2101&quot; title=&quot;Surprising memory consumption when extracting images&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PDFBOX-2101&quot;&gt;&lt;del&gt;PDFBOX-2101&lt;/del&gt;&lt;/a&gt; to decrease memory consumption during extraction of embedded images.  This also applies the recommendation by &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tilman&quot; class=&quot;user-hover&quot; rel=&quot;tilman&quot;&gt;Tilman Hausherr&lt;/a&gt; on the PDFBox dev &lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/pdfbox-dev/201407.mbox/%3c53CFF0CE.9090507@t-online.de%3e&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;list &lt;/a&gt; to clear resources after handling each page.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDF2XHTML.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 25 Jul 2014 15:02:28 +0000" id="79" opendate="Thu, 24 Jul 2014 15:16:40 +0000">
		<buginformation>
			<summary>Improve embedded file name extraction in PDFParser</summary>
			<description>&lt;p&gt;When we extract embedded files from PDFs, we are currently using the key in the PDEmbeddedFilesNameTreeNode as the file name that we store as the value of Metadata.RESOURCE_NAME_KEY in the embedded document's  metadata.&lt;/p&gt;

&lt;p&gt;I think we should try to get the file name from PDComplexFileSpecification's getFilename() first.  If that is null, then we should fall back to the key value.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDF2XHTML.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 31 Jul 2014 18:32:56 +0000" id="80" opendate="Thu, 31 Jul 2014 16:59:47 +0000">
		<buginformation>
			<summary>Add Lingo24Translate implementation of Translate API</summary>
			<description>&lt;p&gt;Add an implementation of the Translation API that uses the Lingo24 Premium Machine Translate API.  Available now but with developer portal coming soon at &lt;a href=&quot;http://developer.lingo24.com&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://developer.lingo24.com&lt;/a&gt;.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-translate/src/main/java/org/apache/tika/language/translate/Lingo24Translator.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 2 Jul 2015 14:19:39 +0000" id="81" opendate="Wed, 20 Aug 2014 03:05:51 +0000">
		<buginformation>
			<summary>Extract Excel (xls, xlsx) headers and footers</summary>
			<description>&lt;p&gt;When I parser xls file,&lt;br/&gt;
the headers's and footers's content can not be parsed. &lt;br/&gt;
The xlsx file has the same problem.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 18 Jan 2015 22:46:02 +0000" id="82" opendate="Wed, 21 Nov 2012 12:56:42 +0000">
		<buginformation>
			<summary>Tika-server quits parsing of rfc-822 document prematurely when it encounters encrypted zip file as attachment.</summary>
			<description>&lt;p&gt;The Zip parser in tika-server does not allow passing in the password for decrypting the zip file and doesn't handle the unsupported feature gracefully. Problem happens when zip file is attached part of email document being parsed, and the parser gives up and throws an exception:&lt;/p&gt;

&lt;p&gt;WARNING: all: Unpacker failed&lt;br/&gt;
org.apache.tika.exception.TikaException: &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-198&quot; title=&quot;Better distinction between IOException and TikaException&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-198&quot;&gt;&lt;del&gt;TIKA-198&lt;/del&gt;&lt;/a&gt;: Illegal IOException from org.apache.tika.parser.pkg.PackageParser@10fcc945&lt;/p&gt;

&lt;p&gt;Caused by: org.apache.commons.compress.archivers.zip.UnsupportedZipFeatureException: unsupported feature encryption used in entry&lt;/p&gt;

&lt;p&gt;Instead of returning the successfully parsed components, Tika-server returns nothing. &lt;/p&gt;

&lt;p&gt;It would be better to return rest of the parsed document contents along with the untouched offending zip file in the archive that Tika-server returns as a result. Until the feature of zip file decrypting is added this would always return untouched zip file, and after it is implemented it should return the untouched zip file in the cases where wrong password was provided.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/mail/MailContentHandler.java</file>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/mail/RFC822ParserTest.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pkg/PackageParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 3 Feb 2015 19:24:01 +0000" id="83" opendate="Tue, 5 Aug 2014 12:58:13 +0000">
		<buginformation>
			<summary>Simplify TikeServerCli endpoint setup code</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/tika-server/src/main/java/org/apache/tika/server/TikaServerCli.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 31 Aug 2014 16:21:27 +0000" id="84" opendate="Fri, 29 Aug 2014 14:34:57 +0000">
		<buginformation>
			<summary>tika-app server leaking temporary files when converting Word97 (doc)</summary>
			<description>&lt;p&gt;When converting Word97 documents (*.doc), tika-server reproducibly leaves behind temporary files.&lt;/p&gt;

&lt;p&gt;Steps to reproduce:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Start &lt;tt&gt;tika-app-1.5.jar&lt;/tt&gt; in &lt;tt&gt;--server&lt;/tt&gt; mode&lt;/li&gt;
	&lt;li&gt;Send a &lt;tt&gt;*.doc&lt;/tt&gt; file to server for conversion&lt;/li&gt;
	&lt;li&gt;Stop tika-server using CTRL+C or &lt;tt&gt;kill -15&lt;/tt&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;For example:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;lukas@host:~&amp;gt; java -jar tika-app-1.5.jar -v --server --port 8077 --text

# ...

lukas@host:/tmp&amp;gt; ls -lah apache-tika-*
ls: cannot access apache-tika-*: No such file or directory
lukas@host:/tmp&amp;gt;
lukas@host:/tmp&amp;gt; netcat 127.0.0.1 8077 &amp;lt; simple_word97.doc
Simple Word-97 Document
Lorem Ipsum.
lukas@host:/tmp&amp;gt; ls -lah apache-tika-*
-rw-r--r-- 1 lukas users 22K 2014-08-29 15:48 apache-tika-2457738389388821864.tmp

# after conversion is done, tmp file handles are still open

lukas@host:/tmp&amp;gt; lsof | grep tika
java   29857   lukas   32r   REG   104,2  28628386  4571740 /home/lukas/tika-app-1.5.jar
java   29857   lukas   85r   REG   104,2     22528  8604717 /tmp/apache-tika-2457738389388821864.tmp
java   29857   lukas   86r   REG   104,2     22528  8604717 /tmp/apache-tika-2457738389388821864.tmp

# stop tika-server...

^C
lukas@host:~&amp;gt;

# ...

lukas@host:/tmp&amp;gt; lsof | grep tika
lukas@host:/tmp&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;No exceptions are thrown, and the plaintext is being extracted correctly from the document, but temporary files are still left behind every single time.&lt;/p&gt;

&lt;p&gt;This obviously is a major issue in a production environment when converting thousands of documents a day. Our temp directories are filling up rapidly, and we had to configure cron jobs to clean up after Tika on most of our production servers. I wasn't able to reproduce this issue using &lt;tt&gt;tika-app-1.5.jar&lt;/tt&gt; in non-server mode. However, booting up a JVM for every single conversion is just too slow.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 20 Mar 2015 22:08:10 +0000" id="85" opendate="Mon, 15 Sep 2014 19:20:21 +0000">
		<buginformation>
			<summary>Refactor Translator Exception Handling</summary>
			<description>&lt;p&gt;`Translator.translate()` currently throws `Exception`. We should make it more specific. The only real limitation comes from MicrosoftTranslator &amp;#8211; the library used throws `Exception`, but that shouldn't mean Tika does too.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 24 Nov 2014 09:08:51 +0000" id="86" opendate="Sun, 12 Oct 2014 19:50:06 +0000">
		<buginformation>
			<summary>CHM parser : wrong decompression of aligned blocks</summary>
			<description>&lt;p&gt;If an embedded file contains aligned blocks, the parser outputs chaotic text or empty text as to this file.&lt;/p&gt;

&lt;p&gt;I have fixed it myself, corrected decompressAlignedBlock() and its preparation methods. Mostly this bug is due to misusing main tree/align tree/length tree. And some tree is built wrong.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/sax/ToHTMLContentHandler.java</file>
			<file>/tika-core/src/main/resources/org/apache/tika/language/be.ngp</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/html/HtmlHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 3 Mar 2015 18:52:10 +0000" id="87" opendate="Wed, 26 Nov 2014 07:46:32 +0000">
		<buginformation>
			<summary>PDF Text extraction without permission</summary>
			<description>&lt;p&gt;In &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1442&quot; title=&quot;Upgrade to PDFBox 1.8.8&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1442&quot;&gt;&lt;del&gt;TIKA-1442&lt;/del&gt;&lt;/a&gt; text extraction from files like 717226.pdf that don't have text extraction permission works. The permissions in PDF files are only enforced by the application (i.e. PDFBox), i.e. the text information isn't stored separately in encrypted form. &lt;/p&gt;

&lt;p&gt;PDFBox ExtractText command line does throw an exception.&lt;br/&gt;
So I wonder why TIKA is able to extract text. Either TIKA or the PDFBox call used bypasses the permission checking.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/exception/AccessPermissionException.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 15 Dec 2014 06:27:45 +0000" id="88" opendate="Fri, 12 Dec 2014 12:51:21 +0000">
		<buginformation>
			<summary>Parser for BPG (Better Portable Graphics) format</summary>
			<description>&lt;p&gt;Following on from &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1491&quot; title=&quot;Identification of BPG (Better Portable Graphics) format&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1491&quot;&gt;&lt;del&gt;TIKA-1491&lt;/del&gt;&lt;/a&gt;, it would be good to also have a parser for BPG files as well. Likely this would pull out some very basic metadata from the header, then locate the EXIF and XMP blocks + hand those on for parsing&lt;/p&gt;

&lt;p&gt;There doesn't appear to be a suitable Java library yet, but based on reading the file format spec at &lt;a href=&quot;http://bellard.org/bpg/bpg_spec.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://bellard.org/bpg/bpg_spec.txt&lt;/a&gt; it doesn't look like a basic parser would be that much work!&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/image/BPGParser.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/metadata/Photoshop.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/io/EndianUtils.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 21 Apr 2015 14:04:19 +0000" id="89" opendate="Mon, 22 Dec 2014 00:43:07 +0000">
		<buginformation>
			<summary>Fix the disabled Tika Bundle OSGi related unit tests</summary>
			<description>&lt;p&gt;Currently, the unit tests for the Tika Bundle contain several bits like:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;@Ignore &lt;span class=&quot;code-comment&quot;&gt;// TODO Fix &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; test&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We should really fix these unit tests so they work, and re-enable them&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-bundle/src/test/java/org/apache/tika/bundle/BundleIT.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 24 Dec 2014 08:09:10 +0000" id="90" opendate="Tue, 23 Dec 2014 21:29:28 +0000">
		<buginformation>
			<summary>TestGDALParser fails if gdalinfo does not support FITS</summary>
			<description>&lt;p&gt;gdalinfo is used as external parser (see &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-605&quot; title=&quot;Tika GDAL parser&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-605&quot;&gt;&lt;del&gt;TIKA-605&lt;/del&gt;&lt;/a&gt;). The test testParseFITS fails if gdalinfo is compiled without support of FITS:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;testParseFITS(org.apache.tika.parser.gdal.TestGDALParser)  Time elapsed: 0.206 sec  &amp;lt;&amp;lt;&amp;lt; FAILURE!
java.lang.AssertionError
        at org.junit.Assert.fail(Assert.java:86)
        at org.junit.Assert.assertTrue(Assert.java:41)
        at org.junit.Assert.assertNotNull(Assert.java:621)
        at org.junit.Assert.assertNotNull(Assert.java:631)
        at org.apache.tika.parser.gdal.TestGDALParser.testParseFITS(TestGDALParser.java:153)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;On Ubuntu 14.04 gdalinfo does not seem to support FITS:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt; % gdalinfo tika-parsers/src/test/resources/test-documents/WFPC2u5780205r_c0fx.fits
 ERROR 4: `tika-parsers/src/test/resources/test-documents/WFPC2u5780205r_c0fx.fits' not recognised as a supported file format.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The test should check whether gdalinfo supports the format, and skip the test if not.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 4 Nov 2015 04:05:41 +0000" id="91" opendate="Wed, 7 Jan 2015 15:26:07 +0000">
		<buginformation>
			<summary>Under OSGi, ForkParser failes to send core parser classes like ExternalParser</summary>
			<description>&lt;p&gt;Under OSGi, if you try to use ForkParser with the Tesseract OCR parser, it will fail with:&lt;/p&gt;

&lt;p&gt;java.lang.NoClassDefFoundError: org/apache/tika/parser/external/ExternalParser&lt;br/&gt;
	at org.apache.tika.parser.ocr.TesseractOCRParser.hasTesseract(TesseractOCRParser.java:117)&lt;br/&gt;
	at org.apache.tika.parser.ocr.TesseractOCRParser.getSupportedTypes(TesseractOCRParser.java:91)&lt;br/&gt;
	at org.apache.tika.parser.CompositeParser.getParsers(CompositeParser.java:81)&lt;br/&gt;
	at org.apache.tika.parser.DefaultParser.getParsers(DefaultParser.java:95)&lt;br/&gt;
	at org.apache.tika.parser.CompositeParser.getParser(CompositeParser.java:209)&lt;br/&gt;
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:244)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:622)&lt;br/&gt;
	at org.apache.tika.fork.ForkServer.call(ForkServer.java:144)&lt;br/&gt;
	at org.apache.tika.fork.ForkServer.processRequests(ForkServer.java:124)&lt;br/&gt;
	at org.apache.tika.fork.ForkServer.main(ForkServer.java:69)&lt;br/&gt;
Caused by: java.lang.ClassNotFoundException: Unable to find class org.apache.tika.parser.external.ExternalParser&lt;br/&gt;
	at org.apache.tika.fork.ClassLoaderProxy.findClass(ClassLoaderProxy.java:117)&lt;br/&gt;
	at java.lang.ClassLoader.loadClass(ClassLoader.java:323)&lt;br/&gt;
	at java.lang.ClassLoader.loadClass(ClassLoader.java:268)&lt;br/&gt;
	... 13 more&lt;/p&gt;

&lt;p&gt;ExternalParser lives in the Tika Core jar, not the Tika Parsers one. This all works fine outside of OSGi, so it looks like something about the OSGi bundling is causing the fork parser to fail to send the parser-related classes from Tika Core over to the forked JVM&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-bundle/src/test/java/org/apache/tika/bundle/BundleIT.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 14 Nov 2015 03:08:01 +0000" id="92" opendate="Mon, 12 Jan 2015 06:59:32 +0000">
		<buginformation>
			<summary>WordParser fails on many Word files</summary>
			<description>&lt;p&gt;WordParser fail on some word files. A negative value is sent to substring&lt;/p&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/WordExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 21 Feb 2015 06:31:05 +0000" id="93" opendate="Sun, 18 Jan 2015 22:48:16 +0000">
		<buginformation>
			<summary>Handle password protected 7zip files</summary>
			<description>&lt;p&gt;While working on &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1028&quot; title=&quot;Tika-server quits parsing of rfc-822 document prematurely when it encounters encrypted zip file as attachment.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1028&quot;&gt;&lt;del&gt;TIKA-1028&lt;/del&gt;&lt;/a&gt;, I notice that while Commons Compress doesn't currently handle decrypting password protected zip files, it does handle password protected 7zip files&lt;/p&gt;

&lt;p&gt;We should therefore add logic into the package parser to spot password protected 7zip files, and fetch the password for them from a PasswordProvider if given&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pkg/PackageParser.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 28 Mar 2015 14:15:50 +0000" id="94" opendate="Wed, 21 Jan 2015 23:03:43 +0000">
		<buginformation>
			<summary>ExternalParser should trap/ignore/workarround JDK-8047340 &amp; JDK-8055301 so Turkish Tika users can still use non-external parsers</summary>
			<description>&lt;p&gt;the JDK has numerous pain points regarding the Turkish locale, &quot;posix_spawn&quot; lowercasing being one of them...&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://bugs.openjdk.java.net/browse/JDK-8047340&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://bugs.openjdk.java.net/browse/JDK-8047340&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;https://bugs.openjdk.java.net/browse/JDK-8055301&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://bugs.openjdk.java.net/browse/JDK-8055301&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As of Tika 1.7, the TesseractOCRParser (which is an ExternalParser) is enabled &amp;amp; configured by default in Tika, and uses ExternalParser.check to see if tesseract is available &amp;#8211; but because of the JDK bug, this means that Tika fails fast for Turkish users on BSD/UNIX variants (including MacOSX) like so...&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;  [junit4]    &amp;gt; Throwable #1: java.lang.Error: posix_spawn is not a supported process launch mechanism on this platform.
  [junit4]    &amp;gt; 	at java.lang.UNIXProcess$1.run(UNIXProcess.java:105)
  [junit4]    &amp;gt; 	at java.lang.UNIXProcess$1.run(UNIXProcess.java:94)
  [junit4]    &amp;gt; 	at java.security.AccessController.doPrivileged(Native Method)
  [junit4]    &amp;gt; 	at java.lang.UNIXProcess.&amp;lt;clinit&amp;gt;(UNIXProcess.java:92)
  [junit4]    &amp;gt; 	at java.lang.ProcessImpl.start(ProcessImpl.java:130)
  [junit4]    &amp;gt; 	at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029)
  [junit4]    &amp;gt; 	at java.lang.Runtime.exec(Runtime.java:620)
  [junit4]    &amp;gt; 	at java.lang.Runtime.exec(Runtime.java:485)
  [junit4]    &amp;gt; 	at org.apache.tika.parser.external.ExternalParser.check(ExternalParser.java:344)
  [junit4]    &amp;gt; 	at org.apache.tika.parser.ocr.TesseractOCRParser.hasTesseract(TesseractOCRParser.java:117)
  [junit4]    &amp;gt; 	at org.apache.tika.parser.ocr.TesseractOCRParser.getSupportedTypes(TesseractOCRParser.java:90)
  [junit4]    &amp;gt; 	at org.apache.tika.parser.CompositeParser.getParsers(CompositeParser.java:81)
  [junit4]    &amp;gt; 	at org.apache.tika.parser.DefaultParser.getParsers(DefaultParser.java:95)
  [junit4]    &amp;gt; 	at org.apache.tika.parser.CompositeParser.getSupportedTypes(CompositeParser.java:229)
  [junit4]    &amp;gt; 	at org.apache.tika.parser.CompositeParser.getParsers(CompositeParser.java:81)
  [junit4]    &amp;gt; 	at org.apache.tika.parser.CompositeParser.getParser(CompositeParser.java:209)
  [junit4]    &amp;gt; 	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:244)
  [junit4]    &amp;gt; 	at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;...unless they go out of their way to white list only the parsers they need/want so TesseractOCRParser (and any other ExternalParsers) will never even be check()ed.&lt;/p&gt;

&lt;p&gt;It would be nice if Tika's ExternalParser class added a similar hack/workarround to what was done in &lt;a href=&quot;https://issues.apache.org/jira/browse/SOLR-6387&quot; title=&quot;Solr specific work around for JDK bug #8047340: posix_spawn error with turkish locale&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SOLR-6387&quot;&gt;&lt;del&gt;SOLR-6387&lt;/del&gt;&lt;/a&gt; to trap these types of errors.  In Solr we just propogate a better error explaining why Java hates the turkish langauge...&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;} &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (Error err) {
  &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (err.getMessage() != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; &amp;amp;&amp;amp; (err.getMessage().contains(&lt;span class=&quot;code-quote&quot;&gt;&quot;posix_spawn&quot;&lt;/span&gt;) || err.getMessage().contains(&lt;span class=&quot;code-quote&quot;&gt;&quot;UNIXProcess&quot;&lt;/span&gt;))) {
    log.warn(&lt;span class=&quot;code-quote&quot;&gt;&quot;Error forking command due to JVM locale bug (see https:&lt;span class=&quot;code-comment&quot;&gt;//issues.apache.org/jira/browse/SOLR-6387): &quot;&lt;/span&gt; + err.getMessage());
&lt;/span&gt;    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&quot;(error executing: &quot;&lt;/span&gt; + cmd + &lt;span class=&quot;code-quote&quot;&gt;&quot;)&quot;&lt;/span&gt;;
  }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;...but with Tika, it might be better for all ExternalParsers to just &quot;opt out&quot; as if they don't recognize the filetype when they detect this type of error fro m the check method (or perhaps it would be better if AutoDetectParser handled this? ... i'm not really sure how it would best fit into Tika's architecture)&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/parser/external/ExternalParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 23 Jan 2015 19:56:35 +0000" id="95" opendate="Fri, 23 Jan 2015 14:35:27 +0000">
		<buginformation>
			<summary>Turn forbidden-apis back on</summary>
			<description>&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=thetaphi&quot; class=&quot;user-hover&quot; rel=&quot;thetaphi&quot;&gt;Uwe Schindler&lt;/a&gt; recently noticed that forbidden-apis was turned off in r1624185, and he submitted a patch to the dev list.  Let's turn it back on.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/test/java/org/apache/tika/sax/BasicContentHandlerFactoryTest.java</file>
			<file>/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 24 Mar 2015 19:39:03 +0000" id="96" opendate="Mon, 26 Jan 2015 17:33:32 +0000">
		<buginformation>
			<summary>Upgrade to POI 3.12-beta1 when available</summary>
			<description>&lt;p&gt;Opening this issue to track integration items with POI 3.12-beta1.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 28 Jan 2015 19:04:59 +0000" id="97" opendate="Wed, 28 Jan 2015 19:00:21 +0000">
		<buginformation>
			<summary>Upgrade to Commons Compress 1.9</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 2 Jul 2015 08:15:34 +0000" id="98" opendate="Fri, 30 Jan 2015 04:21:05 +0000">
		<buginformation>
			<summary>Upgrade compiler definition in pom's to Java 7</summary>
			<description>&lt;p&gt;Since we committed &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1423&quot; title=&quot;Build a parser to extract data from GRIB formats&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1423&quot;&gt;&lt;del&gt;TIKA-1423&lt;/del&gt;&lt;/a&gt; it would appear through &lt;a href=&quot;http://www.mail-archive.com/dev%40tika.apache.org/msg11542.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;mailing list&lt;/a&gt; commentary that there is a willingness to drop support for Java 1.6 in favour of &amp;gt;= Java 1.7.&lt;br/&gt;
This issue simply addresses this.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/pom.xml</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 7 Feb 2015 02:02:57 +0000" id="99" opendate="Tue, 3 Feb 2015 11:15:16 +0000">
		<buginformation>
			<summary>GRB file magic bytes and extension matching </summary>
			<description>&lt;p&gt;GRB type detection with magic bytes and extension probably needs to be supported Tika, the GRB parser is under development, so it may be good to have its magic bytes and extension matching detection.&lt;/p&gt;

&lt;p&gt;However, GRB does not have standard mime type, the following extension and MAGIC matching settings in the tika-mimetypes.xml are proposed to used for GRB mime type idenfication.&lt;/p&gt;

&lt;p&gt;&amp;lt;mime-type type=&quot;application/x-grib&quot;&amp;gt;&lt;br/&gt;
    &amp;lt;acronym&amp;gt;GRIB&amp;lt;/acronym&amp;gt;&lt;br/&gt;
    &amp;lt;_comment&amp;gt;General Regularly-distributed Information in Binary form&amp;lt;/_comment&amp;gt;&lt;br/&gt;
    &amp;lt;tika:link&amp;gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/GRIB&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/GRIB&lt;/a&gt;&amp;lt;/tika:link&amp;gt;&lt;br/&gt;
    &amp;lt;magic priority=&quot;50&quot;&amp;gt;&lt;br/&gt;
      &amp;lt;match value=&quot;GRIB&quot; type=&quot;string&quot; offset=&quot;0&quot;/&amp;gt;&lt;br/&gt;
    &amp;lt;/magic&amp;gt;&lt;br/&gt;
    &amp;lt;glob pattern=&quot;*.grb&quot;/&amp;gt;&lt;br/&gt;
&amp;lt;glob pattern=&quot;*.grb1&quot;/&amp;gt;&lt;br/&gt;
&amp;lt;glob pattern=&quot;*.grb2&quot;/&amp;gt;&lt;/p&gt;


&lt;p&gt;Any kind suggestion and advice will be welcomed and appreciated.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 6 Feb 2015 20:37:11 +0000" id="100" opendate="Fri, 6 Feb 2015 02:24:53 +0000">
		<buginformation>
			<summary>Substitute Apache TTF test file for current non-Apache friendly file</summary>
			<description>&lt;p&gt;On &lt;a href=&quot;https://issues.apache.org/jira/browse/PDFBOX-2383&quot; title=&quot;PDFBox tests include copyright files&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PDFBOX-2383&quot;&gt;&lt;del&gt;PDFBOX-2383&lt;/del&gt;&lt;/a&gt;, copyrighted docs were identified in the test cases.  The ttf test file was one of the offenders, and it came from our test.  Let's remove our current file and substitute Aclonica, licensed as Apache v2.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/mime/TestMimeTypes.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 10 Feb 2015 23:27:13 +0000" id="101" opendate="Tue, 10 Feb 2015 23:16:26 +0000">
		<buginformation>
			<summary>Use POST for tika-server form resources</summary>
			<description>&lt;p&gt;According to the &lt;a href=&quot;http://www.w3.org/TR/html401/interact/forms.html#h-17.13.1&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;HTML Spec&lt;/a&gt;, HTML forms can only use POST and GET. But, the &lt;tt&gt;multipart/form-data&lt;/tt&gt; methods in tika-server use PUT.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-server/src/main/java/org/apache/tika/server/MetadataResource.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 13 Feb 2015 01:20:14 +0000" id="102" opendate="Wed, 11 Feb 2015 15:47:01 +0000">
		<buginformation>
			<summary>System property added while catching exception on parsing PDF encrypted doc</summary>
			<description>&lt;p&gt;I'm using Tika 1.7. I'm parsing an encrypted PDF document which raise an exception. So far, so good.&lt;/p&gt;

&lt;p&gt;My concern is that after that I have a new System property set &lt;tt&gt;sun.font.CFontManager&lt;/tt&gt;. &lt;/p&gt;

&lt;p&gt;Code to reproduce the error:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;@Test
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void testSystem() {
    Properties props = &lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.getProperties();
    assertThat(props.get(&lt;span class=&quot;code-quote&quot;&gt;&quot;sun.font.fontmanager&quot;&lt;/span&gt;), nullValue());
    &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
        tika().parseToString(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; URL(&lt;span class=&quot;code-quote&quot;&gt;&quot;https:&lt;span class=&quot;code-comment&quot;&gt;//github.com/elasticsearch/elasticsearch-mapper-attachments/raw/master/src/test/resources/org/elasticsearch/index/mapper/xcontent/encrypted.pdf&quot;&lt;/span&gt;));
&lt;/span&gt;    } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (Throwable e) {
    }
    assertThat(props.get(&lt;span class=&quot;code-quote&quot;&gt;&quot;sun.font.fontmanager&quot;&lt;/span&gt;), nullValue());
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;With Tika 1.7:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;[2015-02-11 16:43:36,166][INFO ][org.apache.pdfbox.pdfparser.PDFParser] Document is encrypted
[2015-02-11 16:43:36,837][ERROR][org.apache.pdfbox.filter.FlateFilter] FlateFilter: stop reading corrupt stream due to a DataFormatException
[2015-02-11 16:43:36,837][ERROR][org.apache.pdfbox.filter.FlateFilter] FlateFilter: stop reading corrupt stream due to a DataFormatException
[2015-02-11 16:43:36,838][ERROR][org.apache.pdfbox.filter.FlateFilter] FlateFilter: stop reading corrupt stream due to a DataFormatException
[2015-02-11 16:43:36,838][ERROR][org.apache.pdfbox.filter.FlateFilter] FlateFilter: stop reading corrupt stream due to a DataFormatException
[2015-02-11 16:43:36,839][ERROR][org.apache.pdfbox.filter.FlateFilter] FlateFilter: stop reading corrupt stream due to a DataFormatException
[2015-02-11 16:43:36,840][ERROR][org.apache.pdfbox.filter.FlateFilter] FlateFilter: stop reading corrupt stream due to a DataFormatException
[2015-02-11 16:43:36,840][ERROR][org.apache.pdfbox.filter.FlateFilter] FlateFilter: stop reading corrupt stream due to a DataFormatException
[2015-02-11 16:43:36,841][ERROR][org.apache.pdfbox.filter.FlateFilter] FlateFilter: stop reading corrupt stream due to a DataFormatException
[2015-02-11 16:43:36,841][ERROR][org.apache.pdfbox.filter.FlateFilter] FlateFilter: stop reading corrupt stream due to a DataFormatException
[2015-02-11 16:43:36,842][ERROR][org.apache.pdfbox.filter.FlateFilter] FlateFilter: stop reading corrupt stream due to a DataFormatException

java.lang.AssertionError: 
Expected: &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;
     but: was &lt;span class=&quot;code-quote&quot;&gt;&quot;sun.font.CFontManager&quot;&lt;/span&gt;
 &amp;lt;Click to see difference&amp;gt;
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:8)
	at org.elasticsearch.plugin.mapper.attachments.test.TikaSystemTest.testSystem(TikaSystemTest.java:41)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:300)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:157)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:74)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:211)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:67)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;With Tika 1.6:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;[2015-02-11 16:38:42,922][INFO ][org.apache.pdfbox.pdfparser.PDFParser] Document is encrypted
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note also that it logs a lot of errors which was not the case in Tika 1.6.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDFParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 6 Mar 2015 14:43:28 +0000" id="103" opendate="Fri, 20 Feb 2015 13:58:17 +0000">
		<buginformation>
			<summary>Let's add a mock parser to be used in testing parser drivers and wrappers</summary>
			<description>&lt;p&gt;As part of &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1302&quot; title=&quot;Let&amp;#39;s run Tika against a large batch of docs nightly&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1302&quot;&gt;TIKA-1302&lt;/a&gt; and as part of making Tika more robust generally, it would be useful to have an evil parser that will throw exceptions/errors and hang for lengths of time.  &lt;/p&gt;

&lt;p&gt;This will allow us to test timeouts and handling of exceptions and errors in tika-server and in tika-batch.  &lt;/p&gt;

&lt;p&gt;We could also use this for tests with ForkParser.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 20 Feb 2015 19:29:57 +0000" id="104" opendate="Fri, 20 Feb 2015 19:26:01 +0000">
		<buginformation>
			<summary>Clean up whitespace in tika-server</summary>
			<description>&lt;p&gt;We have 2- and 4-space indents in different parts of tika-server's code.  Let's make consistent with rest of Tika.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-server/src/main/java/org/apache/tika/server/CSVMessageBodyWriter.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 21 Feb 2015 05:22:20 +0000" id="105" opendate="Fri, 20 Feb 2015 21:51:22 +0000">
		<buginformation>
			<summary>Create a Parser Blacklist</summary>
			<description>&lt;p&gt;As talked about in &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1555&quot; title=&quot;posix_spawn is not a supported process launch mechanism on this platform&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1555&quot;&gt;&lt;del&gt;TIKA-1555&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1557&quot; title=&quot;Create TesseractOCR Option to Never Run&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1557&quot;&gt;&lt;del&gt;TIKA-1557&lt;/del&gt;&lt;/a&gt;, it would be nice to be able to disable Parsers without pulling their dependencies out. In some cases (e.g. disable all ExternalParsers), there may not be an easy way to exclude the dependencies via Maven.&lt;/p&gt;

&lt;p&gt;&lt;del&gt;So, an initial design would be to include another file like &lt;tt&gt;META-INF/services/org.apache.tika.parser.Parser.blacklist&lt;/tt&gt;. We create a new method &lt;tt&gt;ServiceLoader#loadServiceProviderBlacklist&lt;/tt&gt;. Then, in &lt;tt&gt;ServiceLoader#loadServiceProviders&lt;/tt&gt;, we remove all elements of the list that are assignable to an element in &lt;tt&gt;ServiceLoader#loadServiceProviderBlacklist&lt;/tt&gt;.&lt;/del&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/config/TikaConfig.java</file>
			<file>/tika-parsers/src/test/java/org/apache/tika/config/TikaParserConfigTest.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 1 Mar 2015 17:55:31 +0000" id="106" opendate="Sun, 1 Mar 2015 16:52:35 +0000">
		<buginformation>
			<summary>Use .gz as the default extension for application/gzip</summary>
			<description>&lt;p&gt;This ticket is in reference to the following code:&lt;/p&gt;

&lt;p&gt;MimeType mimeType = config.getMimeRepository().forName(&quot;application/gzip&quot;); &lt;br/&gt;
String extension = mimeType.getExtension();&lt;/p&gt;

&lt;p&gt;The string extension will be &quot;.tgz&quot;. While it is possible to get .gz or other common extensions with mimeType.getExtensions(), it would be nice if the default was .gz. Every .tgz is an application/gzip, but every application/gzip is not a .tgz. &lt;/p&gt;

&lt;p&gt;Note that although the code above uses the static mime type application/gzip, it is more likely that real-world code is performing mime type detection on specific files and looking up the returned type in the mime registry. Returning .gz for a gzipped tarball isn't wrong, but is less specific than its real type. However, returning .tgz for gzipped text is incorrect.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 13 Mar 2015 22:11:04 +0000" id="107" opendate="Mon, 2 Mar 2015 22:07:51 +0000">
		<buginformation>
			<summary>Organize tika-server package structure</summary>
			<description>&lt;p&gt;We should create &lt;tt&gt;org.apache.tika.server.{writer, resource&lt;/tt&gt;} packages. The other files can stay in &lt;tt&gt;org.apache.tika.server&lt;/tt&gt;. Any objections?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-server/src/main/java/org/apache/tika/server/MetadataList.java</file>
			<file>/tika-server/src/main/java/org/apache/tika/server/TikaServerCli.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 11 Mar 2015 00:56:25 +0000" id="108" opendate="Tue, 10 Mar 2015 23:40:51 +0000">
		<buginformation>
			<summary>Upgrade UCAR dependencies to 4.5.5</summary>
			<description>&lt;p&gt;The UCAR 4.5.5 dependencies were recently released and I pushed them to &lt;a href=&quot;http://search.maven.org/#search&quot; title=&quot;ga|1|ucar&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;Maven central&lt;/a&gt; today.&lt;br/&gt;
Apparently there are bug fixes in a good few areas which we can take advantage of&lt;br/&gt;
&lt;a href=&quot;https://www.unidata.ucar.edu/blogs/news/entry/netcdf_java_library_version_44&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://www.unidata.ucar.edu/blogs/news/entry/netcdf_java_library_version_44&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 15 Mar 2015 05:08:21 +0000" id="109" opendate="Sun, 15 Mar 2015 05:04:43 +0000">
		<buginformation>
			<summary>Upgrade metadata-extractor to version 2.7.2</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 28 Mar 2015 21:51:14 +0000" id="110" opendate="Fri, 20 Mar 2015 01:15:55 +0000">
		<buginformation>
			<summary>ISA-Tab parsers</summary>
			<description>&lt;p&gt;We are going to add parsers for ISA-Tab data formats.&lt;br/&gt;
ISA-Tab files are related to &lt;a href=&quot;http://www.isa-tools.org/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;ISA Tools&lt;/a&gt; which help to manage an increasingly diverse set of life science, environmental and biomedical experiments that employing one or a combination of technologies.&lt;br/&gt;
The ISA tools are built upon &lt;em&gt;Investigation&lt;/em&gt;, &lt;em&gt;Study&lt;/em&gt;, and &lt;em&gt;Assay&lt;/em&gt; tabular format. Therefore, ISA-Tab data format includes three types of file: Investigation file (&lt;tt&gt;a_xxxx.txt&lt;/tt&gt;), Study file (&lt;tt&gt;s_xxxx.txt&lt;/tt&gt;), Assay file (&lt;tt&gt;a_xxxx.txt&lt;/tt&gt;). These files are organized as &lt;a href=&quot;http://www.isa-tools.org/format/specification/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;top-down hierarchy&lt;/a&gt;: An Investigation file includes one or more Study files: each Study files includes one or more Assay files.&lt;br/&gt;
Essentially, the Investigation files contains high-level information about the related study, so it provides only metadata about ISA-Tab files.&lt;br/&gt;
More details on file format specification are &lt;a href=&quot;http://isatab.sourceforge.net/docs/ISA-TAB_release-candidate-1_v1.0_24nov08.pdf&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;available online&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The patch in attachment provides a preliminary version of ISA-Tab parsers (there are three parsers; one parser for each ISA-Tab filetype):&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;&lt;tt&gt;ISATabInvestigationParser.java&lt;/tt&gt;: parses Investigation files. It extracts only metadata.&lt;/li&gt;
	&lt;li&gt;&lt;tt&gt;ISATabStudyParser.java&lt;/tt&gt;: parses Study files.&lt;/li&gt;
	&lt;li&gt;&lt;tt&gt;ISATabAssayParser.java&lt;/tt&gt;: parses Assay files.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The most important improvements are:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Combine these three parsers in order to parse an ISArchive&lt;/li&gt;
	&lt;li&gt;Provide a better mapping of both study and assay data on XHML. Currently, &lt;tt&gt;ISATabStudyParser&lt;/tt&gt; and &lt;tt&gt;ISATabAssayParser&lt;/tt&gt; provide a naive mapping function relying on &lt;a href=&quot;https://commons.apache.org/proper/commons-csv/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;Apache Commons CSV&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Thanks for supporting me on this work &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=chrismattmann&quot; class=&quot;user-hover&quot; rel=&quot;chrismattmann&quot;&gt;Chris A. Mattmann&lt;/a&gt;. &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/isatab/ISArchiveParserTest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 27 Mar 2015 14:54:00 +0000" id="111" opendate="Fri, 20 Mar 2015 08:33:07 +0000">
		<buginformation>
			<summary>jhighlight license concerns</summary>
			<description>&lt;p&gt;jhighlight jar is a Tika dependency.  The Lucene team discovered that, while it claims to be a CDDL/LGPL dual-license, some of its functionality is LGPL only:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Solr's contrib/extraction contains jhighlight-1.0.jar which declares itself as dual CDDL or LGPL license. However, some of its classes are distributed only under LGPL, e.g.

com.uwyn.jhighlight.highlighter.
  CppHighlighter.java
  GroovyHighlighter.java
  JavaHighlighter.java
  XmlHighlighter.java

I downloaded the sources from Maven (http:&lt;span class=&quot;code-comment&quot;&gt;//search.maven.org/remotecontent?filepath=com/uwyn/jhighlight/1.0/jhighlight-1.0-sources.jar) to confirm that, and also found &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; SVN repo: http://svn.rifers.org/jhighlight/tags/release-1.0, though the project's website seems to not exist anymore (https://jhighlight.dev.java.net/).
&lt;/span&gt;
I didn&lt;span class=&quot;code-quote&quot;&gt;'t find any direct usage of it in our code, so I guess it'&lt;/span&gt;s probably needed by a 3rd party dependency, such as Tika. Therefore &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; we e.g. omit it, things will compile, but may fail at runtime.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Is it possible to remove this dependency for future releases, or allow only optional inclusion of this package?  It is of concern to the ManifoldCF project because we distribute a binary package that includes Tika and its required dependencies, which currently includes jHighlight.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/NOTICE.txt</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 27 Mar 2015 18:21:39 +0000" id="112" opendate="Fri, 27 Mar 2015 18:11:22 +0000">
		<buginformation>
			<summary>Convert Module Level READMEs to Markdown</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/tika-server/README.md</file>
			<file>/tika-server/README</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 30 Mar 2015 14:01:21 +0000" id="113" opendate="Sat, 28 Mar 2015 14:21:35 +0000">
		<buginformation>
			<summary>Tika 1.7 possible regression (nested attachment files not getting parsed)</summary>
			<description>&lt;p&gt;I tried to send this to the tika user list, but got a qmail failure so I am opening a jira to see if I can get help with this.&lt;/p&gt;

&lt;p&gt;There appears to be a change in the behavior of tika since 1.5 (the last version we have used). In 1.5, if we pass a file with content type of rfc822 which contains a zip that contains a docx file, the entire content would get recursed and the text returned. In 1.7, tika only unwinds as far as the zip file and ignores the content of the contained docx file. This is causing a regression failure in our search tests because the contents of the docx file are not found when searched for.&lt;/p&gt;

&lt;p&gt;We are testing with tika-server if this helps. If we ask the meta service to just characterize the test data, it correctly determines the input is of type rfc822. However, on extract, the contents of the attachment are not extracted as expected.&lt;/p&gt;

&lt;p&gt;curl -X PUT -T test.eml -q -H Content-Type:application/octet-stream  &lt;a href=&quot;http://localhost:9998/meta&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://localhost:9998/meta&lt;/a&gt; 2&amp;gt;/dev/null | grep Content-Type&lt;br/&gt;
&quot;Content-Type&quot;,&quot;message/rfc822&quot;&lt;/p&gt;

&lt;p&gt;curl -X PUT -T test.eml -q -H Content-Type:application/octet-stream  &lt;a href=&quot;http://localhost:9998/tika&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://localhost:9998/tika&lt;/a&gt; 2&amp;gt;/dev/null | grep docx&lt;br/&gt;
sign.docx       &amp;lt;&amp;lt;&amp;lt;&amp;lt;--- this is not expected, need contents of this extracted&lt;/p&gt;


&lt;p&gt;We can easily reproduce this problem with a simple eml file with an attachment. Can someone please comment if this seems like a problem or perhaps we need to change something in our call to get the old behavior?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-server/src/main/java/org/apache/tika/server/resource/MetadataResource.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 28 Mar 2015 16:34:37 +0000" id="114" opendate="Sat, 28 Mar 2015 14:49:09 +0000">
		<buginformation>
			<summary>Enable CORS on Tika Server</summary>
			<description>&lt;p&gt;Tika Server should allow configuration of CORS requests (for uses like &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1585&quot; title=&quot;Create Example Website with Form Submission&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1585&quot;&gt;&lt;del&gt;TIKA-1585&lt;/del&gt;&lt;/a&gt;). See &lt;a href=&quot;http://cxf.apache.org/docs/jax-rs-cors.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;this example&lt;/a&gt; from CXF for how to add it.&lt;/p&gt;

&lt;p&gt;The only change from that site is that we will need to add a &lt;tt&gt;CrossOriginResourceSharingFilter&lt;/tt&gt; as a provider.&lt;/p&gt;

&lt;p&gt;Ideally, this is configurable (limit which resources have CORS, and which origins are allowed). But, I'm not thinking of any general methods of how to do that...&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-server/src/main/java/org/apache/tika/server/TikaServerCli.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 31 Mar 2015 13:21:57 +0000" id="115" opendate="Tue, 31 Mar 2015 12:37:05 +0000">
		<buginformation>
			<summary>Mp3 parser does not add duration to metadata if there are no ID3 tags</summary>
			<description>&lt;p&gt;Steps to reproduce:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Have a file without any ID3 tags (v1 or v2)&lt;/li&gt;
	&lt;li&gt;Parse the file&lt;/li&gt;
	&lt;li&gt;Attempt to retrieve the duration by calling 'metadata.get(XMPDM.DURATION)'.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Expected result:&lt;br/&gt;
The duration should be set even for a file without ID3 tags, since it is independent information.&lt;/p&gt;

&lt;p&gt;Actual result:&lt;br/&gt;
The duration is null&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 10 Apr 2015 00:59:36 +0000" id="116" opendate="Tue, 7 Apr 2015 07:50:39 +0000">
		<buginformation>
			<summary>Webp parsing support</summary>
			<description>&lt;p&gt;webp content type is correctly detected, but parsing is not supported. &lt;/p&gt;

&lt;p&gt;I noticed that metadata-extractor 2.8.0 supports webp:&lt;br/&gt;
&lt;a href=&quot;https://github.com/drewnoakes/metadata-extractor/issues/85&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/drewnoakes/metadata-extractor/issues/85&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;However, Tika does currently not work with this version (I tried manually overriding the dependency). &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 19 Apr 2015 23:33:19 +0000" id="117" opendate="Wed, 15 Apr 2015 18:58:20 +0000">
		<buginformation>
			<summary>Tika has a dependency on a very old version of Guava</summary>
			<description>&lt;p&gt;I've run into one problem while testing Tika 1.8-rc2 with Bixo&lt;/p&gt;

&lt;p&gt;It involves a dependency issue involving (of course) Guava, since that project loves to break their API &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;The bixo-core jar has these transitive dependencies on various versions of Guava:&lt;/p&gt;

&lt;p&gt;Hadoop - 11.0.2&lt;br/&gt;
Cascading - 14.0.1&lt;br/&gt;
Tika-parsers - 10.0.1&lt;br/&gt;
	cdm - 17.0&lt;/p&gt;

&lt;p&gt;Everyone winds up using version 10.0.1 (note that Tika has a dependency on cdm, which wants to use 17.0)&lt;/p&gt;

&lt;p&gt;The problem is that Hadoop (for any recent version) uses an API from Guava's cache implementation that no longer exists:&lt;/p&gt;

&lt;p&gt;com.google.common.cache.CacheBuilder.build(Lcom/google/common/cache/CacheLoader;)Lcom/google/common/cache/LoadingCache;&lt;br/&gt;
java.lang.NoSuchMethodError: com.google.common.cache.CacheBuilder.build(Lcom/google/common/cache/CacheLoader;)Lcom/google/common/cache/LoadingCache;&lt;br/&gt;
	at org.apache.hadoop.io.compress.CodecPool.createCache(CodecPool.java:62)&lt;br/&gt;
	at org.apache.hadoop.io.compress.CodecPool.&amp;lt;clinit&amp;gt;(CodecPool.java:74)&lt;br/&gt;
	at org.apache.hadoop.io.SequenceFile$Writer.close(SequenceFile.java:1272)&lt;br/&gt;
	at org.apache.hadoop.mapred.SequenceFileOutputFormat$1.close(SequenceFileOutputFormat.java:79)&lt;/p&gt;

&lt;p&gt;So what this means is that anyone trying to use Tika with Hadoop will need to play games with the class loader to get the older version of Guava - though that can cause other issues if Hadoop (or Cascading, etc) rely on anything that's only in the newer Guava API.&lt;/p&gt;

&lt;p&gt;Guava 10.0.1 was released about 3.5 years ago; 11.0.2 was from about 3 years ago. So it seems like we should upgrade to at least 11.0.2&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 1 May 2015 16:23:58 +0000" id="118" opendate="Tue, 21 Apr 2015 06:41:42 +0000">
		<buginformation>
			<summary>CBOR Parser and detection [improvement]</summary>
			<description>&lt;p&gt;CBOR is a data format whose design goals include the possibility of extremely small code size, fairly small message size, and extensibility without the need for version negotiation (cited from &lt;a href=&quot;http://cbor.io/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://cbor.io/&lt;/a&gt; ).&lt;/p&gt;

&lt;p&gt;It would be great if Tika is able to provide the support with CBOR parser and identification. In the current project with Nutch, the Nutch CommonCrawlDataDumper is used to dump the crawled segments to the files in the format of CBOR. In order to read/parse those dumped files by this tool, it would be great if tika is able to support parsing the cbor, the thing is that the CommonCrawlDataDumper is not dumping with correct extension, it dumps with its own rule, the default extension of the dumped file is html, so it might be less painful if tika is able to detect and parse those files without any pre-processing steps. &lt;/p&gt;

&lt;p&gt;CommonCrawlDataDumper is calling the following to dump with cbor.&lt;br/&gt;
import com.fasterxml.jackson.dataformat.cbor.CBORFactory;&lt;br/&gt;
import com.fasterxml.jackson.dataformat.cbor.CBORGenerator;&lt;/p&gt;

&lt;p&gt;fasterxml is a 3rd party library for converting json to .cbor and Vice Versa.&lt;/p&gt;

&lt;p&gt;According to RFC 7049 (&lt;a href=&quot;http://tools.ietf.org/html/rfc7049&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://tools.ietf.org/html/rfc7049&lt;/a&gt;), it looks like CBOR does not yet have its magic numbers to be detected/identified by other applications (PFA: rfc_cbor.jpg)&lt;br/&gt;
It seems that the only way to inform other applications of the type as of now is using the extension (i.e. .cbor), or probably content detection (i.e. byte histogram distribution estimation).  &lt;/p&gt;

&lt;p&gt;There is another thing worth the attention, it looks like tika has attempted to add the support with cbor mime detection in the tika-mimetypes.xml (PFA:cbor_tika.mimetypes.xml.jpg); This detection is not working with the cbor file dumped by CommonCrawlDataDumper. &lt;br/&gt;
According to &lt;a href=&quot;http://tools.ietf.org/html/rfc7049#section-2.4.5&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://tools.ietf.org/html/rfc7049#section-2.4.5&lt;/a&gt;, there is a self-describing Tag 55799 that seems to be used for cbor type identification(the hex code might be 0xd9d9f7), but it is probably up to the application that take care of this tag, and it is also possible that the fasterxml that the nutch dumping tool is missing this tag, an example cbor file dumped by the Nutch tool i.e. CommonCrawlDataDumper has also been attached (PFA: 1424402690000.html).&lt;br/&gt;
The following info is cited from the rfc, &quot;...a decoder might be able to parse both CBOR and JSON.&lt;br/&gt;
   Such a decoder would need to mechanically distinguish the two&lt;br/&gt;
   formats.  An easy way for an encoder to help the decoder would be to&lt;br/&gt;
   tag the entire CBOR item with tag 55799, the serialization of which&lt;br/&gt;
   will never be found at the beginning of a JSON text...&quot;&lt;br/&gt;
It looks like the a file can have two parts/sections i.e. the plain text parts and the json prettified by cbor, this might be also worth the attention and consideration with the parsing and type identification.&lt;/p&gt;

&lt;p&gt;On the other hand, it is worth noting that the entries for cbor extension detection needs to be appended in the tika-mimetypes.xml too &lt;br/&gt;
e.g.&lt;br/&gt;
&amp;lt;glob pattern=&quot;*.cbor&quot;/&amp;gt;&lt;/p&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 19 Aug 2015 15:07:31 +0000" id="119" opendate="Mon, 10 Aug 2015 20:08:17 +0000">
		<buginformation>
			<summary>Update ASM dependency to 5.0.4</summary>
			<description>&lt;p&gt;Currently the Class file parser uses ASM 4.1. This older version cannot read Java 8 / Java 9 class files (fails with Exception).&lt;/p&gt;

&lt;p&gt;The upgrade to ASM 5.0.4 is very simple, just Maven dependency change. The code change is only to update the visitor version, so it gets new Java 8 features like lambdas reported, but this is not really required, but should be done for full support.&lt;/p&gt;

&lt;p&gt;FYI, in &lt;a href=&quot;https://issues.apache.org/jira/browse/LUCENE-6729&quot; title=&quot;Upgrade ASM version to 5.0.4 (expressions / Solr&amp;#39;s TIKA)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;LUCENE-6729&quot;&gt;&lt;del&gt;LUCENE-6729&lt;/del&gt;&lt;/a&gt; we want to upgrade the Lucene Expressions module to ASM 5, too.&lt;/p&gt;

&lt;p&gt;You can hot-swap ASM 4.1 with ASM 5.0.4 without recompilation (so we have no problem with Lucene using a newer version). Since ASM 4.x the updates are more easy (no visitor interfaces anymore, instead abstract classes), so it does not break if you just replace the JAR file. So just see this as a recommendatation, not urgent! Solr/Lucene will also work without this patch (it just replaces the shipped ASM by newer version in our packaging).&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-bundle/pom.xml</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 20 May 2011 16:54:42 +0000" id="120" opendate="Mon, 12 Nov 2007 02:22:51 +0000">
		<buginformation>
			<summary>Pluggable magic header detectors</summary>
			<description>&lt;p&gt;Some file formats like MS Office files or specific XML schemas don't have simple magic marker bytes that could be used to easily identify the type of the document. However, it would in many cases be possible to detect such formats with more complex parsing logic.&lt;/p&gt;

&lt;p&gt;Also, there are some external libraries (like Sanselan as mentioned in &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-92&quot; title=&quot;Image metadata extraction&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-92&quot;&gt;&lt;del&gt;TIKA-92&lt;/del&gt;&lt;/a&gt;) that contain their own magic header rules. Instead of duplicating such rules in Tika, it would be better if Tika could just invoke the existing external functionality.&lt;/p&gt;

&lt;p&gt;To support these cases Tika should provide a mechanism to plug in custom magic header detector components in addition to the traditional configured magic patterns.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/src/main/java/org/apache/tika/detect/TypeDetector.java</file>
			<file>/src/test/java/org/apache/tika/detect/NameDetectorTest.java</file>
			<file>/src/main/java/org/apache/tika/mime/MimeTypes.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/detect/CompositeDetector.java</file>
			<file>/src/main/java/org/apache/tika/detect/TextDetector.java</file>
			<file>/src/main/java/org/apache/tika/detect/Detector.java</file>
			<file>/src/main/java/org/apache/tika/detect/NameDetector.java</file>
			<file>/src/main/java/org/apache/tika/detect/CompositeDetector.java</file>
			<file>/src/main/java/org/apache/tika/detect/MagicDetector.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 24 Nov 2014 09:14:36 +0000" id="121" opendate="Thu, 9 Jun 2011 09:24:49 +0000">
		<buginformation>
			<summary>Proper error handling in the CHM parser</summary>
			<description>&lt;p&gt;The new CHM parser (&lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-245&quot; title=&quot;Support of CHM Format&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-245&quot;&gt;&lt;del&gt;TIKA-245&lt;/del&gt;&lt;/a&gt;) swallows exceptions and uses System.err and System.out prints to report problems in many places. We should change that to properly throw exceptions as follows:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;IOExceptions when the document stream can not be read&lt;/li&gt;
	&lt;li&gt;TikaExceptions when the stream can not be parsed&lt;/li&gt;
&lt;/ul&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/chm/CHMDocumentInformation.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 16 Mar 2015 00:15:39 +0000" id="122" opendate="Thu, 16 Jan 2014 00:10:32 +0000">
		<buginformation>
			<summary>Tika does not extract attachments from RFC822 files</summary>
			<description>&lt;p&gt;TikaApp --extract option does not extract attachments from RFC822 files. The issue happens because MailContentHandler.body(...) method gets a Parser.class object from the context and calls parser.parse(). It should get a EmbeddedDocumentExtractor.class object from the ParseContext one and call embeddedDocumentExtractor.parseEmbedded(), similar to other Container parsers.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/mail/MailContentHandler.java</file>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/mail/RFC822ParserTest.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/extractor/ParsingEmbeddedDocumentExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 11 Feb 2014 12:09:17 +0000" id="123" opendate="Tue, 11 Feb 2014 12:07:57 +0000">
		<buginformation>
			<summary>Upgrade to POI-3.10-FINAL</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 26 Feb 2014 12:19:27 +0000" id="124" opendate="Wed, 26 Feb 2014 08:57:39 +0000">
		<buginformation>
			<summary>Vcard files detection</summary>
			<description>&lt;p&gt;Please improve vcard (&lt;a href=&quot;http://it.wikipedia.org/wiki/VCard&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://it.wikipedia.org/wiki/VCard&lt;/a&gt;) files detection with the following &quot;mime-type&quot; entry:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;&amp;lt;mime-type type=&quot;text/x-vcard&quot;&amp;gt;
	  &amp;lt;glob pattern=&quot;*.vcf&quot;/&amp;gt;
	  &amp;lt;sub-class-of type=&quot;text/plain&quot;/&amp;gt;
	  &amp;lt;magic priority=&quot;50&quot;&amp;gt;
	     &amp;lt;match value=&quot;BEGIN:VCARD&quot; type=&quot;string&quot; offset=&quot;0&quot;/&amp;gt;
	  &amp;lt;/magic&amp;gt;
&amp;lt;/mime-type&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 22 Mar 2016 17:39:01 +0000" id="125" opendate="Wed, 30 Apr 2014 00:46:57 +0000">
		<buginformation>
			<summary>Upgrade to PDFBox 2.0.0 when available</summary>
			<description>&lt;p&gt;This issue is to track fixes required when upgrading the PDFbox dependency to 2.0.0 Final once it's available, and using PDFBox's daily build before then.&lt;/p&gt;


&lt;p&gt;See &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1268&quot; title=&quot;Extract images from PDF documents&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1268&quot;&gt;&lt;del&gt;TIKA-1268&lt;/del&gt;&lt;/a&gt; comment.&lt;/p&gt;

&lt;p&gt;Relates to &lt;a href=&quot;https://issues.apache.org/jira/browse/PDFBOX-1893&quot; title=&quot;Refactor color spaces&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PDFBOX-1893&quot;&gt;&lt;del&gt;PDFBOX-1893&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/image/ImageParserTest.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 30 May 2014 18:24:49 +0000" id="126" opendate="Mon, 19 May 2014 15:45:28 +0000">
		<buginformation>
			<summary>New list processing changes appear to be causing RTFParser exception</summary>
			<description>&lt;p&gt;Some RTFs cause RTFParser to throw a RuntimeException:&lt;/p&gt;

&lt;p&gt;Unexpected RuntimeException from org.apache.tika.parser.rtf.RTFParser@425e60f2&lt;/p&gt;

&lt;p&gt;When tracing in the debugger (surfaces in CompositeParser.parse() where it catches the RuntimeException, line 244 in my copy), the exception (e) is:&lt;/p&gt;

&lt;p&gt;java.lang.ArrayIndexOutOfBoundsException: -1&lt;/p&gt;

&lt;p&gt;A committer (Tim Allison) believes that it is being caused by recent list processing changes.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/resources/test-documents/testRTFCorruptListOverride.rtf</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 23 Jul 2015 17:31:03 +0000" id="127" opendate="Fri, 30 May 2014 15:53:03 +0000">
		<buginformation>
			<summary>Basic list support in WordExtractor</summary>
			<description>&lt;p&gt;Hello guys, I am really sorry to post issue like this because I have no other way of contacting you and I don't quite understand how you manage forks and pull requests (I don't think you do that). Plus I don't know your coding styles and stuff.&lt;/p&gt;

&lt;p&gt;In my project I needed for tika to parse numbered lists from word .doc documents, but TIKA doesn't support it. So I looked for solution and found one here: &lt;a href=&quot;http://developerhints.blog.com/2010/08/28/finding-out-list-numbers-in-word-document-using-poi-hwpf/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://developerhints.blog.com/2010/08/28/finding-out-list-numbers-in-word-document-using-poi-hwpf/&lt;/a&gt; . So I adapted this solution to Apache TIKA with few fixes and improvements. Anyway feel free to use any of it so it can help people who struggle with lists in TIKA like I did.&lt;/p&gt;

&lt;p&gt;Attached files are:&lt;br/&gt;
Updated test&lt;br/&gt;
Fixed WordExtractor&lt;br/&gt;
Added ListUtils&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/AbstractListManager.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 20 Mar 2015 20:49:16 +0000" id="128" opendate="Fri, 6 Jun 2014 10:50:52 +0000">
		<buginformation>
			<summary>Move the font metadata definitions to properties</summary>
			<description>&lt;p&gt;As noticed while working on &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1182&quot; title=&quot;Out of memory exception when parsing TTF file&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1182&quot;&gt;&lt;del&gt;TIKA-1182&lt;/del&gt;&lt;/a&gt;, the AFM font parser has a bunch of hard coded strings it uses as metadata keys, while the TTF font parser doesn't have many&lt;/p&gt;

&lt;p&gt;We should switch these to being proper Properties, with definitions from a well known standard (+ compatibility fallbacks), and have both use largely the same set&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/font/FontParsersTest.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/font/AdobeFontMetricParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 17 Jun 2014 16:06:14 +0000" id="129" opendate="Tue, 17 Jun 2014 10:54:28 +0000">
		<buginformation>
			<summary>double invocation of handler.endDocument() in PDFParser</summary>
			<description>&lt;p&gt;I currently migrate to Tika 1.5, and fall into this behaviour, which leads to double entries in my database for one pdf file as I work directly with the handler.&lt;/p&gt;

&lt;p&gt;Here are the two calls:&lt;/p&gt;

&lt;p&gt;First call is in PDF2HTML, line 197: handler.endDocument();&lt;br/&gt;
this is part of the PDF2XHTML.process(pdfDocument, handler, context, metadata, localConfig);&lt;br/&gt;
invocation from PDFParser, line 143.&lt;/p&gt;


&lt;p&gt;The second call is then directly in PDFParser, line 151: handler.endDocument();&lt;/p&gt;

&lt;p&gt;Must stay at Tika 1.4 for now - still thanks for good work!&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDFParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 5 Aug 2014 19:02:51 +0000" id="130" opendate="Wed, 30 Jul 2014 15:52:22 +0000">
		<buginformation>
			<summary>Upgrade to Apache POI 3.11 beta 1</summary>
			<description>&lt;p&gt;All being well, in a week there'll be a new release of Apache POI available, 3.11 beta 1&lt;/p&gt;

&lt;p&gt;This issue is to track the upgrade, any required changes, and fixing any TODOs that this upgrade permits&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/test/java/org/apache/tika/cli/TikaCLITest.java</file>
			<file>/tika-parsers/src/test/resources/test-documents/testWORD_missing_text.docx</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 14 Dec 2014 03:24:33 +0000" id="131" opendate="Tue, 5 Aug 2014 15:46:23 +0000">
		<buginformation>
			<summary>Use tika-parent dependency management for common dependencies</summary>
			<description>&lt;p&gt;If we list a dependency in the dependencyManagement section of the tika-parent pom.xml, we can then include that dependency in a child module without specifying a version.&lt;/p&gt;

&lt;p&gt;For example, I updated the junit dependencies yesterday: &lt;a href=&quot;https://github.com/apache/tika/commit/2fec4c61267ed2c465e7411d50fbf7e9841523d5&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/tika/commit/2fec4c61267ed2c465e7411d50fbf7e9841523d5&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;By using dependencyManagement, we can update the dependency version for all modules at once, rather than have different versions in different modules, like it was for junit.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 8 Sep 2014 19:12:35 +0000" id="132" opendate="Mon, 8 Sep 2014 18:32:58 +0000">
		<buginformation>
			<summary>NPE in OpenDocumentParser</summary>
			<description>&lt;p&gt;There's a missing &quot;else&quot; in OpenDocumentParser when it constructs a ZipInputStream from the InputStream, which results in NPE when the InputStream is an instance of TikaInputStream but has neither openContainer nor file:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;...
Caused by: java.lang.NullPointerException
        at org.apache.tika.parser.odf.OpenDocumentParser.parse(OpenDocumentParser.java:161) ~[tika-parsers-1.6.jar:1.6]
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:244) ~[tika-core-1.6.jar:1.6]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/odf/ODFParserTest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 9 Sep 2014 13:02:49 +0000" id="133" opendate="Mon, 8 Sep 2014 21:54:28 +0000">
		<buginformation>
			<summary>OOXML thumbnail name added to body</summary>
			<description>&lt;p&gt;AbstractOOXMLExtractor.handleThumbnail processes thumbnails using EmbeddedDocumentExtractor, but with the outputHtml flag set to true (unlike other embedded parts in handleEmbeddedParts(...)).&lt;br/&gt;
This results in adding the thumbnail name to the main body of the document (as a package-entry), which in my opinion is wrong.&lt;/p&gt;

&lt;p&gt;Example:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&amp;lt;?xml version=&lt;span class=&quot;code-quote&quot;&gt;&quot;1.0&quot;&lt;/span&gt; encoding=&lt;span class=&quot;code-quote&quot;&gt;&quot;UTF-8&quot;&lt;/span&gt;?&amp;gt;&amp;lt;html xmlns=&lt;span class=&quot;code-quote&quot;&gt;&quot;http:&lt;span class=&quot;code-comment&quot;&gt;//www.w3.org/1999/xhtml&quot;&lt;/span&gt;&amp;gt;
&lt;/span&gt;&amp;lt;head&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;meta:slide-count&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;1&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;cp:revision&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;5&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;meta:last-author&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;Nick Burch&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;Slide-Count&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;1&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;Last-Author&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;Nick Burch&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;meta:save-date&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;2010-09-08T16:15:14Z&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;Content-Length&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;202969&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;subject&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;Gym &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;featuring a brown fox and lazy dog&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;Application-Name&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;Microsoft Office PowerPoint&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;Author&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;Nevin Nollop&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;dcterms:created&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;1601-01-01T00:00:00Z&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;Application-Version&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;12.0000&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;date&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;2010-09-08T16:15:14Z&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;Total-Time&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;2&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;extended-properties:Template&quot;&lt;/span&gt; content=&quot;&quot;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;publisher&quot;&lt;/span&gt; content=&quot;&quot;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;creator&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;Nevin Nollop&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;Word-Count&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;9&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;meta:paragraph-count&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;1&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;extended-properties:AppVersion&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;12.0000&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;Creation-Date&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;1601-01-01T00:00:00Z&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;meta:author&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;Nevin Nollop&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;cp:subject&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;Gym &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;featuring a brown fox and lazy dog&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;extended-properties:Application&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;Microsoft Office PowerPoint&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;resourceName&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;testPPT_embeded.pptx&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;Paragraph-Count&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;1&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;dc:title&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;The quick brown fox jumps over the lazy dog&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;Last-Save-Date&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;2010-09-08T16:15:14Z&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;custom:Version&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;1&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;Revision-&lt;span class=&quot;code-object&quot;&gt;Number&lt;/span&gt;&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;5&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;Last-Printed&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;1601-01-01T00:00:00Z&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;meta:print-date&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;1601-01-01T00:00:00Z&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;meta:creation-date&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;1601-01-01T00:00:00Z&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;dcterms:modified&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;2010-09-08T16:15:14Z&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;Template&quot;&lt;/span&gt; content=&quot;&quot;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;dc:creator&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;Nevin Nollop&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;meta:word-count&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;9&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;extended-properties:Company&quot;&lt;/span&gt; content=&quot;&quot;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;Last-Modified&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;2010-09-08T16:15:14Z&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;extended-properties:PresentationFormat&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;On-screen Show (4:3)&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;X-Parsed-By&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.tika.parser.DefaultParser&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;X-Parsed-By&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.tika.parser.microsoft.ooxml.OOXMLParser&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;modified&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;2010-09-08T16:15:14Z&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;xmpTPg:NPages&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;1&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;extended-properties:TotalTime&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;2&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;dc:publisher&quot;&lt;/span&gt; content=&quot;&quot;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;Content-Type&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;application/vnd.openxmlformats-officedocument.presentationml.presentation&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;Presentation-Format&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;On-screen Show (4:3)&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;title&amp;gt;The quick brown fox jumps over the lazy dog&amp;lt;/title&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;&amp;lt;p&amp;gt;The quick brown fox jumps over the lazy dog&amp;lt;/p&amp;gt;
&amp;lt;div class=&lt;span class=&quot;code-quote&quot;&gt;&quot;embedded&quot;&lt;/span&gt; id=&lt;span class=&quot;code-quote&quot;&gt;&quot;slide1_rId4&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;div class=&lt;span class=&quot;code-quote&quot;&gt;&quot;embedded&quot;&lt;/span&gt; id=&lt;span class=&quot;code-quote&quot;&gt;&quot;slide1_rId5&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;div class=&lt;span class=&quot;code-quote&quot;&gt;&quot;embedded&quot;&lt;/span&gt; id=&lt;span class=&quot;code-quote&quot;&gt;&quot;slide1_rId6&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;div class=&lt;span class=&quot;code-quote&quot;&gt;&quot;embedded&quot;&lt;/span&gt; id=&lt;span class=&quot;code-quote&quot;&gt;&quot;slide1_rId7&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;div class=&lt;span class=&quot;code-quote&quot;&gt;&quot;embedded&quot;&lt;/span&gt; id=&lt;span class=&quot;code-quote&quot;&gt;&quot;slide1_rId8&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;div class=&lt;span class=&quot;code-quote&quot;&gt;&quot;embedded&quot;&lt;/span&gt; id=&lt;span class=&quot;code-quote&quot;&gt;&quot;slide1_rId9&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;div class=&lt;span class=&quot;code-quote&quot;&gt;&quot;embedded&quot;&lt;/span&gt; id=&lt;span class=&quot;code-quote&quot;&gt;&quot;thumbnail_0.jpeg&quot;&lt;/span&gt;/&amp;gt;&amp;lt;div class=&lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt;-entry&quot;&lt;/span&gt;&amp;gt;&amp;lt;h1&amp;gt;thumbnail_0.jpeg&amp;lt;/h1&amp;gt;&amp;lt;/div&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The extracted plain text looks like this (using tika-app):&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;The quick brown fox jumps over the lazy dog






thumbnail_0.jpeg
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The fix is trivial - change the flag in AbstractOOXMLExtractor:158 to false.&lt;/p&gt;

&lt;p&gt;I think also that the id attribute should be set to the real thumbnail path within the package (i.e. tPart.getPartName().getName()) instead of the artificially created sequential name.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/AbstractOOXMLExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 19 Sep 2014 14:14:44 +0000" id="134" opendate="Thu, 18 Sep 2014 20:02:06 +0000">
		<buginformation>
			<summary>Add TikaConfigDumperExample to example package</summary>
			<description>&lt;p&gt;It would be useful to be able to dump a TikaConfig so that users who want to modify the xml config can have a basis from which to start.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-example/src/main/java/org/apache/tika/example/DumpTikaConfigExample.java</file>
			<file>/tika-app/src/test/resources/test-data/bad_xml.xml</file>
			<file>/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 24 Sep 2014 13:13:16 +0000" id="135" opendate="Fri, 19 Sep 2014 14:47:54 +0000">
		<buginformation>
			<summary>Upgrade to PDFBox 1.8.7</summary>
			<description>&lt;p&gt;Will run against govdocs1 early next week and then upgrade if no major regressions are found.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 24 Oct 2014 16:57:48 +0000" id="136" opendate="Sun, 21 Sep 2014 03:48:39 +0000">
		<buginformation>
			<summary>org.apache.tika.parser.mail.RFC822ParserTest fails</summary>
			<description>&lt;p&gt;I'm seeing test failures from:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;
Results :

Failed tests:   testMultipart(org.apache.tika.parser.mail.RFC822ParserTest): (..)

Tests run: 538, Failures: 1, Errors: 0, Skipped: 1

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;CentOS6 VM image, running:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;[mattmann@memex tika]$ java -version
java version &quot;1.7.0_67&quot;
Java(TM) SE Runtime Environment (build 1.7.0_67-b01)
Java HotSpot(TM) 64-Bit Server VM (build 24.65-b04, mixed mode)
[mattmann@memex tika]$ mvn -version
Apache Maven 3.2.1 (ea8b2b07643dbb1b84b6d16e1f08391b666bc1e9; 2014-02-14T09:37:52-08:00)
Maven home: /usr/share/apache-maven
Java version: 1.7.0_65, vendor: Oracle Corporation
Java home: /data/home/mattmann/dist/jdk1.7.0_65/jre
Default locale: en_US, platform encoding: UTF-8
OS name: &quot;linux&quot;, version: &quot;2.6.32-431.23.3.el6.centos.plus.x86_64&quot;, arch: &quot;amd64&quot;, family: &quot;unix&quot;
[mattmann@memex tika]$ 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here are the surefire reports - no clue what's up here:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;[mattmann@memex tika]$ more tika-parsers/target/surefire-reports/org.apache.tika.parser.mail.RFC822ParserTest.txt 
-------------------------------------------------------------------------------
Test set: org.apache.tika.parser.mail.RFC822ParserTest
-------------------------------------------------------------------------------
Tests run: 8, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.699 sec &amp;lt;&amp;lt;&amp;lt; FAILURE!
testMultipart(org.apache.tika.parser.mail.RFC822ParserTest)  Time elapsed: 0.152 sec  &amp;lt;&amp;lt;&amp;lt; FAILURE!
org.mockito.exceptions.verification.TooManyActualInvocations: 
xHTMLContentHandler.startElement(
    &quot;http://www.w3.org/1999/xhtml&quot;,
    &quot;div&quot;,
    &quot;div&quot;,
    isA(org.xml.sax.Attributes)
);
Wanted 4 times but was 5
	at org.apache.tika.parser.mail.RFC822ParserTest.testMultipart(RFC822ParserTest.java:87)
Caused by: org.mockito.exceptions.cause.UndesiredInvocation: 
Undesired invocation:
	at org.apache.tika.sax.ContentHandlerDecorator.startElement(ContentHandlerDecorator.java:126)
	at org.apache.tika.sax.SafeContentHandler.startElement(SafeContentHandler.java:264)
	at org.apache.tika.sax.XHTMLContentHandler.startElement(XHTMLContentHandler.java:254)
	at org.apache.tika.sax.ContentHandlerDecorator.startElement(ContentHandlerDecorator.java:126)
	at org.apache.tika.sax.xpath.MatchingContentHandler.startElement(MatchingContentHandler.java:60)
	at org.apache.tika.sax.ContentHandlerDecorator.startElement(ContentHandlerDecorator.java:126)
	at org.apache.tika.sax.ContentHandlerDecorator.startElement(ContentHandlerDecorator.java:126)
	at org.apache.tika.sax.ContentHandlerDecorator.startElement(ContentHandlerDecorator.java:126)
	at org.apache.tika.sax.ContentHandlerDecorator.startElement(ContentHandlerDecorator.java:126)
	at org.apache.tika.sax.SafeContentHandler.startElement(SafeContentHandler.java:264)
	at org.apache.tika.sax.XHTMLContentHandler.startElement(XHTMLContentHandler.java:254)
	at org.apache.tika.sax.XHTMLContentHandler.startElement(XHTMLContentHandler.java:284)
	at org.apache.tika.parser.ocr.TesseractOCRParser.extractOutput(TesseractOCRParser.java:243)
	at org.apache.tika.parser.ocr.TesseractOCRParser.parse(TesseractOCRParser.java:155)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:247)
	at org.apache.tika.parser.mail.MailContentHandler.body(MailContentHandler.java:102)
	at org.apache.james.mime4j.parser.MimeStreamParser.parse(MimeStreamParser.java:133)
	at org.apache.tika.parser.mail.RFC822Parser.parse(RFC822Parser.java:76)
	at org.apache.tika.parser.mail.RFC822ParserTest.testMultipart(RFC822ParserTest.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:236)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:134)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:113)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)
	at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)
	at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:103)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:74)

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/mail/RFC822ParserTest.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/ocr/TesseractOCRParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 29 Jan 2015 21:53:38 +0000" id="137" opendate="Mon, 22 Sep 2014 06:07:31 +0000">
		<buginformation>
			<summary>Build a parser to extract data from GRIB formats</summary>
			<description>&lt;p&gt;Arctic dataset contains a MIME format called GRIB -  General &lt;br/&gt;
Regularly­distributed information in Binary form &lt;a href=&quot;http://en.wikipedia.org/wiki/GRIB&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/GRIB&lt;/a&gt; . GRIB is a well known data format which is a concise data format used in meteorology to store historical and &lt;br/&gt;
weather data. There are 2 different types of the format ­ GRIB 0, GRIB 2.  The focus will be on GRIB 2 which is the most prevalent. Each GRIB record intended for either transmission or storage contains a single parameter with values located at an array of grid points, or represented as a set of spectral coefficients, for a single level (or layer), encoded as a continuous bit stream. Logical divisions of the record are designated as &quot;sections&quot;, each of which provides control information and/or data. A GRIB record consists of six sections, two of which are optional: &lt;/p&gt;

&lt;p&gt;(0) Indicator Section &lt;br/&gt;
(1) Product Definition Section (PDS) &lt;br/&gt;
(2) Grid Description Section (GDS) ­ optional &lt;br/&gt;
(3) Bit Map Section (BMS) ­ optional &lt;br/&gt;
(4) Binary Data Section (BDS) &lt;br/&gt;
(5) '7777' (ASCII Characters)&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/pom.xml</file>
			<file>/tika-parsers/pom.xml</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 24 Sep 2014 12:59:11 +0000" id="138" opendate="Mon, 22 Sep 2014 19:36:22 +0000">
		<buginformation>
			<summary>Clear PDFont's resources after each file to prevent memory leak</summary>
			<description>&lt;p&gt;PDFBox-2200 identified a memory-leak/caching strategy that can cause problems for some documents.  A workaround of clearing the cache was recommended for now.  Let's add that to Tika.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDFParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 1 Oct 2014 14:14:19 +0000" id="139" opendate="Wed, 24 Sep 2014 17:24:31 +0000">
		<buginformation>
			<summary>PDF Images don't appear in structured view</summary>
			<description>&lt;p&gt;When viewing, say, a Word Document, any images appear in the 'structured view' of the document as &amp;lt;img&amp;gt; tags. The same is not true of PDF documents, and we lose both the fact that there is an image present, and where it is in the document.&lt;/p&gt;

&lt;p&gt;Some discussion of this issue in the comments of &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1396&quot; title=&quot;Embedded images in PDF documents&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1396&quot;&gt;&lt;del&gt;TIKA-1396&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDF2XHTML.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 30 Sep 2014 01:43:30 +0000" id="140" opendate="Tue, 30 Sep 2014 01:40:04 +0000">
		<buginformation>
			<summary>Extract documents embedded within annotations in PDFs</summary>
			<description>&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sunxingzhe359&quot; class=&quot;user-hover&quot; rel=&quot;sunxingzhe359&quot;&gt;sunxingzhe&lt;/a&gt; raised this issue on &lt;a href=&quot;https://issues.apache.org/jira/browse/PDFBOX-2393&quot; title=&quot;PDF embeded with document can not parse.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PDFBOX-2393&quot;&gt;&lt;del&gt;PDFBOX-2393&lt;/del&gt;&lt;/a&gt; and submitted a test document; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tilman&quot; class=&quot;user-hover&quot; rel=&quot;tilman&quot;&gt;Tilman Hausherr&lt;/a&gt; offered example code on that issue.  I've used the test file and example code from &lt;a href=&quot;https://issues.apache.org/jira/browse/PDFBOX-2393&quot; title=&quot;PDF embeded with document can not parse.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PDFBOX-2393&quot;&gt;&lt;del&gt;PDFBOX-2393&lt;/del&gt;&lt;/a&gt; for this patch.  Thank you, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sunxingzhe359&quot; class=&quot;user-hover&quot; rel=&quot;sunxingzhe359&quot;&gt;sunxingzhe&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tilman&quot; class=&quot;user-hover&quot; rel=&quot;tilman&quot;&gt;Tilman Hausherr&lt;/a&gt;! &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDF2XHTML.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 11 Oct 2014 15:22:28 +0000" id="141" opendate="Fri, 10 Oct 2014 04:51:52 +0000">
		<buginformation>
			<summary>ExternalParsers should allow dynamic keys to be specified for Regexs</summary>
			<description>&lt;p&gt;While working on &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-605&quot; title=&quot;Tika GDAL parser&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-605&quot;&gt;&lt;del&gt;TIKA-605&lt;/del&gt;&lt;/a&gt;, I was trying to use ExternalParsers and I came across an interesting use case. What if there are so many met keys that specifying all of them by hand as individual regexs would be repetitive, and tedious. What if the met key itself could also be specified by a regex, e.g., we just take the first group to be the key, and then the next group would be the actual value? I ran across this in parsing GDAL output and so a very simple improvement to the ExternalParsers Map&amp;lt;Pattern, String&amp;gt; map would be to allow it to take e.g., null or &quot;&quot; Strings and then take that to mean that the Pattern specifies &lt;b&gt;both&lt;/b&gt; the key name &lt;b&gt;and&lt;/b&gt; the key value.&lt;br/&gt;
I've got a patch I'll upload all tests pass and I need this to get &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-605&quot; title=&quot;Tika GDAL parser&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-605&quot;&gt;&lt;del&gt;TIKA-605&lt;/del&gt;&lt;/a&gt; in and done.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 13 Oct 2014 10:17:45 +0000" id="142" opendate="Sat, 11 Oct 2014 13:50:10 +0000">
		<buginformation>
			<summary>Detection for VirtualPC VHD files</summary>
			<description>&lt;p&gt;Please, remove the &amp;lt;glob pattern=&quot;*.vhd&quot;/&amp;gt; entry from text/x-vhdl mimetype definition and add the following:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&amp;lt;mime-type type=&lt;span class=&quot;code-quote&quot;&gt;&quot;application/x-vhd&quot;&lt;/span&gt;&amp;gt;
	&amp;lt;glob pattern=&lt;span class=&quot;code-quote&quot;&gt;&quot;*.vhd&quot;&lt;/span&gt;/&amp;gt;
	&amp;lt;magic priority=&lt;span class=&quot;code-quote&quot;&gt;&quot;50&quot;&lt;/span&gt;&amp;gt;
    	 	&amp;lt;match value=&lt;span class=&quot;code-quote&quot;&gt;&quot;conectix&quot;&lt;/span&gt; type=&lt;span class=&quot;code-quote&quot;&gt;&quot;string&quot;&lt;/span&gt; offset=&lt;span class=&quot;code-quote&quot;&gt;&quot;0&quot;&lt;/span&gt;/&amp;gt;
    	&amp;lt;/magic&amp;gt;
&amp;lt;/mime-type&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 8 Jan 2015 17:56:25 +0000" id="143" opendate="Sun, 12 Oct 2014 16:19:50 +0000">
		<buginformation>
			<summary>Figure out how to add Image metadata extraction to Tesseract parser</summary>
			<description>&lt;p&gt;Now that Tesseract is the default image parser in Tika for many image types, consider how to add back in the metadata extraction capabilities by the other Image parsers.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/config/TikaConfig.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/ocr/TesseractOCRParser.java</file>
			<file>/tika-server/src/test/java/org/apache/tika/server/TikaMimeTypesTest.java</file>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/ocr/TesseractOCRParserTest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 22 Oct 2014 00:37:09 +0000" id="144" opendate="Tue, 21 Oct 2014 03:20:44 +0000">
		<buginformation>
			<summary>Add Recursive Metadata Parser Wrapper output to tika-app and gui</summary>
			<description>&lt;p&gt;It would be helpful to expose the output of the recursive metadata parser wrapper in the gui and in the command line for tika-app.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 27 Oct 2014 17:04:27 +0000" id="145" opendate="Mon, 27 Oct 2014 16:59:23 +0000">
		<buginformation>
			<summary>Fix write limit bug in BasicContentHandlerFactory for BodyContentHandler</summary>
			<description>&lt;p&gt;Small bug in BasicContentHandlerFactory that does not properly set write limit to -1 when creating a BodyContentHandler.  The result is that BodyContentHandler falls back to default of 100k.  &lt;/p&gt;

&lt;p&gt;Thank you, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tilman&quot; class=&quot;user-hover&quot; rel=&quot;tilman&quot;&gt;Tilman Hausherr&lt;/a&gt;, for your collaboration, without which I wouldn't have found this for a good while!&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/main/java/org/apache/tika/gui/TikaGUI.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 29 Oct 2014 19:23:54 +0000" id="146" opendate="Wed, 29 Oct 2014 10:15:36 +0000">
		<buginformation>
			<summary>Bad mime detection of certain JAR file</summary>
			<description>&lt;p&gt;Given this &quot;ordinary&quot; Java JAR file&lt;br/&gt;
&lt;a href=&quot;https://maven.atlassian.com/content/groups/public/com/atlassian/support/healthcheck/support-healthcheck-plugin/1.0.3/support-healthcheck-plugin-1.0.3.jar&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://maven.atlassian.com/content/groups/public/com/atlassian/support/healthcheck/support-healthcheck-plugin/1.0.3/support-healthcheck-plugin-1.0.3.jar&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Manually inspected and tested it, it is a Jar file and is valid one.&lt;/p&gt;

&lt;p&gt;Still, Tika Core's Detector detects it as type &lt;tt&gt;application/x-msdownload; format=pe&lt;/tt&gt;. Tthe detection is &quot;hinted&quot; with file name, hence &quot;jar&quot; hint is present, still it's not detected as desired &lt;tt&gt;application/java-archive&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;IMO, this happens due to the problem with priority of &lt;tt&gt;application/x-msdownload; format=pe&lt;/tt&gt;, which is 55. If it would be 50, the &quot;mediation&quot; would kick in, see &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1292&quot; title=&quot;Inconsistent priorities in bundled tika-mimetypes.xml&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1292&quot;&gt;&lt;del&gt;TIKA-1292&lt;/del&gt;&lt;/a&gt;. &lt;/p&gt;

&lt;p&gt;Changing/overriding magic priority is not possible using &lt;tt&gt;custom-mimetypes.xml&lt;/tt&gt; is also not possible.&lt;/p&gt;

&lt;p&gt;Unsure what the correct solution is here, nor how to circumvent this without patching Tika.&lt;/p&gt;

&lt;p&gt;The problem affects versions 1.5 but also 1.6, but we target 1.6.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 3 Nov 2014 16:30:01 +0000" id="147" opendate="Mon, 3 Nov 2014 15:32:24 +0000">
		<buginformation>
			<summary>TesseractOCRParser does not work in Windows</summary>
			<description>&lt;p&gt;STR:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Case 1:
	&lt;ul&gt;
		&lt;li&gt;Setting tesseractPath to a common installation path of Tesseract:  C:\Program Files (x86)\Tesseract-OCR&lt;/li&gt;
		&lt;li&gt;the checking available Tesseract command returns always false&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Case 2:
	&lt;ul&gt;
		&lt;li&gt;Even setting to no space value in tesseractPath, says C:\Tesseract-OCR&lt;/li&gt;
		&lt;li&gt;the checking &amp;amp; running command of tesseract on Windows is not correct: C:\Tesseract-OCR\tesseract, it must be C:\Tesseract-OCR\tesseract.exe&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/parser/external/ExternalParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 5 Nov 2014 09:45:30 +0000" id="148" opendate="Mon, 3 Nov 2014 19:25:01 +0000">
		<buginformation>
			<summary>Implement extraction of non-global variables from netCDF3 and netCDF4</summary>
			<description>&lt;p&gt;Speaking to Eric Nienhouse at the ongoing NSF funded Polar Cyberinfrastructure hackathon in NYC, we became aware that variables parameters contained within netCDF3 and netCDF4 are just as valuable (if not more valuable) as global attribute values. &lt;br/&gt;
AFAIK, right now we only extract global attributes however we could extend the support to cater for the above observations.  &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/netcdf/NetCDFParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 5 Nov 2014 09:45:30 +0000" id="149" opendate="Fri, 7 Nov 2014 17:44:47 +0000">
		<buginformation>
			<summary>pdf:encrypted:false with encrypted pdf</summary>
			<description>&lt;p&gt;When extracting metadata from the encryption_noprinting.pdf file found in the pdfCabinetOfHorrors (&lt;a href=&quot;https://github.com/openplanets/format-corpus/tree/master/pdfCabinetOfHorrors&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/openplanets/format-corpus/tree/master/pdfCabinetOfHorrors&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;$java -jar tika-app-1.7-20141105.092424-471.jar -j encryption_noprinting.pdf&lt;/p&gt;

&lt;p&gt;We get a &lt;br/&gt;
INFO - Document is encrypted&lt;/p&gt;

&lt;p&gt;but the resulting JSON has : &quot;pdf:encrypted&quot;:&quot;false&quot;&lt;/p&gt;

&lt;p&gt;Looking at the PDFParser, it seems that the first information comes when reading the PDF but when the metadata is retrieve the PDF is no longer encrypted... the encryption fact should be retain to be added to the metadata.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDFParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 13 Nov 2014 21:09:31 +0000" id="150" opendate="Thu, 13 Nov 2014 20:55:43 +0000">
		<buginformation>
			<summary>Reformat pom.xml files</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 16 Nov 2014 17:41:18 +0000" id="151" opendate="Sun, 16 Nov 2014 17:26:02 +0000">
		<buginformation>
			<summary>Allow TesseractOCRParser to be configured using an external configuration file</summary>
			<description>&lt;p&gt;The TesseractOCRParser is great but configuration at the moment requires configuring up a &lt;em&gt;TesseractOCRConfig&lt;/em&gt; instance and placing it on the &lt;em&gt;ParseContext&lt;/em&gt;.  For those who are not using Tika programmatically in their code, such as users of the Tika Server, this is difficult and requires code changes. It would be more helpful is this could also be configured using a properties file on the classpath.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/ocr/TesseractOCRConfig.java</file>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/ocr/TesseractOCRConfigTest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 20 Nov 2014 13:30:52 +0000" id="152" opendate="Mon, 17 Nov 2014 16:12:40 +0000">
		<buginformation>
			<summary>Add custom header processing to allow overriding of OCR and PDF configuration to be used in Tika Server</summary>
			<description>&lt;p&gt;The &lt;em&gt;TesseractOCRParser&lt;/em&gt; and &lt;em&gt;PDFParser&lt;/em&gt; provide different configuration options via their dedicated config classes (&lt;em&gt;TesseractOCRConfig&lt;/em&gt; and &lt;em&gt;PDFParserConfig&lt;/em&gt;). The settings these provide can be configured by creating an instance of the class and setting on the &lt;em&gt;ParseContext&lt;/em&gt; used during parsing.&lt;/p&gt;

&lt;p&gt;Whilst these can be set globally in configuration files via the classpath, it would also be good to allow these to be overridden for individual requests using custom HTTP Headers.&lt;/p&gt;

&lt;p&gt;It is proposed these are essentially made up of the following:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;X-Tika-OCR&amp;lt;Property Name&amp;gt; for &lt;em&gt;TesseractOCRConfig&lt;/em&gt;&lt;/li&gt;
	&lt;li&gt;X-Tika-PDF&amp;lt;Property Name&amp;gt; for &lt;em&gt;PDFParserConfig&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;For example, to set the language for the OCR parser you could send:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;curl -T /path/to/somefile.pdf http://localhost:9998/tika --header &quot;X-Tika-OCRLanguage: fra&quot;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Or to ask the PDF Parser to extract inline images you could send:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;curl -T /path/to/somefile.pdf http://localhost:9998/tika --header &quot;X-Tika-PDFExtractInlineImages: true&quot;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Properties set that do not exist would raise an HTTP 500 error.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/ocr/TesseractOCRConfig.java</file>
			<file>/tika-server/src/main/java/org/apache/tika/server/TikaResource.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 20 Nov 2014 18:49:36 +0000" id="153" opendate="Tue, 25 Nov 2014 15:00:41 +0000">
		<buginformation>
			<summary>Minor issues with the Tika MIME type magic file</summary>
			<description>&lt;p&gt;I've started running some routine tests on format information held in a number of tools, including &lt;a href=&quot;http://www.digipres.org/formats/sources/tika/issues/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;Tika&lt;/a&gt;. This uncovered a number of minor issues when working with the tika-mimetypes.xml file:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Duplicate MIME type application/gzip-compressed for type application/gzip.&lt;/li&gt;
	&lt;li&gt;Duplicate MIME type image/vnd.dwg for type image/vnd.dwg.&lt;/li&gt;
	&lt;li&gt;Error when parsing XML: Namespace prefix tika on link is not defined, line 169, column 15&lt;/li&gt;
	&lt;li&gt;Format application/dita+xml;format=task has itself as a supertype!&lt;/li&gt;
	&lt;li&gt;Glob '^owl$' for entry application/rdf+xml does not appear to be a valid filename specification.&lt;/li&gt;
	&lt;li&gt;Glob '^rdf$' for entry application/rdf+xml does not appear to be a valid filename specification.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;With the last two, it's really a matter of consistency. The other full-filename globs do &lt;b&gt;not&lt;/b&gt; use the ^ and $ start and end markers, but owl and rdf do.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 27 Nov 2014 13:44:36 +0000" id="154" opendate="Tue, 25 Nov 2014 16:50:15 +0000">
		<buginformation>
			<summary>Add mime for pre-OLE2 xls file</summary>
			<description>&lt;p&gt;On the govdocs1 corpus, nearly 91% of xls exceptions have this stacktrace:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Caused by: java.io.IOException: Invalid header signature; read 0x0010000000060409, expected 0xE11AB1A1E011CFD0 - Your file appears not to be a valid OLE2 document at org.apache.poi.poifs.storage.HeaderBlock.&amp;lt;init&amp;gt;(HeaderBlock.java:140) at org.apache.poi.poifs.storage.HeaderBlock.&amp;lt;init&amp;gt;(HeaderBlock.java:115) at org.apache.poi.poifs.filesystem.NPOIFSFileSystem.&amp;lt;init&amp;gt;(NPOIFSFileSystem.java:198) at org.apache.poi.poifs.filesystem.NPOIFSFileSystem.&amp;lt;init&amp;gt;(NPOIFSFileSystem.java:184) at org.apache.tika.parser.microsoft.OfficeParser.parse(OfficeParser.java:162) at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242) ... 13 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Excel is able to open the few files that I tried, and it looks like Excel thinks these are version 4.&lt;/p&gt;

&lt;p&gt;On the POI user list, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gagravarr&quot; class=&quot;user-hover&quot; rel=&quot;gagravarr&quot;&gt;Nick Burch&lt;/a&gt; identified this header as pre-OLE2 and asked that we add the mime to Tika so that we can handle appropriately.  Test file soon to be attached.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
			<file>/tika-parsers/src/test/resources/test-documents/testEXCEL_4.xls</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 26 Nov 2014 16:12:41 +0000" id="155" opendate="Tue, 25 Nov 2014 17:27:58 +0000">
		<buginformation>
			<summary>Add X-Tika- as &quot;namespace&quot; for info from Tika about the parsing process</summary>
			<description>&lt;p&gt;As we discuss moving into more complex combinations of parsers, there may be a usefulness for Tika to report information about the parsing process via the Metadata object.  There are a few areas where we're already doing this, such as the &quot;X-ParsedBy&quot; key or a few others that I've added in the RTFParser, the PDFParser and the RecursiveParserWrapper.&lt;/p&gt;

&lt;p&gt;I'm opening this issue for discussion and input from the community.&lt;/p&gt;

&lt;p&gt;Proposal 1: We add a prefix String &quot;X-TIKA-&quot; to TikaCoreProperties that parsers should use as a property prefix when they want to report something about the parsing process via the Metadata object, e.g.:&lt;/p&gt;

&lt;p&gt;X-TIKA-millisToParse&lt;br/&gt;
X-TIKA-embeddedDocException&lt;/p&gt;

&lt;p&gt;Proposal 2: We start adding specific properties to TikaCoreProperties or maybe create a TikaMetaMetaProperties class to store these. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;Do we want to do something like this?  Is there a better mechanism?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/test/java/org/apache/tika/cli/TikaCLITest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 22 Dec 2014 05:59:14 +0000" id="156" opendate="Fri, 28 Nov 2014 09:30:55 +0000">
		<buginformation>
			<summary>Basic parser for old Excel files (eg Excel 4)</summary>
			<description>&lt;p&gt;In &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1487&quot; title=&quot;Add mime for pre-OLE2 xls file&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1487&quot;&gt;&lt;del&gt;TIKA-1487&lt;/del&gt;&lt;/a&gt;, we added mime magic for the pre-OLE2 excel file formats. Based on the reading of the OpenOffice Excel docs for that, it looks like it should be possible to produce a basic parser to extract key bits of info (eg strings) from these older file formats. &lt;/p&gt;

&lt;p&gt;This would likely largely be done by having a custom record iterator for the older formats, then passing the handful of &quot;interesting&quot; records to POI's record classes (maybe with some tweaks for the older formats) to have the binary data parsed, then returned by the parser&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/OldExcelParser.java</file>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/microsoft/AbstractPOIContainerExtractionTest.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 12 Dec 2014 12:52:54 +0000" id="157" opendate="Mon, 8 Dec 2014 14:10:38 +0000">
		<buginformation>
			<summary>Identification of BPG (Better Portable Graphics) format</summary>
			<description>&lt;p&gt;Following the release of the &lt;a href=&quot;http://bellard.org/bpg/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;BPG (Better Portable Graphics)&lt;/a&gt; format a few days ago, I created the magic definitions for this format. See attached file. I've tested this against the example files listed &lt;a href=&quot;http://fileformats.archiveteam.org/wiki/BPG&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Note that no officially registered MIME type exists for this format. I picked &lt;tt&gt;image/bpg&lt;/tt&gt; as this would be the most obvious choice, but I don't know if Tika sticks to any policy in such cases?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
			<file>/tika-parsers/src/test/java/org/apache/tika/mime/TestMimeTypes.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 15 Dec 2014 06:27:45 +0000" id="158" opendate="Fri, 12 Dec 2014 09:13:20 +0000">
		<buginformation>
			<summary>JAXRS server: allow passing PDF password in the request</summary>
			<description>&lt;p&gt;I have to extract content from encrypted PDFs. Setting the PDF password using the TIKA_PASSWORD environment variable works, however it only allows for one PDF password.&lt;/p&gt;

&lt;p&gt;It would be very useful to be able to pass the password in during the HTTP request - as a header or by some other means. This way I can run a central JAXRS server and extract content from all PDFs, rather than a separate server for each department/group that has its own password.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-server/src/test/java/org/apache/tika/server/MetadataResourceTest.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/AbstractPOIFSExtractor.java</file>
			<file>/tika-server/src/main/java/org/apache/tika/server/TikaResource.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 19 Dec 2014 03:15:29 +0000" id="159" opendate="Thu, 18 Dec 2014 14:49:37 +0000">
		<buginformation>
			<summary>tika-server cannot output JSON</summary>
			<description>&lt;p&gt;I would like the response from &lt;br/&gt;
curl -X PUT -T /path/to/file.pdf &lt;a href=&quot;http://localhost:9998/meta&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://localhost:9998/meta&lt;/a&gt;&lt;br/&gt;
to be JSON and not CSV?.&lt;/p&gt;

&lt;p&gt;I've discovered JSONMessageBodyWriter.java (&lt;a href=&quot;https://github.com/apache/tika/blob/af19f3ea04792cad81b428f1df9f5ebbb2501913/tika-server/src/main/java/org/apache/tika/server/JSONMessageBodyWriter.java&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/tika/blob/af19f3ea04792cad81b428f1df9f5ebbb2501913/tika-server/src/main/java/org/apache/tika/server/JSONMessageBodyWriter.java&lt;/a&gt;) so I think the functionality is present, tried adding --header &quot;Accept: application/json&quot; to the cURL call, in line with the documentation for outputting CSV, but no luck so far.&lt;/p&gt;

&lt;p&gt;According to &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sergey_beryozkin&quot; class=&quot;user-hover&quot; rel=&quot;sergey_beryozkin&quot;&gt;Sergey Beryozkin&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&quot;I see MetadataResource returning StreamingOutput and it has @Produces(text/csv) only. As such this MBW has no effect at the moment.&lt;/p&gt;

&lt;p&gt;We can update MetadataResource to return Metadata directly if application/json is requested or update MetadataResource to directly convert Metadata to JSON in case of JSON being accepted.&quot;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-server/pom.xml</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 19 Dec 2014 02:08:23 +0000" id="160" opendate="Thu, 18 Dec 2014 18:07:12 +0000">
		<buginformation>
			<summary>Add RecursiveParserWrapper to tika-server</summary>
			<description>&lt;p&gt;In &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1451&quot; title=&quot;Add Recursive Metadata Parser Wrapper output to tika-app and gui&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1451&quot;&gt;&lt;del&gt;TIKA-1451&lt;/del&gt;&lt;/a&gt;, we added Jukka/Nick's Recursive Parser Wrapper to tika-app.  Let's add that format of output to tika-server.&lt;/p&gt;

&lt;p&gt;What should we call the endpoint: rmeta?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-server/src/main/java/org/apache/tika/server/TikaServerCli.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 19 Dec 2014 15:47:25 +0000" id="161" opendate="Fri, 19 Dec 2014 14:01:25 +0000">
		<buginformation>
			<summary>Let's fold MetadataEP's capabilities into MetadataResource in tika-server</summary>
			<description>&lt;p&gt;MetadataEP (&quot;/metadata&quot;) has three capabilities:&lt;/p&gt;

&lt;p&gt;1) JSON view of metadata&lt;br/&gt;
2) String value for a user requested metadata field&lt;br/&gt;
3) A reduced metadata object that only includes the values of a user-requested field&lt;/p&gt;

&lt;p&gt;Given that don't seem to have loaded MetadataEP into tika-server cli, I don't think that functionality worked.  I could be very wrong, though, about its use!  Please let me know.&lt;/p&gt;

&lt;p&gt;MetadataResource (&quot;/meta&quot;) now handles 1), and I propose folding 2) and 3) into MetadataResource and deleting MetadataEP.&lt;/p&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 24 Dec 2014 07:28:50 +0000" id="162" opendate="Sun, 21 Dec 2014 11:34:38 +0000">
		<buginformation>
			<summary>FeedParser extracts XML markup with BodyContentHandler</summary>
			<description>&lt;p&gt;I am using FeedParser to extract text and links from feeds and have discovered, that the extracted text contains XML markup.&lt;br/&gt;
Usually FeedParser strips markup from text when generating SAX events,&lt;br/&gt;
but one line is missing it.&lt;br/&gt;
The fix is trivial. I will provide a patch.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 23 Dec 2014 06:46:25 +0000" id="163" opendate="Tue, 23 Dec 2014 02:25:18 +0000">
		<buginformation>
			<summary>Mime magic for database file formats</summary>
			<description>&lt;p&gt;I noticed today that Tika can't detect a lot of common database formats, such as sqlite or Berkeley DB or MISAM&lt;/p&gt;

&lt;p&gt;The unix file utility got most of those, which makes me think that there's a sensible-ish header on most we can write some mime magic for&lt;/p&gt;

&lt;p&gt;It'd therefore be good to add mime entries, with magic where possible, for many of these common database file formats&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 6 Jan 2015 01:40:32 +0000" id="164" opendate="Mon, 5 Jan 2015 13:51:13 +0000">
		<buginformation>
			<summary>OutlookPSTParser not closing PSTFile's InputStream, causing exception when called by AutoDetectParser</summary>
			<description>&lt;p&gt;On Windows, PSTFile can leave its InputStream open, which can prevent AutoDetectParser from deleting the temporary file.&lt;/p&gt;

&lt;p&gt;Let's make sure to close the InputStream.&lt;/p&gt;

&lt;p&gt;I view this as a blocker on 1.7 because this prevents parsing of all PST files on Windows.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 20 Apr 2015 11:25:34 +0000" id="165" opendate="Sun, 11 Jan 2015 21:29:26 +0000">
		<buginformation>
			<summary>Create a parser for SQLite3</summary>
			<description>&lt;p&gt;I think it would be very useful, as sqlite is used as data storage by a wide range of applications. Opening the ticket to track it. &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/LICENSE.txt</file>
			<file>/tika-parsers/pom.xml</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 2 May 2015 05:00:08 +0000" id="166" opendate="Thu, 15 Jan 2015 00:44:57 +0000">
		<buginformation>
			<summary>MIME type selection with probability</summary>
			<description>&lt;p&gt;Improvement and intuition&lt;br/&gt;
The original implementation for MIME type selection/detection is a bit less flexible by initial design, as it heavily relies on the outcome produced by magic-bytes MIME Type identification; Thus e.g. if magic-bytes is applicable in a file, Tika will follow the file type detected by magic-bytes. It may be better to provide more control over the method of choice.&lt;/p&gt;

&lt;p&gt;This proposed approach slightly incorporate the Bayesian probability theorem, where users are able to assign weights to each approach in terms of probability, so they have the control over preference of which file type or mime type identification methods implemented/available in Tika, and currently there are 3 methods for identifying MIME type in Tika (i.e. Magic-Bytes, File extension and Metadata content-type hint). By introducing some weights on the approach in the proposed approach, users are able to choose which method they trust most, the magic-bytes method is often trust-worthy though. But the virtue is that in some situations, file type identification must be sensitive, some might want all of the MIME type identification methods to agree on the same file type before they start processing those files, incorrect file type identification is less intolerable. The current implementation seems to be less flexible for this purpose and heavily rely on the Magic-bytes file identification method (although magic-bytes is most reliable compared to the other 2 ); &lt;/p&gt;


&lt;p&gt;Proposed design:&lt;br/&gt;
The idea of selection is to incorporate probability as weights on each MIME type identification method currently being implemented in Tika (they are Magic bytes approach, file extension match and metadata content-type hint).&lt;/p&gt;

&lt;p&gt;for example,&lt;br/&gt;
as an user, i would probably like to assign the the preference to the method based on the degree of the trust, and order the results if they don't coincide.&lt;br/&gt;
Bayesian rule may be a bit appropriate here to meet the intuition.&lt;br/&gt;
The following is what are needed for Bayesian rule implementation.&lt;/p&gt;

&lt;p&gt;&amp;gt; Prior probability P(file_type) e.g. P(pdf), theoretically this is computed based on the samples, and this depends on the domain or use cases, intuitively we more care the orders of the weights or probability of the results rather than the actual numbers, and also the context of Prior depends on samples for a particular use case or domain, e.g. if we happen to crawl a website that contains mostly the pdf files, we probably can collect some samples and compute the prior, based on the samples we can say 90% of docs are pdf, so our prior is defined to be P(pdf) = 0.9, but here we propose to define the prior as configurable param for users, and by default we leave the prior to be &quot;unapplicable&quot;. Alternatively, we can define prior for each file type to be  1/&lt;span class=&quot;error&quot;&gt;&amp;#91;number of supported file types in Tika&amp;#93;&lt;/span&gt; I think the number would be approximately 1/1157 and using this number seems to be more fair, but the point of avoiding it is that this prior is fixed for every type, and eventually we care more the orders of the result and if the number is fixed, so will the order be, bringing this number of 1/1157 into the Bayesian equation will not only be unable to affect the order but also it will lumber our implementation with extra computation, thus we will leave it as &quot;unapplicable&quot; which means we assign 1 to it as it never exists! but note we care more the order rather the actual number, and this param is configurable, and we believe it provides much flexibilities in some use cases.&lt;/p&gt;


&lt;p&gt;&amp;gt; Conditional probability of positive tests given a file type P(test| file_type) e.g. P(test1 = pdf | pdf), this probability is also based on collection of samples and domain or use cases, we leave it configurable, but based on our intuition we think test1(i.e. Magic-bytes method) is most trustworthy, thus the default value is 0.75 for P(test1 = a_file_type | a_file_type), this is to say given the file whose type is &quot;a file type&quot;, the probability of the test1 predicting the file is &quot;a_file_type&quot; is 0.75, that is really our intuition, as we trust test1 most, next we propose to use 0.7 for test3, and 0.65 for test2;&lt;br/&gt;
(note again, test1 = magic-bytes, test2 = file extension, test3 = Metadata Content-type hint)&lt;/p&gt;

&lt;p&gt;&amp;gt; Conditional probability of negative tests also need to be intuitively defined.&lt;br/&gt;
E.g. By default, given a file type that is not pdf, the probability of test1 predicting it is pdf is 1-P(test1 = pdf | pdf), thus P(test1=pdf | ~pdf) = 1- 0.75 = 0.25, as we trust the test1 the most, the other tests are defined with 0.35 and 0.3 respectively with the same intuition.&lt;/p&gt;


&lt;p&gt;&amp;gt;&amp;gt; The goal is to find out &lt;br/&gt;
P(file_type | test1 = file_type, test2=file_type, test3=file_type)&lt;/p&gt;

&lt;p&gt;(Please note, we are mostly interested in the order of choice rather than the explicit computation, we selectively drop some of the parameters used in Bayesian rule. Those are not considered will by default be set to 1 .)&lt;/p&gt;

&lt;p&gt;For example, given a file the 3 tests have predicted as follows&lt;br/&gt;
test1 = pdf&lt;br/&gt;
test2 = pdf&lt;br/&gt;
test3 = pdf&lt;/p&gt;

&lt;p&gt;prior: P(pdf) = 1 and P(~pdf) = 1 (meaning they are not applicable )&lt;br/&gt; P(test1=pdf|pdf) = 0.75&lt;br/&gt; P(test2=pdf|pdf) =0.65&lt;br/&gt; P(test3=pdf|pdf) = 0.7&lt;br/&gt;
With the same concept or intuition, we have the negative conditional probability by default&lt;br/&gt; P(test1=pdf|~pdf) = 0.25&lt;br/&gt; P(test2=pdf|~pdf) =0.35&lt;br/&gt; P(test3=pdf|~pdf) = 0.3&lt;/p&gt;

&lt;p&gt;Then we ready to compute.&lt;br/&gt;
Our goal is P(pdf|test1=pdf, test2=pdf, test3=pdf)&lt;/p&gt; &lt;p&gt;P(pdf|test1=pdf, test2=pdf, test3=pdf) = &lt;span class=&quot;error&quot;&gt;&amp;#91;P(pdf) * P(test1=pdf|pdf) * P(test2=pdf|pdf) * P(test3=pdf|pdf)&amp;#93;&lt;/span&gt;/total probability &lt;br/&gt;
where &lt;br/&gt;
total probability = P(pdf) * P(test1=pdf|pdf) * P(test2=pdf|pdf) * P(test3=pdf|pdf) + P(~pdf) P(test1=pdf|~pdf) * P(test2=pdf|~pdf) * P(test3=pdf|~pdf) = 0.3675&lt;/p&gt;

&lt;p&gt;Thus, &lt;br/&gt; P(pdf|test1=pdf, test2=pdf, test3=pdf) = 0.92857&lt;/p&gt;

&lt;p&gt;---------------------------------------------------------------------------------&lt;br/&gt;
example 2&lt;br/&gt; test1=pdf&lt;br/&gt; test2=txt&lt;br/&gt; test3=txt&lt;/p&gt;

&lt;p&gt;In this example, test2 and test3 does not agree test1.&lt;br/&gt;
So we have 2 types to compare, let's compute the 2 file type probabilities with conditions on those test results.&lt;/p&gt;

&lt;p&gt;for simplicity, &lt;br/&gt; test1=pdf, i will write test1+ &lt;br/&gt;
&amp;gt; pdf&lt;br/&gt;
P(&amp;#43;|test1+, test2-, test3-) &lt;br/&gt; = &lt;span class=&quot;error&quot;&gt;&amp;#91;P(+)P(test1+|pdf)*P(test2-|pdf)*P(test3-|pdf)&amp;#93;&lt;/span&gt;/total probability&lt;br/&gt; = 0.40909&lt;/p&gt;

&lt;p&gt;&amp;gt;text&lt;br/&gt;
P(&amp;#43;|test1-, test2+, test3+)  = 0.590909&lt;/p&gt;

&lt;p&gt;---------------------------------------------------------------------------------&lt;br/&gt;
example 3&lt;br/&gt; test1=pdf&lt;br/&gt; test2=txt&lt;br/&gt; test3=uc&lt;/p&gt;

&lt;p&gt;&amp;gt; pdf &lt;br/&gt;
P(&amp;#43;|test1+, test2-, test3-)  = 0.40909&lt;/p&gt;

&lt;p&gt;&amp;gt;txt&lt;br/&gt;
P(&amp;#43;|test1-, test2+, test3-) = 0.20968&lt;/p&gt;

&lt;p&gt;&amp;gt;nc&lt;br/&gt;
P(&amp;#43;|test1-, test2-, test3+)  = 0.29518&lt;/p&gt;


&lt;p&gt;---------------------------------------------------------------------------------&lt;br/&gt;
Since we are more interested in the weight order in a way we prefer to put more weight on the methods with higher preference, we can further simplify computation by ignoring the probability of the tests that have negative prediction.&lt;/p&gt;

&lt;p&gt;Consider the example 3 above, &lt;/p&gt;

&lt;p&gt;&amp;gt; pdf &lt;br/&gt;
P(&amp;#43;|test1+, test2-, test3-)  = 0.40909&lt;br/&gt;
we can ignore probability of test2- and test3-, as we are more interested in the order of preference;&lt;br/&gt;
the equation can be rewriten as follows&lt;br/&gt;
P(&amp;#43;|test1+) = 0.75/(0.75+0.35) = 0.75 where the total probability becomes 1, note prior is set to 1 by default for simplicity too.&lt;br/&gt;
Similarly, we have &lt;br/&gt;
&amp;gt;txt&lt;br/&gt;
P(&amp;#43;|test2+) = 0.65&lt;/p&gt; &lt;p&gt;&amp;gt;uc&lt;br/&gt; P(&amp;#43;|test3+)=0.7&lt;/p&gt;

&lt;p&gt;This follows the initial intuitive assumption and the intuitive order is also preserved. &lt;/p&gt;

&lt;p&gt;some of the parameters are being left out for computation simplicity by default, but the goal is to provide a way thru which users are able to control which method they want to use with probability weights, and this also provides some rooms or flexibility for more MIME detection algorithms. &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/detect/DefaultDetector.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 11 Apr 2015 00:47:56 +0000" id="167" opendate="Thu, 15 Jan 2015 18:03:33 +0000">
		<buginformation>
			<summary>Don't allow whatever is in http-equiv Content-Type to overwrite actual Content-Type in HtmlParser</summary>
			<description>&lt;p&gt;The HtmlParser will overwrite the value of Content-Type in Metadata with any value of content in an http-equiv=Content-Type header, e.g.&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;&amp;lt;meta http-equiv=Content-Type content=&quot;blah de blah blah&quot;&amp;gt;&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;.&lt;/p&gt;

&lt;p&gt;or even worse, perhaps:&lt;br/&gt;
&amp;lt;meta http-equiv=Content-Type content=&quot;application/pdf&quot;&amp;gt;&lt;/p&gt;

&lt;p&gt;Let's capture the content type alleged by the html file in a different key from Content-Type; I'd prefer to reserve Content-Type for &quot;text/html; charset=X&quot;.&lt;/p&gt;

&lt;p&gt;Candidate key/Property: Content-Type-Meta-HTTP-Equiv?&lt;/p&gt;

&lt;p&gt;See &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1514&quot; title=&quot;http-equiv content-type extraction should pick first parseable content value &quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1514&quot;&gt;&lt;del&gt;TIKA-1514&lt;/del&gt;&lt;/a&gt; for example output.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/metadata/TikaCoreProperties.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 11 Feb 2015 13:00:37 +0000" id="168" opendate="Fri, 6 Feb 2015 14:17:06 +0000">
		<buginformation>
			<summary>empty lines are not preserved</summary>
			<description>&lt;p&gt;I'm trying to extract the text content from RTF documents. The files contain empty lines (two or more consecutive paragraph-end marks), on which the further processing relies to tell apart different parts of the text. But unfortuantely Tika (with --text switch) eliminates all those empty lines, instead of preserving them.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/rtf/TextExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 13 Aug 2012 17:55:10 +0000" id="169" opendate="Thu, 3 Nov 2011 15:08:30 +0000">
		<buginformation>
			<summary>&quot;Hello, World!&quot; in UTF-8/ASCII gets detected as IBM500</summary>
			<description>&lt;p&gt;Looks like the encoding detection heuristics need some adjustment.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/txt/TXTParserTest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 9 Aug 2012 21:55:03 +0000" id="170" opendate="Wed, 7 Mar 2012 19:27:31 +0000">
		<buginformation>
			<summary>IdentityHtmlMapper.mapSafeElement() needs to return lower-cased incoming name</summary>
			<description>&lt;p&gt;Currently IdentityHtmlMapper.mapSafeElement(String name) just returns name as-is. This makes the XHTMLContentHandler think that it hasn't received a &amp;lt;body&amp;gt; tag, since it assumes input is lower-cased. So you get output that looks like:&lt;/p&gt;

&lt;p&gt;&amp;lt;body&amp;gt;&amp;lt;BODY/&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;&lt;/p&gt;

&lt;p&gt;The solution is a trivial change to lower-case the incoming name, the same as what the mapSafeAttribute() method is already doing.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/html/IdentityHtmlMapper.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 8 Mar 2018 22:56:15 +0000" id="171" opendate="Wed, 21 Mar 2012 13:44:02 +0000">
		<buginformation>
			<summary>Detection problem: message/rfc822 file is detected as text/plain.</summary>
			<description>&lt;p&gt;When using &lt;tt&gt;DefaultDetector&lt;/tt&gt; mime type for &lt;tt&gt;.eml&lt;/tt&gt; files is different (you can test it on &lt;tt&gt;testRFC822&lt;/tt&gt; and &lt;tt&gt;testRFC822_base64&lt;/tt&gt; in &lt;tt&gt;tika-parsers/src/test/resources/test-documents/&lt;/tt&gt;).&lt;/p&gt;

&lt;p&gt;Main reason for such behavior is that only magic detector is really works for such files. Even if you set &lt;tt&gt;CONTENT_TYPE&lt;/tt&gt; in metadata or some &lt;tt&gt;.eml&lt;/tt&gt; file name in &lt;tt&gt;RESOURCE_NAME_KEY&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;As I found &lt;tt&gt;MediaTypeRegistry.isSpecializationOf(&quot;message/rfc822&quot;, &quot;text/plain&quot;)&lt;/tt&gt; returns &lt;tt&gt;false&lt;/tt&gt;, so detection by &lt;tt&gt;MimeTypes.detect(...)&lt;/tt&gt; works only by magic.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 9 Aug 2012 21:58:27 +0000" id="172" opendate="Fri, 30 Mar 2012 18:52:42 +0000">
		<buginformation>
			<summary>XHTMLContentHandler wont emit newline when html element matches ENDLINE set</summary>
			<description>&lt;p&gt;XHTMLContentHandler.endElement checks if the element is in the ENDLINE set to see if it should emit a newline.  The html elements in ENDLINE are all lower case, but the HtmlParser class uses the XHTMLDowngradeHandler handler to upper case all html elements.  This means that none of the html elements in the web page will match the elements in the ENDLINE set.  &lt;/p&gt;

&lt;p&gt;This also is a problem with the INDENT set as well&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/html/HtmlParserTest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 8 Jul 2012 23:27:52 +0000" id="173" opendate="Wed, 11 Apr 2012 13:49:36 +0000">
		<buginformation>
			<summary>Tika does not use the HTML5 meta charset tag when determining charset</summary>
			<description>&lt;p&gt;HTML5 introduced a new way of specifying the charset for an HTML document:&lt;br/&gt;
&amp;lt;meta charset=&quot;utf-8&quot;/&amp;gt;&lt;/p&gt;

&lt;p&gt;The code in HtmlParser only looks for the HTML4 style meta http-equiv tag:&lt;br/&gt;
&amp;lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot; /&amp;gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/html/HtmlEncodingDetector.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 30 Jul 2012 18:19:45 +0000" id="174" opendate="Tue, 8 May 2012 14:51:59 +0000">
		<buginformation>
			<summary>Image geodata being rounded to integers</summary>
			<description>&lt;p&gt;This was initially reported as an Alfresco issue, &lt;a href=&quot;https://issues.alfresco.com/jira/browse/ALF-13004&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.alfresco.com/jira/browse/ALF-13004&lt;/a&gt;, but is actually a Tika problem. It seems that for some images, the geo metadata is being incorrectly rounded to an integer:&lt;/p&gt;

&lt;p&gt;   $ tika --metadata 2012-02-19\ 16.43.29.jpg | grep --text geo&lt;br/&gt;
   geo:lat: 51.0&lt;br/&gt;
   geo:long: -1.0&lt;/p&gt;

&lt;p&gt;The image was actually taken at (as extracted by exiftool)&lt;/p&gt;

&lt;p&gt;   $ exiftool 2012-02-19\ 16.43.29.jpg | grep GPS&lt;br/&gt;
   ....&lt;br/&gt;
   GPS Altitude                    : 295 m Above Sea Level&lt;br/&gt;
   GPS Date/Time                   : 2012:02:20 16:44:22Z&lt;br/&gt;
   GPS Latitude                    : 51 deg 34' 32.74&quot; N&lt;br/&gt;
   GPS Longitude                   : 1 deg 34' 4.39&quot; W&lt;br/&gt;
   GPS Position                    : 51 deg 34' 32.74&quot; N, 1 deg 34' 4.39&quot; W&lt;/p&gt;

&lt;p&gt;The sample file for this example is available at &amp;lt;&lt;a href=&quot;https://issues.alfresco.com/jira/secure/attachment/29236/2012-02-19+16.43.29.jpg&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.alfresco.com/jira/secure/attachment/29236/2012-02-19+16.43.29.jpg&lt;/a&gt;&amp;gt;. We do have the OK to use the photo in a test suite, but it's possibly a bit big as-is so we may need to resize it whilst preserving the exif data for a unit test.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/image/ImageMetadataExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 3 Jul 2012 03:24:01 +0000" id="175" opendate="Tue, 22 May 2012 01:56:10 +0000">
		<buginformation>
			<summary>Consolidation of Some Tika Core Properties</summary>
			<description>
&lt;p&gt;There are a few properties in TikaCoreProperties which overlap and I think we should minimize ambiguity by consolidating them into a single composite property with the clearest name, the most general specification referenced as its primary property, and the others and deprecated strings as its secondaries.&lt;/p&gt;

&lt;p&gt;Here's the proposed pseudo-code for the changes:&lt;/p&gt;

&lt;p&gt;Remove TikaCoreProperties.SUBJECT&lt;br/&gt;
TikaCoreProperties.KEYWORDS &amp;lt;- DublinCore.SUBJECT, &lt;/p&gt;
{ Office.KEYWORDS, MSOffice.KEYWORDS, Metadata.SUBJECT }

&lt;p&gt;Remove TikaCoreProperties.DATE&lt;br/&gt;
TikaCoreProperties.CREATION_DATE &amp;lt;- DublinCore.DATE, &lt;/p&gt;
{ Office.CREATION_DATE, MSOffice.CREATION_DATE, Metadata.DATE }

&lt;p&gt;Remove TikaCoreProperties.MODIFIED&lt;br/&gt;
TikaCoreProperties.SAVE_DATE &amp;lt;- DublinCore.MODIFIED, &lt;/p&gt;
{ Office.SAVE_DATE, MSOffice.LAST_SAVED, Metadata.MODIFIED, &quot;Last-Modified&quot; }


&lt;p&gt;and an example of the Java changes:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeHeader panelHeader&quot; style=&quot;border-bottom-width: 1px;&quot;&gt;&lt;b&gt;TikaCoreProperties.java &lt;b&gt;Before&lt;/b&gt;&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    /**
     * @see DublinCore#SUBJECT
     */
    &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Property SUBJECT = Property.composite(DublinCore.SUBJECT, 
            &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Property[] { Property.internalText(Metadata.SUBJECT) });
      
    /**
     * @see Office#KEYWORDS
     */
    &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Property KEYWORDS = Property.composite(Office.KEYWORDS,
            &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Property[] { Property.internalTextBag(MSOffice.KEYWORDS) });
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;would become&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeHeader panelHeader&quot; style=&quot;border-bottom-width: 1px;&quot;&gt;&lt;b&gt;TikaCoreProperties.java &lt;b&gt;After&lt;/b&gt;&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    /**
     * @see DublinCore#SUBJECT
     * @see Office#KEYWORDS
     */
    &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Property KEYWORDS = Property.composite(DublinCore.SUBJECT,
            &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Property[] { 
    		    Office.KEYWORDS, 
    		    Property.internalTextBag(MSOffice.KEYWORDS),
    		    Property.internalText(Metadata.SUBJECT)
    		});
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;Since this would require a bit of refactoring for parsers that use the properties being removed I thought it best to get some feedback before working up a full patch.&lt;/p&gt;

&lt;p&gt;Does this seem like a reasonable approach?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/metadata/DublinCore.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 28 May 2012 04:18:44 +0000" id="176" opendate="Mon, 28 May 2012 02:37:36 +0000">
		<buginformation>
			<summary>TikaException thrown when trying to parse archive (*.ar) files</summary>
			<description>&lt;p&gt;A TikaException is thrown when trying to drop either of the two .ar files from the parsers' test-documents folder into Tika-GUI.  From looking at this: &lt;a href=&quot;http://stuff.mit.edu/afs/athena/software/cygwin/cygwin_v1.3.2/usr/share/magic.mime&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://stuff.mit.edu/afs/athena/software/cygwin/cygwin_v1.3.2/usr/share/magic.mime&lt;/a&gt;   the archive detection is not done correctly for these types of files in the PackageExtractor class, and a TarArchiveInputStream is chosen by default, incorrectly.  &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/.gitattributes</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 2 Jul 2012 18:58:31 +0000" id="177" opendate="Mon, 2 Jul 2012 18:39:18 +0000">
		<buginformation>
			<summary>AbstractMetadataHandler addMetadata Does not Check Property.isMultiValuePermitted</summary>
			<description>&lt;p&gt;The addMetadata method in AbstractMetadataHandler does not check its property for isMultiValuePermitted before calling metadata.add which will throw a PropertyTypeException if isMultiValuePermitted is false.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/xml/AbstractMetadataHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 7 Aug 2012 21:42:08 +0000" id="178" opendate="Tue, 17 Jul 2012 15:01:37 +0000">
		<buginformation>
			<summary>Embedded docs in Word doc are not inlined (text is always added to the end)</summary>
			<description>&lt;p&gt;You can see this with the recently added testWORD_embedded_pdf.doc&lt;br/&gt;
(for &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-948&quot; title=&quot;Embedded PDF extracted incorrectly as MS Works file from Word 97-2003 doc&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-948&quot;&gt;&lt;del&gt;TIKA-948&lt;/del&gt;&lt;/a&gt;): the &quot;Bye Bye&quot; text comes before the &quot;Wer&lt;br/&gt;
wjelrwoierj...&quot; text from the embedded PDF, opposite of what you see&lt;br/&gt;
when you open the doc with Word.&lt;/p&gt;

&lt;p&gt;Yet, the thumbnail images do seem to be extracted at the right place&lt;br/&gt;
(inlined).&lt;/p&gt;

&lt;p&gt;This is because WordExtractor.java has a separate pass at the end to&lt;br/&gt;
visit the embedded docs.&lt;/p&gt;

&lt;p&gt;Would it be possible to recurse into an embedded doc at the point when&lt;br/&gt;
it's first encountered instead...?  Or maybe somehow correlate the&lt;br/&gt;
images with their corresponding attachment (right now they are just&lt;br/&gt;
named image1, image2, ...)?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 18 Jul 2012 22:39:23 +0000" id="179" opendate="Wed, 18 Jul 2012 22:38:05 +0000">
		<buginformation>
			<summary>Mimetype magic entry for NITF images</summary>
			<description>&lt;p&gt;As reported in &lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/tika-user/201207.mbox/ajax/%3C1342645464678-7572828.post%40n2.nabble.com%3E&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://mail-archives.apache.org/mod_mbox/tika-user/201207.mbox/ajax/%3C1342645464678-7572828.post%40n2.nabble.com%3E&lt;/a&gt; - Tika lacks a mimetype entry for NITF (National Imagery Transmission Format)&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 28 Aug 2013 17:19:47 +0000" id="180" opendate="Mon, 30 Jul 2012 13:02:04 +0000">
		<buginformation>
			<summary>No whitespace added if BoilerpipeContentHandler.setIncludeMarkup(true)</summary>
			<description>&lt;p&gt;ignorableWhitespace is not properly added when using the BoilerpipeContentHandler and if markus is included.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/html/BoilerpipeContentHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 9 Jan 2013 01:59:28 +0000" id="181" opendate="Mon, 30 Jul 2012 18:57:33 +0000">
		<buginformation>
			<summary>Backwards Compatibility for Metadata.LAST_AUTHOR is Broken</summary>
			<description>&lt;p&gt;As a result of changes in &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-930&quot; title=&quot;Consolidation of Some Tika Core Properties&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-930&quot;&gt;&lt;del&gt;TIKA-930&lt;/del&gt;&lt;/a&gt;, support for the deprecated Metadata.LAST_AUTHOR property has been dropped.&lt;/p&gt;

&lt;p&gt;The new TikaCoreProperties.MODIFIED should be a composite property containing Metadata.LAST_AUTHOR.&lt;/p&gt;

&lt;p&gt;Should we consider a fix release for this?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/metadata/TikaCoreProperties.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 9 Jan 2013 01:59:47 +0000" id="182" opendate="Mon, 30 Jul 2012 19:39:42 +0000">
		<buginformation>
			<summary>Backwards Compatibility for Metadata.DATE is Incorrect</summary>
			<description>&lt;p&gt;Metadata.DATE was always somewhat ambiguous, but during the consolidation in &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-930&quot; title=&quot;Consolidation of Some Tika Core Properties&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-930&quot;&gt;&lt;del&gt;TIKA-930&lt;/del&gt;&lt;/a&gt; it was incorrectly assumed that most parsers used it as a creation date.&lt;/p&gt;

&lt;p&gt;Metadata.DATE needs to instead be part of the TikaCoreProperties.MODIFIED composite property.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/microsoft/ooxml/OOXMLParserTest.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/metadata/TikaCoreProperties.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 1 Aug 2012 13:46:44 +0000" id="183" opendate="Tue, 31 Jul 2012 11:48:48 +0000">
		<buginformation>
			<summary>Text Detection Fails on Mostly Non-ASCII UTF-8 Files</summary>
			<description>&lt;p&gt;If a file contains relatively few ASCII characters and more 8 bit UTF-8 characters the TextDetector and TextStatistics classes fail to detect it as text.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/detect/TextDetector.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 5 Aug 2012 17:14:23 +0000" id="184" opendate="Tue, 31 Jul 2012 14:08:35 +0000">
		<buginformation>
			<summary>org.apache.tika.Tika missing from tika-bundle-1.2.jar</summary>
			<description>&lt;p&gt;In version 0.9 I was able to use the org.apache.tika.Tika class in my OSGi application simply by including the tika-bundle.&lt;/p&gt;

&lt;p&gt;I am now trying to move to version 1.2 and this class is missing from the tika-bundle-1.2.jar&lt;/p&gt;

&lt;p&gt;The top level documentation says &quot;Tika bundle. An OSGi bundle that includes everything you need to use all Tika functionality in an OSGi environment.&quot; so I think that this should be continued to included.&lt;/p&gt;

&lt;p&gt;I tried including the tika-core-1.2.jar into my application, but even after fiddling with various start levels I can't get the Tika class to find the parsers in the tika-bundle.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/src/site/apt/gettingstarted.apt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 5 Aug 2012 17:57:23 +0000" id="185" opendate="Thu, 2 Aug 2012 17:41:13 +0000">
		<buginformation>
			<summary>tika-bundle missing org.apache.commons.logging.LogFactory</summary>
			<description>&lt;p&gt;Trying to invoke the PDFParser from the tika-bundle results in this error:&lt;/p&gt;

&lt;p&gt;java.lang.NoClassDefFoundError: org/apache/commons/logging/LogFactory&lt;br/&gt;
	at org.apache.pdfbox.pdfparser.BaseParser.&amp;lt;clinit&amp;gt;(BaseParser.java:58)&lt;br/&gt;
	at org.apache.pdfbox.pdmodel.PDDocument.load(PDDocument.java:1089)&lt;br/&gt;
	at org.apache.pdfbox.pdmodel.PDDocument.load(PDDocument.java:1055)&lt;br/&gt;
	at org.apache.tika.parser.pdf.PDFParser.parse(PDFParser.java:123)&lt;br/&gt;
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)&lt;br/&gt;
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)&lt;br/&gt;
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)&lt;br/&gt;
	at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)&lt;br/&gt;
	at org.apache.tika.Tika.parseToString(Tika.java:421)&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-bundle/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 4 Feb 2014 23:16:22 +0000" id="186" opendate="Thu, 9 Aug 2012 19:53:25 +0000">
		<buginformation>
			<summary>PDF form data isn't included in extracted content.</summary>
			<description>&lt;p&gt;When extracting content from PDFs, PDF form data isn't extracted. &lt;/p&gt;

&lt;p&gt;The following code extracts this data via PDF box, but it seems like something Tika should be doing.&lt;/p&gt;

&lt;p&gt;PDDocumentCatalog docCatalog = load.getDocumentCatalog();&lt;br/&gt;
if (docCatalog != null) {&lt;br/&gt;
  PDAcroForm acroForm = docCatalog.getAcroForm();&lt;br/&gt;
  if (acroForm != null) {&lt;br/&gt;
	@SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
	List&amp;lt;PDField&amp;gt; fields = acroForm.getFields();&lt;br/&gt;
	if (fields != null &amp;amp;&amp;amp; fields.size() &amp;gt; 0) {&lt;br/&gt;
	  documentContent.append(&quot; &quot;);&lt;br/&gt;
	  for (PDField field : fields) {&lt;br/&gt;
		if (field.getValue()!=null) &lt;/p&gt;
{
		  documentContent.append(field.getValue());
		  documentContent.append(&quot; &quot;);
		}
&lt;p&gt;	  }&lt;br/&gt;
	}&lt;br/&gt;
  }&lt;br/&gt;
}&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDF2XHTML.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDFParserConfig.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 22 Aug 2012 18:53:03 +0000" id="187" opendate="Wed, 15 Aug 2012 13:29:48 +0000">
		<buginformation>
			<summary>LinkBuilder to optionally collapse anchor whitespace</summary>
			<description>&lt;p&gt;Links extracted by the LinkContentHandler contain the verbatim anchor text. This is usually fine but unfortunately many websites have the anchor text spread over multiple lines or have it indented with tabulators or spaces.&lt;/p&gt;

&lt;p&gt;This patch adds a boolean option to LinkContentHandler with which whitespace collapsing can be toggled on or off. Default behaviour remains as-is and the API remains backward compatible.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/sax/LinkBuilder.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 17 Dec 2012 04:22:47 +0000" id="188" opendate="Wed, 22 Aug 2012 11:24:00 +0000">
		<buginformation>
			<summary>Inaccurate XLS detection trough POIFSContainerDetector</summary>
			<description>&lt;p&gt;I've found an inaccurate detection with the attached xls file. POIFSContainerDetector is unable to determine the exact mimetype (vnd.ms-excel) and returns the generic &quot;x-tika-msoffice&quot;. This is due to the fact this file's root names are :&lt;span class=&quot;error&quot;&gt;&amp;#91;Book, �DocumentSummaryInformation, �SummaryInformation&amp;#93;&lt;/span&gt;. POIFSContainerDetector checks only that names contains &quot;WorkBook&quot;.&lt;br/&gt;
Could it be possible to add a further or-check like this:&lt;/p&gt;

&lt;p&gt;if (names.contains(&quot;Workbook&quot;) || names.contains(&quot;Book&quot;))&lt;/p&gt;

&lt;p&gt;Thank you,&lt;br/&gt;
Marco&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ExcelExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 2 Sep 2012 12:59:08 +0000" id="189" opendate="Mon, 27 Aug 2012 13:02:26 +0000">
		<buginformation>
			<summary>Text isn't extracted from PDF pop-up annotations</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 2 Sep 2012 14:10:31 +0000" id="190" opendate="Mon, 27 Aug 2012 13:20:05 +0000">
		<buginformation>
			<summary>RTF document embedded into Word (.doc) document is extracted as .unknown</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 27 Aug 2012 22:26:15 +0000" id="191" opendate="Mon, 27 Aug 2012 22:06:06 +0000">
		<buginformation>
			<summary>HTML parser should add Open Graph meta tag data to Metadata returned by parser</summary>
			<description>&lt;p&gt;HtmlHandler currently only checks for http-equiv and name attributes, when trying to decide whether to add &amp;lt;meta&amp;gt; data to the Metadata response.&lt;/p&gt;

&lt;p&gt;But Open Graph data uses property=xxx attributes, e.g. &amp;lt;meta property=&quot;og:descrioption&quot; content=&quot;some description&quot; /&amp;gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/html/HtmlHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 29 Oct 2012 01:17:24 +0000" id="192" opendate="Tue, 28 Aug 2012 11:49:41 +0000">
		<buginformation>
			<summary>JpegParserTest fails for some locales</summary>
			<description>&lt;p&gt;On a German locale I get the following test failures:&lt;br/&gt;
Failed tests:&lt;br/&gt;
testJPEGGeo(org.apache.tika.parser.jpeg.JpegParserTest): expected:&amp;lt;12&lt;span class=&quot;error&quot;&gt;&amp;#91;.&amp;#93;&lt;/span&gt;54321&amp;gt; but was:&amp;lt;12&lt;span class=&quot;error&quot;&gt;&amp;#91;,&amp;#93;&lt;/span&gt;54321&amp;gt;&lt;br/&gt;
  testJPEGGeo2(org.apache.tika.parser.jpeg.JpegParserTest): expected:&amp;lt;51&lt;span class=&quot;error&quot;&gt;&amp;#91;.&amp;#93;&lt;/span&gt;575762&amp;gt; but was:&amp;lt;51&lt;span class=&quot;error&quot;&gt;&amp;#91;,&amp;#93;&lt;/span&gt;575762&amp;gt;&lt;/p&gt;

&lt;p&gt;I can provide a patch to make the test pass, but I wonder whether it is desired that the output depends on the locale. This makes it harder for client applications to evaluate parsing results.&lt;/p&gt;

&lt;p&gt;The cause of the problem seems to be the field &lt;tt&gt;GEO_DECIMAL_FORMAT&lt;/tt&gt; in class &lt;tt&gt;ImageMetadataExtractor&lt;/tt&gt;. The formatted strings produced by this object are locale-dependent.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/image/ImageMetadataExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 2 Sep 2012 14:10:23 +0000" id="193" opendate="Thu, 30 Aug 2012 16:21:48 +0000">
		<buginformation>
			<summary>NullPointerException trying to parse detached .pk7s signature</summary>
			<description>&lt;p&gt;Our Pkcs7Parser tries to pull the signed content out and then parses&lt;br/&gt;
that, but if the signature is detached then there is no content (we&lt;br/&gt;
get null return from CMSSignedDataParser.getSignedContent) and we hit&lt;br/&gt;
NPE:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Exception in thread &quot;main&quot; org.apache.tika.exception.TikaException: Unexpected RuntimeException from org.apache.tika.parser.crypto.Pkcs7Parser@5545757a
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:244)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)
	at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)
	at org.apache.tika.cli.TikaCLI$OutputType.process(TikaCLI.java:138)
	at org.apache.tika.cli.TikaCLI.process(TikaCLI.java:399)
	at org.apache.tika.cli.TikaCLI.main(TikaCLI.java:111)
Caused by: java.lang.NullPointerException
	at org.apache.tika.parser.crypto.Pkcs7Parser.parse(Pkcs7Parser.java:64)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)
	... 5 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I think fix is trivial: if we get null return then throw a&lt;br/&gt;
TikaException saying there's nothing to extract.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 11 Sep 2012 15:19:59 +0000" id="194" opendate="Sun, 2 Sep 2012 15:12:49 +0000">
		<buginformation>
			<summary>We don't extract a placeholder for documents embedded in a Word OOXML (.docx) document</summary>
			<description>&lt;p&gt;In &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-956&quot; title=&quot;Embedded docs in Word doc are not inlined (text is always added to the end)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-956&quot;&gt;&lt;del&gt;TIKA-956&lt;/del&gt;&lt;/a&gt; we fixed the Word parser so that at the point where an embedded document appears, we output a &amp;lt;div class=&quot;embedded&quot; id=&quot;_XXX&quot;/&amp;gt; tag.&lt;/p&gt;

&lt;p&gt;It would be nice to do this for documents embedded in OOXML documents too.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 13 Dec 2012 22:38:49 +0000" id="195" opendate="Mon, 3 Sep 2012 20:17:22 +0000">
		<buginformation>
			<summary>Mp3Parser extracts wrong number of channels</summary>
			<description>&lt;p&gt;In class &lt;tt&gt;AudioFrame&lt;/tt&gt; the last two bits of the MPEG frame header are used to determine the number of channels. According to my documentation, this information is encoded in bits 7 and 6.&lt;/p&gt;

&lt;p&gt;I did a cross check with the ID3 tag editor tool ID3-TagIT (&lt;a href=&quot;http://www.id3-tagit.de/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.id3-tagit.de/&lt;/a&gt;). The unit tests expect that the test MP3 files have 2 channels. However, the tool reports that the files are mono.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/mp3/AudioFrame.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 16 Jun 2013 19:14:35 +0000" id="196" opendate="Mon, 3 Sep 2012 20:32:49 +0000">
		<buginformation>
			<summary>Mp3Parser cannot extract the duration of an audio file</summary>
			<description>&lt;p&gt;The duration of an MP3 file is an important information. Currently, &lt;tt&gt;Mp3Parser&lt;/tt&gt; is not able to extract this data from a file.&lt;/p&gt;

&lt;p&gt;Unfortunately, it is not easy to implement this functionality with the current design of the &lt;tt&gt;Mp3Parser&lt;/tt&gt; class. In order to obtain the duration, all audio frames have to be read, and their durations have to be summed up. Currently, only the beginning (the ID3v2 tags) and the end (lyrics and ID3v1) are evaluated. Processing the whole file would have additional benefits, e.g. MP3s with variable bit rates could be detected.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/metadata/XMPDM.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 18 Jun 2014 22:30:35 +0000" id="197" opendate="Fri, 21 Sep 2012 10:52:18 +0000">
		<buginformation>
			<summary>XHTMLContentHandler doesn't pass attributes of body element</summary>
			<description>&lt;p&gt;XHTMLContentHandler.startElement() uses lazyHead() for the body element because it's defined in the AUTO Set. As a consequence, attributes of the body element are not passed to downstream content handlers. &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/sax/XHTMLContentHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 28 Sep 2012 12:47:36 +0000" id="198" opendate="Sat, 22 Sep 2012 18:07:00 +0000">
		<buginformation>
			<summary>Leave a placeholder when documents are embedded in .pptx documents</summary>
			<description>&lt;p&gt;Just like &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-956&quot; title=&quot;Embedded docs in Word doc are not inlined (text is always added to the end)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-956&quot;&gt;&lt;del&gt;TIKA-956&lt;/del&gt;&lt;/a&gt;, we should leave a &amp;lt;div class=&quot;embedded&quot; id=&quot;XXX&quot;&amp;gt; to record where a given sub-document appeared.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/XSLFPowerPointExtractorDecorator.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 26 Sep 2012 14:49:45 +0000" id="199" opendate="Wed, 26 Sep 2012 10:49:57 +0000">
		<buginformation>
			<summary>RTF Parser doesn't extract page/word/character count metadata</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/rtf/RTFParserTest.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 3 Mar 2015 20:22:03 +0000" id="200" opendate="Tue, 2 Oct 2012 23:41:36 +0000">
		<buginformation>
			<summary>secure-processing not supported by some JAXP implementations and causes mime type detection to fail</summary>
			<description>&lt;p&gt;The XmlRootExtractor class tries to set the secure-processing feature that JAXP requires all parser implementations to support. Unfortunately Android (and presumably some other parsers) don't support the feature.  When run it causes the following exception: &quot;org.xml.sax.SAXNotRecognizedException: Feature 'http://javax.xml.XMLConstants/feature/secure-processing' is not recognized.&quot;&lt;/p&gt;

&lt;p&gt;However this exception is swallowed and ignored by XmlRootExtractor which returns null.  When org.apache.tika.mime.MimeTypes sees that no root element was found it assumes that the file is not valid XML and downgrades the result to text/plain.&lt;/p&gt;

&lt;p&gt;This was fixed long ago by &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-271&quot; title=&quot;secure-processing not supported by some JAXP implementations&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-271&quot;&gt;&lt;del&gt;TIKA-271&lt;/del&gt;&lt;/a&gt;, but as Michael Pisula points out, commit 1004050 broke it again.  I'd simply reopen that issue, but I don't have permission to do that.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/detect/XmlRootExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 16 Oct 2012 11:18:21 +0000" id="201" opendate="Thu, 11 Oct 2012 14:27:15 +0000">
		<buginformation>
			<summary>In Microsoft Office Word 2010 documents, text inside a textbox is not extracted/parsed out.</summary>
			<description>&lt;p&gt;Text inside a textbox, which itself can be in the body, the header or the footer, is not extracted using any type of parser (including AutoDetectParser) in combination with any type of ContentHandler.  This is NOT a duplicate of &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-904&quot; title=&quot;Pages documents created in Layout mode not supported&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-904&quot;&gt;&lt;del&gt;TIKA-904&lt;/del&gt;&lt;/a&gt;.  This specifically concerns the .docx file format.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 12 Oct 2012 17:24:13 +0000" id="202" opendate="Fri, 12 Oct 2012 11:37:08 +0000">
		<buginformation>
			<summary>NPE in extractParagraph (styleClass) in XWPFWordExtractorDecorator</summary>
			<description>&lt;p&gt;The following line &lt;/p&gt;

&lt;p&gt;TagAndStyle tas = WordExtractor.buildParagraphTagAndStyle(style.getName(),paragraph.getPartType() == BodyType.TABLECELL);&lt;/p&gt;

&lt;p&gt;Throws an NPE if style is null. This should be checked, patch is attatched&lt;/p&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 7 Nov 2012 15:06:57 +0000" id="203" opendate="Wed, 17 Oct 2012 12:17:20 +0000">
		<buginformation>
			<summary>Expose TextDocument in BoilerpipeContentHandler</summary>
			<description>&lt;p&gt;The BoilerpipeContentHandler builds a TextDocument but external programs cannot access it. This issue introduces a getTextDocument() method.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/html/BoilerpipeContentHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 16 Apr 2014 18:24:12 +0000" id="204" opendate="Fri, 19 Oct 2012 13:36:43 +0000">
		<buginformation>
			<summary>Embedded documents in RTF are not extracted</summary>
			<description>&lt;p&gt;When an RTF doc embeds a doc it looks like this:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;{\object\objemb
\objw628\objh765{\*\objclass Package}{\*\objdata 0105000002000000080000005061636b61676500000000000000000066000000
020048772e74787400433a5c444f43554d457e315c6967616c73685c4465736b746f705c48572e747874000000030022000000433a5c444f43554d457e315c6967616c73685c4465736b746f705c48572e747874000b00000048656c6c6f20576f726c64000001050000050000000d0000004d45544146494c455049435400
54040000bbfaffffee0000000800540445050000
0100090000037300000002001c0000000000050000000b0200000000050000000c02320029001c000000fb02f5ff000000000000900100000001000000005461686f6d61000055170a7000fc070058b1f37761b1f3772040f57749366683040000002d01000005000000090200000000050000000102ffffff0005000000
020101000000050000002e0106000000090000002105060048772e747874210015001c000000fb021000070000000000bc02000000000102022253797374656d00004936668300000a0026008a0100000000ffffffff8cfc0700040000002d010100030000000000}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;But, unfortunately, the format of those hex bytes is not spelled out&lt;br/&gt;
in the RTF spec ... the spec merely says the bytes are saved by the&lt;br/&gt;
OLESaveToStream function ... and I haven't been able to find a&lt;br/&gt;
description of what the bytes mean.&lt;/p&gt;

&lt;p&gt;In this case they are a &quot;Package object&quot; (\objclass Package), which I&lt;br/&gt;
think is an &lt;span class=&quot;error&quot;&gt;&amp;#91;old?&amp;#93;&lt;/span&gt; way to wrap any non-OLE file (this is just a .txt&lt;br/&gt;
file).&lt;/p&gt;

&lt;p&gt;Here's the hex dump:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;00000000  01 05 00 00 02 00 00 00  08 00 00 00 50 61 63 6b  |............Pack|
00000010  61 67 65 00 00 00 00 00  00 00 00 00 66 00 00 00  |age.........f...|
00000020  02 00 48 77 2e 74 78 74  00 43 3a 5c 44 4f 43 55  |..Hw.txt.C:\DOCU|
00000030  4d 45 7e 31 5c 69 67 61  6c 73 68 5c 44 65 73 6b  |ME~1\igalsh\Desk|
00000040  74 6f 70 5c 48 57 2e 74  78 74 00 00 00 03 00 22  |top\HW.txt.....&quot;|
00000050  00 00 00 43 3a 5c 44 4f  43 55 4d 45 7e 31 5c 69  |...C:\DOCUME~1\i|
00000060  67 61 6c 73 68 5c 44 65  73 6b 74 6f 70 5c 48 57  |galsh\Desktop\HW|
00000070  2e 74 78 74 00 0b 00 00  00 48 65 6c 6c 6f 20 57  |.txt.....Hello W|
00000080  6f 72 6c 64 00 00 01 05  00 00 05 00 00 00 0d 00  |orld............|
00000090  00 00 4d 45 54 41 46 49  4c 45 50 49 43 54 00 54  |..METAFILEPICT.T|
000000a0  04 00 00 bb fa ff ff ee  00 00 00 08 00 54 04 45  |.............T.E|
000000b0  05 00 00 01 00 09 00 00  03 73 00 00 00 02 00 1c  |.........s......|
000000c0  00 00 00 00 00 05 00 00  00 0b 02 00 00 00 00 05  |................|
000000d0  00 00 00 0c 02 32 00 29  00 1c 00 00 00 fb 02 f5  |.....2.)........|
000000e0  ff 00 00 00 00 00 00 90  01 00 00 00 01 00 00 00  |................|
000000f0  00 54 61 68 6f 6d 61 00  00 55 17 0a 70 00 fc 07  |.Tahoma..U..p...|
00000100  00 58 b1 f3 77 61 b1 f3  77 20 40 f5 77 49 36 66  |.X..wa..w @.wI6f|
00000110  83 04 00 00 00 2d 01 00  00 05 00 00 00 09 02 00  |.....-..........|
00000120  00 00 00 05 00 00 00 01  02 ff ff ff 00 05 00 00  |................|
00000130  00 02 01 01 00 00 00 05  00 00 00 2e 01 06 00 00  |................|
00000140  00 09 00 00 00 21 05 06  00 48 77 2e 74 78 74 21  |.....!...Hw.txt!|
00000150  00 15 00 1c 00 00 00 fb  02 10 00 07 00 00 00 00  |................|
00000160  00 bc 02 00 00 00 00 01  02 02 22 53 79 73 74 65  |..........&quot;Syste|
00000170  6d 00 00 49 36 66 83 00  00 0a 00 26 00 8a 01 00  |m..I6f.....&amp;amp;....|
00000180  00 00 00 ff ff ff ff 8c  fc 07 00 04 00 00 00 2d  |...............-|
00000190  01 01 00 03 00 00 00 00  00                       |.........|
00000199
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Anyway I have no idea how to decode the bytes at this point ... just&lt;br/&gt;
opening the issue in case anyone else does!&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 26 Oct 2012 21:07:26 +0000" id="205" opendate="Thu, 25 Oct 2012 23:18:18 +0000">
		<buginformation>
			<summary>Exception (Null charset name) processing .mhtml file</summary>
			<description>&lt;p&gt;This small test.mhtml file:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;From: &amp;lt;Saved by Windows Internet Explorer 8&amp;gt;
Subject: Index Pages
Date: Tue, 28 Aug 2012 09:53:28 +0300
MIME-Version: 1.0
Content-Type: multipart/related; type=&quot;multipart/alternative&quot;; boundary=&quot;----=_NextPart_000_0000_01CD8502.F991E790&quot;
X-MimeOLE: Produced By Microsoft MimeOLE V6.00.2900.6157

This is a multi-part message in MIME format. ------=_NextPart_000_0000_01CD8502.F991E790
Content-Type: multipart/alternative; boundary=&quot;----=_NextPart_001_0023_01CD8502.F99DCE70&quot; ------=_NextPart_001_0023_01CD8502.F99DCE70
Content-Type: text/html; charset=&quot;x-user-defined&quot;
Content-Transfer-Encoding: quoted-printable
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Hits this exception when run through TikaCLI:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;Exception in thread &quot;main&quot; org.apache.tika.exception.TikaException: Unexpected RuntimeException from org.apache.tika.parser.html.HtmlParser@37e67d34
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:244)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)
	at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)
	at org.apache.tika.parser.mail.MailContentHandler.body(MailContentHandler.java:102)
	at org.apache.james.mime4j.parser.MimeStreamParser.parse(MimeStreamParser.java:133)
	at org.apache.tika.parser.mail.RFC822Parser.parse(RFC822Parser.java:76)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)
	at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)
	at org.apache.tika.cli.TikaCLI$OutputType.process(TikaCLI.java:138)
	at org.apache.tika.cli.TikaCLI.process(TikaCLI.java:399)
	at org.apache.tika.cli.TikaCLI.main(TikaCLI.java:121)
Caused by: java.lang.IllegalArgumentException: Null charset name
	at java.nio.charset.Charset.lookup(Charset.java:467)
	at java.nio.charset.Charset.forName(Charset.java:540)
	at org.apache.tika.parser.txt.CharsetDetector.setCanonicalDeclaredEncoding(CharsetDetector.java:352)
	at org.apache.tika.parser.txt.CharsetDetector.setDeclaredEncoding(CharsetDetector.java:75)
	at org.apache.tika.parser.txt.Icu4jEncodingDetector.detect(Icu4jEncodingDetector.java:49)
	at org.apache.tika.detect.AutoDetectReader.detect(AutoDetectReader.java:51)
	at org.apache.tika.detect.AutoDetectReader.&amp;lt;init&amp;gt;(AutoDetectReader.java:92)
	at org.apache.tika.detect.AutoDetectReader.&amp;lt;init&amp;gt;(AutoDetectReader.java:98)
	at org.apache.tika.parser.html.HtmlParser.parse(HtmlParser.java:74)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)
	... 11 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 8 Jan 2013 23:00:12 +0000" id="206" opendate="Tue, 30 Oct 2012 00:52:45 +0000">
		<buginformation>
			<summary>Add ability to check if a mime-type is already registered</summary>
			<description>&lt;p&gt;Currently you can not ask MimeTypes if it knows about mime-type or not.  This patch adds the ability to get the normalized MimeType for a string &lt;b&gt;without&lt;/b&gt; creating a new one.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/mime/MimeTypes.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 31 Oct 2012 15:07:36 +0000" id="207" opendate="Tue, 30 Oct 2012 14:23:00 +0000">
		<buginformation>
			<summary>Word (.doc) embedded files don't set relationship ID in the Metadata</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 12 Nov 2012 11:29:44 +0000" id="208" opendate="Wed, 7 Nov 2012 16:20:45 +0000">
		<buginformation>
			<summary>Document links in Word documents don't leave a placeholder</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 12 Nov 2012 09:38:55 +0000" id="209" opendate="Mon, 12 Nov 2012 09:34:35 +0000">
		<buginformation>
			<summary>Weird &lt;scope&gt; associated to vorbis-java-core tests in vorbis-java-tika</summary>
			<description>&lt;p&gt;Here is how the dependencu is declared:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    &amp;lt;dependency&amp;gt;
      &amp;lt;groupId&amp;gt;${project.groupId}&amp;lt;/groupId&amp;gt;
      &amp;lt;artifactId&amp;gt;vorbis-java-core&amp;lt;/artifactId&amp;gt;
      &amp;lt;version&amp;gt;${project.version}&amp;lt;/version&amp;gt;
      &amp;lt;classifier&amp;gt;tests&amp;lt;/classifier&amp;gt;
      &amp;lt;scope&amp;gt;test,provided&amp;lt;/scope&amp;gt;
    &amp;lt;/dependency&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It's the first time I see such a multiple scope.&lt;/p&gt;

&lt;p&gt;Among other things it make the Maven WAR plugin include this dependency as if it was a build scope dependency.&lt;/p&gt;


&lt;p&gt;I don't know if this worked in old version of Maven but org.apache.maven.model.Dependency has only one scope for sure.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 18 Nov 2012 15:53:46 +0000" id="210" opendate="Tue, 13 Nov 2012 14:41:50 +0000">
		<buginformation>
			<summary>An MP3 with an UTF-16 ID3 tag containing only the BOM should produce empty string value for that tag</summary>
			<description>&lt;p&gt;This seems to be a difference between JVMs: on IBM's JVM I incorrectly see the BOM as the value of the tag, while on Oracle's JVM I correctly get the empty string.&lt;/p&gt;

&lt;p&gt;I'm not sure if this is a bug in IBM's JVM ... the javadocs are not totally clear how a UTF-16 string containing only the BOM should be decoded by new String(...) ... to fix this I think we should just detect this case and short-circuit empty string return.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 18 Nov 2012 16:11:42 +0000" id="211" opendate="Tue, 13 Nov 2012 19:05:22 +0000">
		<buginformation>
			<summary>Powerpoint (.ppt) parser doesn't leave placeholder where documents are embedded</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 19 Nov 2012 10:31:35 +0000" id="212" opendate="Mon, 19 Nov 2012 09:54:49 +0000">
		<buginformation>
			<summary>ServiceLoader should respect OSGi service ranking</summary>
			<description>&lt;p&gt;Currently the ServiceLoader and TikaActivator classes simply maintain an unordered collection of the Parser and Detector services currently available in an OSGi environment. This is troublesome in the case where there are for example two Parser services that both cover the same media type, and one of them should be preferred. The OSGi way for specifying such preferences is through &lt;a href=&quot;http://www.osgi.org/javadoc/r4v42/org/osgi/framework/Constants.html#SERVICE%5FRANKING&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;service ranking&lt;/a&gt;, but that currently doesn't work with Tika.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/config/ServiceLoader.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 19 Nov 2012 15:25:45 +0000" id="213" opendate="Mon, 19 Nov 2012 14:22:19 +0000">
		<buginformation>
			<summary>Allow null values when setting metadata</summary>
			<description>&lt;p&gt;Many file formats have optional metadata entries that result in a lot of &lt;tt&gt;if (value == null)&lt;/tt&gt; statements around &lt;tt&gt;metadata.set(...)&lt;/tt&gt; calls. It would be more convenient if the &lt;tt&gt;set&lt;/tt&gt; method explicitly allowed a &lt;tt&gt;null&lt;/tt&gt; value and that a call like &lt;tt&gt;metadata.set(name, null)&lt;/tt&gt; was essentially equivalent to a &lt;tt&gt;metadata.remove(name)&lt;/tt&gt; call.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/metadata/Metadata.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 1 Dec 2012 17:53:39 +0000" id="214" opendate="Mon, 26 Nov 2012 13:27:49 +0000">
		<buginformation>
			<summary>TikaCLI doesn't create sub-dirs when extracting Zip files</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/test/java/org/apache/tika/cli/TikaCLITest.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 1 Dec 2012 17:57:13 +0000" id="215" opendate="Mon, 26 Nov 2012 19:11:51 +0000">
		<buginformation>
			<summary>Powerpoint (.pptx) can have duplicate embedded ids</summary>
			<description>&lt;p&gt;Apparently the relId is only unique within one slide ... I fixed it to prefix slideN_.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 1 Dec 2012 17:57:13 +0000" id="216" opendate="Tue, 27 Nov 2012 11:56:03 +0000">
		<buginformation>
			<summary>Tika doesn't parse embedded OLE Chart/Graph objects</summary>
			<description>&lt;p&gt;I have an example ppt that embeds a chart, but Tika mis-identifies it&lt;br/&gt;
as an XLS document.&lt;/p&gt;

&lt;p&gt;The progID (oleShape.getProgID() in&lt;br/&gt;
HSLFExtractor.handleSlideEmbeddedResources) is MSGraph.Chart.8 ... and&lt;br/&gt;
we seem to detect it as Excel (application/vnd.ms-excel) but then the&lt;br/&gt;
ExcelExtractor hits this exception:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.poi.hssf.record.RecordFormatException: Unable to construct record instance
	at org.apache.poi.hssf.record.RecordFactory$ReflectionConstructorRecordCreator.create(RecordFactory.java:65)
	at org.apache.poi.hssf.record.RecordFactory.createSingleRecord(RecordFactory.java:301)
	at org.apache.poi.hssf.record.RecordFactoryInputStream.readNextRecord(RecordFactoryInputStream.java:285)
	at org.apache.poi.hssf.record.RecordFactoryInputStream.nextRecord(RecordFactoryInputStream.java:251)
	at org.apache.poi.hssf.eventusermodel.HSSFEventFactory.genericProcessEvents(HSSFEventFactory.java:143)
	at org.apache.poi.hssf.eventusermodel.HSSFEventFactory.processEvents(HSSFEventFactory.java:106)
	at org.apache.tika.parser.microsoft.ExcelExtractor$TikaHSSFListener.processFile(ExcelExtractor.java:302)
	at org.apache.tika.parser.microsoft.ExcelExtractor.parse(ExcelExtractor.java:147)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Since DelegatingParser silently suppresses all exceptions, when you&lt;br/&gt;
run TikaCLI you won't see any exception nor text extracted, but if you&lt;br/&gt;
run with -z, it will save 1.xls which if you then try to parse with&lt;br/&gt;
TikaCLI hits the above exception.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 1 Dec 2012 18:05:37 +0000" id="217" opendate="Fri, 30 Nov 2012 12:42:39 +0000">
		<buginformation>
			<summary>PDF bookmark text is not extracted</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDF2XHTML.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 1 Dec 2012 18:07:23 +0000" id="218" opendate="Fri, 30 Nov 2012 18:52:45 +0000">
		<buginformation>
			<summary>ZIP parsing doesn't leave placeholders for each package entry</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pkg/PackageParser.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 4 Mar 2015 21:42:41 +0000" id="219" opendate="Wed, 5 Dec 2012 07:58:10 +0000">
		<buginformation>
			<summary>Parsing PDF with StackOverlowError </summary>
			<description>&lt;p&gt;Tika corrupt with StackOverflowError on some pdf documents:&lt;br/&gt;
&lt;a href=&quot;http://www.ellipse-labo.com/fiches/1303214351.pdf&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.ellipse-labo.com/fiches/1303214351.pdf&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;http://downloads.joomlacode.org/frsrelease/5/4/0/54089/handbuch_ckforms-DE-1.3.2.pdf&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://downloads.joomlacode.org/frsrelease/5/4/0/54089/handbuch_ckforms-DE-1.3.2.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Code:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
AutoDetectParser parser = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; AutoDetectParser(
                &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; TypeDetector(),
                &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; PDFParser(),
                &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; OfficeParser(),
                &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; HtmlParser(),
                &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; RTFParser(),
                &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; OOXMLParser());

WriteOutContentHandler contentHandler = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; WriteOutContentHandler();
Metadata metadata = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Metadata();

parser.parse(contentStream, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; BodyContentHandler(contentHandler), metadata, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ParseContext());
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Stack trace:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;java.lang.StackOverflowError
	at java.util.LinkedHashMap$LinkedHashIterator.&amp;lt;init&amp;gt;(LinkedHashMap.java:345)
	at java.util.LinkedHashMap$LinkedHashIterator.&amp;lt;init&amp;gt;(LinkedHashMap.java:345)
	at java.util.LinkedHashMap$KeyIterator.&amp;lt;init&amp;gt;(LinkedHashMap.java:383)
	at java.util.LinkedHashMap$KeyIterator.&amp;lt;init&amp;gt;(LinkedHashMap.java:383)
	at java.util.LinkedHashMap.newKeyIterator(LinkedHashMap.java:396)
	at java.util.HashMap$KeySet.iterator(HashMap.java:874)
	at org.apache.pdfbox.cos.COSDictionary.toString(COSDictionary.java:1416)
	at org.apache.pdfbox.cos.COSDictionary.toString(COSDictionary.java:1421)
	at org.apache.pdfbox.cos.COSDictionary.toString(COSDictionary.java:1421)
	at org.apache.pdfbox.cos.COSDictionary.toString(COSDictionary.java:1421)
	at org.apache.pdfbox.cos.COSDictionary.toString(COSDictionary.java:1421)
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


			</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 13 Dec 2012 08:47:52 +0000" id="220" opendate="Tue, 11 Dec 2012 22:47:12 +0000">
		<buginformation>
			<summary>Tika 1.2 universalcharset errors</summary>
			<description>&lt;p&gt;This is somewhat confusing and frustrating. I successfully crawled Opentext using all of the above. then I recrawled and it aborted almost immediately.&lt;br/&gt;
It choked on images, so I excluded them for now. &lt;br/&gt;
but now it's choking on txt files! &lt;br/&gt;
sometimes I get this error&lt;br/&gt;
SEVERE: null:java.lang.RuntimeException: java.lang.NoClassDefFoundError: org/mozilla/universalchardet/CharsetListener&lt;/p&gt;

&lt;p&gt;and sometimes I get this one&lt;br/&gt;
SEVERE: null:java.lang.RuntimeException: java.lang.NoClassDefFoundError: org/apache/tika/parser/txt/UniversalEncodingListener&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/config/ServiceLoader.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 6 Jan 2013 23:49:05 +0000" id="221" opendate="Thu, 20 Dec 2012 11:03:24 +0000">
		<buginformation>
			<summary>XMLParser should add whitespace between elements</summary>
			<description>&lt;p&gt;If the incoming XML is compact (ie doesn't have whitespace between elements), I think we should somehow add whitespace between elements when extracting text?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 27 Dec 2012 16:57:06 +0000" id="222" opendate="Thu, 27 Dec 2012 01:05:19 +0000">
		<buginformation>
			<summary>Upgrade to PDFBox 1.7.1</summary>
			<description>&lt;p&gt;PDFBox 1.7.1 fixes some bugs. I'm hitting &lt;a href=&quot;https://issues.apache.org/jira/browse/PDFBOX-1353&quot; title=&quot;PDFBox extracts wrong characters for some korean pdf files.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PDFBOX-1353&quot;&gt;&lt;del&gt;PDFBOX-1353&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/PDFBOX-1340&quot; title=&quot;i got wrong characters when i extract some chinese pdf files&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PDFBOX-1340&quot;&gt;&lt;del&gt;PDFBOX-1340&lt;/del&gt;&lt;/a&gt;. So I hope to upgrade PDFBox version of Tika's pom.xml&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 7 Feb 2013 15:41:34 +0000" id="223" opendate="Mon, 7 Jan 2013 13:56:04 +0000">
		<buginformation>
			<summary>Upgrade Tika Parsers to use ASM 4.x</summary>
			<description>&lt;p&gt;Right now Tika 1.2 uses ASM 3.1. &lt;/p&gt;

&lt;p&gt;However this is causing some issues for us on the XWiki project since we also bundle other framework that use a more recent version of ASM (we use pegdown which uses parboiled which draws ASM 4.0).&lt;/p&gt;

&lt;p&gt;The problem is that ASM 3.x and 4.0 are not compatible...&lt;/p&gt;

&lt;p&gt;See &lt;a href=&quot;http://jira.xwiki.org/browse/XE-1269&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://jira.xwiki.org/browse/XE-1269&lt;/a&gt; for more details about the issue we're facing.&lt;/p&gt;

&lt;p&gt;Thanks for considering upgrading to ASM 4.x &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 16 Jan 2013 19:17:00 +0000" id="224" opendate="Fri, 11 Jan 2013 13:53:52 +0000">
		<buginformation>
			<summary>unify ImageMetadataExtractor interface</summary>
			<description>&lt;p&gt;there are several methods in this class that are targeted for different image type but with different visibility:&lt;/p&gt;

&lt;p&gt;public void parseJpeg(File file);&lt;br/&gt;
protected void parseTiff(InputStream stream);&lt;/p&gt;

&lt;p&gt;both simply extract all possible metadata from image file or stream. Would be nice if parseTiff could also be &quot;public&quot; so it will be easier to create custom parsers located in external jars that use this functionality.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/image/ImageMetadataExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 8 Aug 2013 14:51:57 +0000" id="225" opendate="Tue, 2 Jul 2013 23:30:56 +0000">
		<buginformation>
			<summary>javascript files that contain &quot;&lt;html&quot; are detected as text/html</summary>
			<description>&lt;p&gt;The Mimetypes detector will return text/html as the mimetype for any javascript file that contains the string &quot;&amp;lt;html&quot; in it. I believe this is due to the rule &amp;lt;match value=&quot;&amp;lt;html&quot; type=&quot;string&quot; offset=&quot;0:8192&quot;/&amp;gt; in the tika-mimetypes.xml file.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 3 Aug 2012 12:33:44 +0000" id="226" opendate="Fri, 3 Aug 2012 12:04:54 +0000">
		<buginformation>
			<summary>TikaException Thrown When Handling Unknown Fields for Some JPEGs</summary>
			<description>&lt;p&gt;In Tika 0.9: Exception &quot;org.apache.tika.exception.TikaException: Can't read JPEG metada&quot; / &quot;com.drew.metadata.MetadataException: Tag '34855' cannot be cast to int.  It is of type 'class [I&quot; when indexing some items.&lt;/p&gt;

&lt;p&gt;In Tika 1.3-SNAPSHOT a NullPointerException is thrown.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/image/ImageMetadataExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 5 Aug 2012 14:45:20 +0000" id="227" opendate="Fri, 3 Aug 2012 13:19:11 +0000">
		<buginformation>
			<summary>Full identification of the JPEG 2000 family of formats</summary>
			<description>&lt;p&gt;Please find attached a suitable set of magic definitions for allowing Tika to identify JP2 containers, codestreams, and the JP2, JPF, JPM and MJ2 file formats. It is based on the 'file' magic from &lt;a href=&quot;https://github.com/bitsgalore/jp2kMagic&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;, and has been tested against the example files supplied on that site.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 10 Jan 2013 15:30:04 +0000" id="228" opendate="Fri, 9 Nov 2012 18:06:12 +0000">
		<buginformation>
			<summary>DWG Custom properties not extracted</summary>
			<description>&lt;p&gt;Based on some code I provided some time ago (Alfresco forum), Derek Hulley opened ALF-2262, Nick Burch opened &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-413&quot; title=&quot;DWG Parser&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-413&quot;&gt;&lt;del&gt;TIKA-413&lt;/del&gt;&lt;/a&gt; issue and code has been committed to TIKA (0.8).&lt;/p&gt;

&lt;p&gt;With sample dwg provided TIKA (0.8 to 1.2) is correctly working but with attached file returns no custom metadata (my original &quot;C&quot; returns correct custom metadata, dwg is &quot;2010&quot; format).&lt;/p&gt;

&lt;p&gt;Tested tika-app.1.0.jar and tika-app.1.2.jar and tika 1.3 snapshot.&lt;br/&gt;
All versions could be impacted by this bug. &lt;/p&gt;

&lt;p&gt;I found failing code in skipToCustomProperties() of DWGParser.java, lines 320-321: &lt;br/&gt;
if(padding&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt; == 0 &amp;amp;&amp;amp; padding&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; == 0 &amp;amp;&amp;amp;&lt;br/&gt;
  padding&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; == 0 &amp;amp;&amp;amp; padding&lt;span class=&quot;error&quot;&gt;&amp;#91;3&amp;#93;&lt;/span&gt; == 0) {&lt;/p&gt;

&lt;p&gt;padding&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt; byte is not always 0 (attached file has 0x2) and probably there is no need to check those bytes.&lt;/p&gt;

&lt;p&gt;Index: DWGParser.java&lt;br/&gt; ===================================================================&lt;br/&gt;
&amp;#8212; DWGParser.java	(revisione 1407024)&lt;br/&gt;
+++ DWGParser.java	(copia locale)&lt;br/&gt;
@@ -93,7 +93,7 @@&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;How far to skip after the last standard property, before&lt;/li&gt;
	&lt;li&gt;we find any custom properties that might be there.&lt;br/&gt;
      */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static final int CUSTOM_PROPERTIES_SKIP = 20;&lt;br/&gt;
+    private static final int CUSTOM_PROPERTIES_SKIP = 24; &lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     public void parse(&lt;br/&gt;
             InputStream stream, ContentHandler handler,&lt;br/&gt;
@@ -317,13 +317,7 @@&lt;/p&gt;

&lt;p&gt;     private int skipToCustomProperties(InputStream stream) &lt;br/&gt;
             throws IOException, TikaException {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// There should be 4 zero bytes next&lt;/li&gt;
	&lt;li&gt;byte[] padding = new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;4&amp;#93;&lt;/span&gt;;&lt;/li&gt;
	&lt;li&gt;IOUtils.readFully(stream, padding);&lt;/li&gt;
	&lt;li&gt;if(padding&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt; == 0 &amp;amp;&amp;amp; padding&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; == 0 &amp;amp;&amp;amp;&lt;/li&gt;
	&lt;li&gt;padding&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; == 0 &amp;amp;&amp;amp; padding&lt;span class=&quot;error&quot;&gt;&amp;#91;3&amp;#93;&lt;/span&gt; == 0) 
{
-          // Looks hopeful, skip on
-          padding = new byte[CUSTOM_PROPERTIES_SKIP];
+          byte[] padding = new byte[CUSTOM_PROPERTIES_SKIP];
           IOUtils.readFully(stream, padding);
           
           // We should now have the count
@@ -337,10 +331,6 @@
              // No properties / count is too high to trust
              return 0;
           }&lt;/li&gt;
	&lt;li&gt;} else 
{
-          // No padding. That probably means no custom props
-          return 0;
-       }
&lt;p&gt;     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; }&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/dwg/DWGParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 11 Dec 2012 23:31:03 +0000" id="229" opendate="Tue, 11 Dec 2012 23:20:54 +0000">
		<buginformation>
			<summary>Lotus Notes .eml Files Not Always Detected Properly</summary>
			<description>&lt;p&gt;Lotus Notes doesn't guarantee that any of the magic matches defined under mime-type message/rfc822 will be at the start of the file.&lt;/p&gt;

&lt;p&gt;Instead many X-Notes-Item headers may precede those headers.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 21 Jan 2013 07:14:37 +0000" id="230" opendate="Mon, 21 Jan 2013 06:56:31 +0000">
		<buginformation>
			<summary>Degrade gracefully when juniversalchardet not present</summary>
			<description>&lt;p&gt;The UniversalEncodingDetector class used by TXTParser depends on the presence of the juniversalchardet library. The detector tries to fail gracefully when the library is not present by catching unexpected exceptions, but in fact that case leads to a LinkageError that is not caught by the current code.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/txt/UniversalEncodingDetector.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 23 Jan 2013 21:39:26 +0000" id="231" opendate="Wed, 23 Jan 2013 15:45:36 +0000">
		<buginformation>
			<summary>Add list detection to RTFParser</summary>
			<description>&lt;p&gt;RTF supports lists, and the parser could support those, too, using HTML &amp;lt;ul&amp;gt;/&amp;lt;ol&amp;gt;/&amp;lt;li&amp;gt; tags.&lt;/p&gt;

&lt;p&gt;I'm attaching a patch that implements basic support for Word 97 and newer lists. Nested lists are not supported correctly, yet, though, and a number of formatting options are ignored.&lt;/p&gt;

&lt;p&gt;I've also added test cases for this, and adapted existing tests where needed.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 13 Mar 2015 23:27:12 +0000" id="232" opendate="Thu, 24 Jan 2013 16:56:10 +0000">
		<buginformation>
			<summary>OpenDocument basic style support</summary>
			<description>&lt;p&gt;I've added basic support for list and text styles. Paragraph styles are omitted on purpose &amp;#8211; one could use the style names as class names, though.&lt;/p&gt;

&lt;p&gt;Only bold, italic, and underlined text is supported.&lt;/p&gt;

&lt;p&gt;Lists now differentiate between ordered and unordered lists.&lt;/p&gt;

&lt;p&gt;Test case included. I've also changed the ODFParserTest to make a bit more use of the methods of its super class.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/resources/test-documents/testStyles.odt</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 28 Jan 2013 17:05:06 +0000" id="233" opendate="Mon, 28 Jan 2013 16:37:15 +0000">
		<buginformation>
			<summary>Mimetype entries for SAS files</summary>
			<description>&lt;p&gt;The Tika mimetypes list doesn't currently have any SAS related entries. SAS provides a handy list of the file extensions and their types:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://support.sas.com/documentation/cdl/en/hostwin/63285/HTML/default/viewer.htm#introsasfiles.htm&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://support.sas.com/documentation/cdl/en/hostwin/63285/HTML/default/viewer.htm#introsasfiles.htm&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;That page doesn't have the mimetypes, but these can generally be found from googling&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 21 Feb 2013 21:38:54 +0000" id="234" opendate="Mon, 4 Feb 2013 14:53:40 +0000">
		<buginformation>
			<summary>Extraction should continue if an exception is hit visiting an embedded document</summary>
			<description>&lt;p&gt;Spinoff from &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1072&quot; title=&quot;AIOOBE when handling embedded document in .doc file&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1072&quot;&gt;&lt;del&gt;TIKA-1072&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In that issue, a problematic document (still not sure if document is corrupt, or possible POI bug) caused an exception when visiting the embedded documents.&lt;/p&gt;

&lt;p&gt;If I change Tika to suppress that exception, the rest of the document extracts fine.&lt;/p&gt;

&lt;p&gt;So somehow I think we should be more robust here, and maybe log the exception, or save/record the exception(s) somewhere so after parsing the app could decide what to do about them ...&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 27 Sep 2013 16:20:21 +0000" id="235" opendate="Mon, 4 Feb 2013 15:54:10 +0000">
		<buginformation>
			<summary>Upgrade to Apache POI 3.9</summary>
			<description>&lt;p&gt;We should upgrade to Apache POI 3.9, which is the latest version&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/HSLFExtractor.java</file>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 13 Feb 2013 14:43:14 +0000" id="236" opendate="Tue, 12 Feb 2013 23:24:08 +0000">
		<buginformation>
			<summary>Merge image/x-icon and image/vnd.microsoft.icon</summary>
			<description>&lt;p&gt;tika-mimetypes.xml has an entry for image/x-icon and image/vnd.microsoft.icon&lt;/p&gt;

&lt;p&gt;According to:&lt;br/&gt;
&lt;a href=&quot;http://en.wikipedia.org/wiki/.ico&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://en.wikipedia.org/wiki/.ico&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;These are the same thing.  I think we should change it to:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-xml&quot;&gt;
  &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;mime-type type=&lt;span class=&quot;code-quote&quot;&gt;&quot;image/vnd.microsoft.icon&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;acronym&amp;gt;&lt;/span&gt;ICO&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/acronym&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;tika:link&amp;gt;&lt;/span&gt;http://en.wikipedia.org/wiki/.ico&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/tika:link&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;tika:uti&amp;gt;&lt;/span&gt;com.microsoft.ico&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/tika:uti&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;alias&amp;gt;&lt;/span&gt;image/x-icon&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/alias&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;magic priority=&lt;span class=&quot;code-quote&quot;&gt;&quot;50&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;
      &amp;lt;match value=&lt;span class=&quot;code-quote&quot;&gt;&quot;\102\101\050\000\000\000\056\000\000\000\000\000\000\000&quot;&lt;/span&gt; type=&lt;span class=&quot;code-quote&quot;&gt;&quot;string&quot;&lt;/span&gt; offset=&lt;span class=&quot;code-quote&quot;&gt;&quot;0&quot;&lt;/span&gt;/&amp;gt;
      &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;match value=&lt;span class=&quot;code-quote&quot;&gt;&quot;\000\000\001\000&quot;&lt;/span&gt; type=&lt;span class=&quot;code-quote&quot;&gt;&quot;string&quot;&lt;/span&gt; offset=&lt;span class=&quot;code-quote&quot;&gt;&quot;0&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/magic&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;glob pattern=&lt;span class=&quot;code-quote&quot;&gt;&quot;*.ico&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/mime-type&amp;gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 29 Sep 2015 13:46:17 +0000" id="237" opendate="Fri, 15 Feb 2013 10:47:04 +0000">
		<buginformation>
			<summary>PDF header and mime detection</summary>
			<description>&lt;p&gt;I've found some PDF files Tika recognizes as application/octet-stream.&lt;br/&gt;
These files differs from regularly identified PDF having a different header: the %PDF-N.n string isn't at the beginning (zero offset) of the file but in the first 1024 bytes.&lt;/p&gt;

&lt;p&gt;PDF reference states that &quot;The first line of a PDF file shall be a header consisting of the 5 characters  %PDF–  followed by a version &lt;br/&gt;
number of the form 1.N, where N is a digit between 0 and 7&quot; (&lt;a href=&quot;http://tinyurl.com/8vnzm3c&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://tinyurl.com/8vnzm3c&lt;/a&gt; &quot;p. 7.5.2 File Header&quot;). &lt;/p&gt;

&lt;p&gt;Looking further at implementation notes by Adobe (&lt;a href=&quot;http://tinyurl.com/cbqpb24&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://tinyurl.com/cbqpb24&lt;/a&gt; p. 3.4.1 File Header) I've discover that: &quot;Acrobat viewers require only that the header appear somewhere within the first 1024 bytes of the file&quot;&lt;br/&gt;
What do you think about a PDF magic match with an offset 0:1024?&lt;/p&gt;

&lt;p&gt;&amp;lt;match value=&quot;%PDF-&quot; type=&quot;string&quot; offset=&quot;0:1024&quot;/&amp;gt;&lt;/p&gt;

&lt;p&gt;Thank you,&lt;br/&gt;
Marco&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 4 Feb 2014 23:07:27 +0000" id="238" opendate="Fri, 15 Feb 2013 18:19:22 +0000">
		<buginformation>
			<summary>Tika-bundle 1.3 does not import org.w3c.dom package</summary>
			<description>&lt;p&gt;The tika-bundle 1.3 version does not import org.w3c.dom package, as a result it is not able to parse DOM based documents such as Microsoft Word (docx) documents.&lt;/p&gt;

&lt;p&gt;This issue does not have in version 1.2 as it does import the necessary package and therefore the parsing of the documents work fine.&lt;/p&gt;

&lt;p&gt;Can someone please look into the issue, as Microsoft Word is a very popular document.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-bundle/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 20 Feb 2013 13:28:26 +0000" id="239" opendate="Tue, 19 Feb 2013 21:26:58 +0000">
		<buginformation>
			<summary>PICT format detection</summary>
			<description>&lt;p&gt;A simple patch that add detection of PICT &quot;Apple Macintosh QuickDraw/PICT Format&quot;. I use magic pattern from ImageMagick source code.&lt;/p&gt;

&lt;p&gt;I can't send you &quot;freely available&quot; sample file to test, but I found sample picture on internet :&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.kingsgranthoa.com/images/kgbannerbrg.pict&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.kingsgranthoa.com/images/kgbannerbrg.pict&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;http://www.kingsgranthoa.com/images/kgl.pict&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.kingsgranthoa.com/images/kgl.pict&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 28 May 2013 14:27:00 +0000" id="240" opendate="Fri, 5 Apr 2013 14:18:21 +0000">
		<buginformation>
			<summary>Can we add &lt;div&gt; to the list of heuristics for bad html fragments?</summary>
			<description>&lt;p&gt;Good morning,&lt;br/&gt;
Crawling legacy sites with poorly written html fragments causes severe Solr Xml parse errors and in turn causes ManifoldCF to abort.&lt;br/&gt;
Can we add &amp;lt;div&amp;gt; to the list of heuristics so the html parser is used instead of the xml parser?&lt;br/&gt;
see this ticket for further information: &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1101&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;TIKA-1101&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Thank you,&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 1 May 2013 17:44:59 +0000" id="241" opendate="Fri, 26 Apr 2013 22:52:57 +0000">
		<buginformation>
			<summary>ExifHandler throws NullPointerException</summary>
			<description>&lt;p&gt;Notice that in the second if block, there is no check for null on the retrived datetime. I have hit this with a file which apparently has null for this value. Seems like the fix is trivial&lt;/p&gt;

&lt;p&gt;public void handleDateTags(Directory directory, Metadata metadata)&lt;br/&gt;
                throws MetadataException {&lt;br/&gt;
            // Date/Time Original overrides value from ExifDirectory.TAG_DATETIME&lt;br/&gt;
            Date original = null;&lt;br/&gt;
            if (directory.containsTag(ExifSubIFDDirectory.TAG_DATETIME_ORIGINAL)) {&lt;br/&gt;
                original = directory.getDate(ExifSubIFDDirectory.TAG_DATETIME_ORIGINAL);&lt;br/&gt;
                // Unless we have GPS time we don't know the time zone so date must be set&lt;br/&gt;
                // as ISO 8601 datetime without timezone suffix (no Z or +/-)&lt;br/&gt;
                if (original != null) &lt;/p&gt;
{
                    String datetimeNoTimeZone = DATE_UNSPECIFIED_TZ.format(original); // Same time zone as Metadata Extractor uses
                    metadata.set(TikaCoreProperties.CREATED, datetimeNoTimeZone);
                    metadata.set(Metadata.ORIGINAL_DATE, datetimeNoTimeZone);
                }
&lt;p&gt;            }&lt;br/&gt;
            if (directory.containsTag(ExifIFD0Directory.TAG_DATETIME)) {&lt;br/&gt;
                Date datetime = directory.getDate(ExifIFD0Directory.TAG_DATETIME);&lt;br/&gt;
                String datetimeNoTimeZone = DATE_UNSPECIFIED_TZ.format(datetime);&lt;br/&gt;
                metadata.set(TikaCoreProperties.MODIFIED, datetimeNoTimeZone);&lt;br/&gt;
                // If Date/Time Original does not exist this might be creation date&lt;br/&gt;
                if (metadata.get(TikaCoreProperties.CREATED) == null) &lt;/p&gt;
{
                    metadata.set(TikaCoreProperties.CREATED, datetimeNoTimeZone);
                }
&lt;p&gt;            }&lt;br/&gt;
        }&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/image/ImageMetadataExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 14 Mar 2015 22:01:23 +0000" id="242" opendate="Wed, 1 May 2013 13:25:19 +0000">
		<buginformation>
			<summary>IWorkPackageParser should not close the InputStream</summary>
			<description>&lt;p&gt;I am using the Tika parser framework embedded in another application, i.e. I am controlling the output InputStream which is then wrapped as a TikaInputStream and parsed. Under these circumstances, I run into problems due to the IWorkPackageParser closing the InputStream. See org.apache.tika.parser.iwork.IWorkPackageParser.parse(IWorkPackageParser.java:219)&lt;/p&gt;

&lt;p&gt;I can avoid this by wrapping my stream in a CloseShieldInputStream, but I suspect this may cause problems more widely. I believe that any Tika Parser implementation should not close the InputStream it is passed, and therefore the .close() call in the IWorkPackageParser should be removed.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/iwork/IWorkPackageParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 25 May 2013 08:51:52 +0000" id="243" opendate="Wed, 22 May 2013 07:50:06 +0000">
		<buginformation>
			<summary>Add more mimetypes for famous programming languages</summary>
			<description>&lt;p&gt;Would it be possible to add the Mime Types of these programming lanuages from the attached custom-mimetypes.xml into the tika-mimetypes.xml-file?&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Asciidoc: &lt;a href=&quot;http://discuss.asciidoctor.org/Mimetype-for-Asciidoc-td211.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;text/x-asciidoc&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;D: text/x-d&lt;/li&gt;
	&lt;li&gt;HAML: text/x-haml&lt;/li&gt;
	&lt;li&gt;Haxe: text/x-haxe&lt;/li&gt;
	&lt;li&gt;R: text/x-rsrc&lt;/li&gt;
	&lt;li&gt;XQuery: application/xquery&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;These mimetypes are also used in other projects like the CodeMirror editor.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 30 May 2013 15:17:31 +0000" id="244" opendate="Wed, 29 May 2013 11:26:32 +0000">
		<buginformation>
			<summary>Replace line tabulation with line break</summary>
			<description>&lt;p&gt;Tika WordExtractor not replacing line tabular character by line break like POI WordExtractor.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/microsoft/WordParserTest.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 10 Jul 2013 17:11:15 +0000" id="245" opendate="Wed, 5 Jun 2013 21:56:26 +0000">
		<buginformation>
			<summary>.docx text extract leaves out some portions of text</summary>
			<description>&lt;p&gt;When parsing a Microsoft Word .docx (application/vnd.openxmlformats-officedocument.wordprocessingml.document), certain portions of text remain unextracted.&lt;/p&gt;

&lt;p&gt;I have attached a .docx file that can be tested against. The 'gray' portions of text are what are not extracted, while the darker colored text extracts fine.&lt;/p&gt;

&lt;p&gt;Looking at the document.xml portion of the .docx zip file shows the text is all there.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/microsoft/ooxml/OOXMLParserTest.java</file>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 11 Jun 2013 03:17:10 +0000" id="246" opendate="Tue, 11 Jun 2013 02:59:36 +0000">
		<buginformation>
			<summary>Ability to Allow Empty and Duplicate Tika Values for XML Elements</summary>
			<description>&lt;p&gt;In some cases it is beneficial to allow empty and duplicate Tika metadata values for multi-valued XML elements like RDF bags.&lt;/p&gt;

&lt;p&gt;Consider an example where the original source metadata is structured something like:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&amp;lt;Person&amp;gt;
  &amp;lt;FirstName&amp;gt;John&amp;lt;/FirstName&amp;gt;
  &amp;lt;LastName&amp;gt;Smith&amp;lt;/FirstName&amp;gt;
&amp;lt;/Person&amp;gt;
&amp;lt;Person&amp;gt;
  &amp;lt;FirstName&amp;gt;Jane&amp;lt;/FirstName&amp;gt;
  &amp;lt;LastName&amp;gt;Doe&amp;lt;/FirstName&amp;gt;
&amp;lt;/Person&amp;gt;
&amp;lt;Person&amp;gt;
  &amp;lt;FirstName&amp;gt;Bob&amp;lt;/FirstName&amp;gt;
&amp;lt;/Person&amp;gt;
&amp;lt;Person&amp;gt;
  &amp;lt;FirstName&amp;gt;Kate&amp;lt;/FirstName&amp;gt;
  &amp;lt;LastName&amp;gt;Smith&amp;lt;/FirstName&amp;gt;
&amp;lt;/Person&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and since Tika stores only flat metadata we transform that before invoking a parser to something like:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt; &amp;lt;custom:FirstName&amp;gt;
  &amp;lt;rdf:Bag&amp;gt;
   &amp;lt;rdf:li&amp;gt;John&amp;lt;/rdf:li&amp;gt;
   &amp;lt;rdf:li&amp;gt;Jane&amp;lt;/rdf:li&amp;gt;
   &amp;lt;rdf:li&amp;gt;Bob&amp;lt;/rdf:li&amp;gt;
   &amp;lt;rdf:li&amp;gt;Kate&amp;lt;/rdf:li&amp;gt;
  &amp;lt;/rdf:Bag&amp;gt;
 &amp;lt;/custom:FirstName&amp;gt;
 &amp;lt;custom:LastName&amp;gt;
  &amp;lt;rdf:Bag&amp;gt;
   &amp;lt;rdf:li&amp;gt;Smith&amp;lt;/rdf:li&amp;gt;
   &amp;lt;rdf:li&amp;gt;Doe&amp;lt;/rdf:li&amp;gt;
   &amp;lt;rdf:li&amp;gt;&amp;lt;/rdf:li&amp;gt;
   &amp;lt;rdf:li&amp;gt;Smith&amp;lt;/rdf:li&amp;gt;
  &amp;lt;/rdf:Bag&amp;gt;
 &amp;lt;/custom:LastName&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The current behavior ignores empties and duplicates and we don't know if Bob or Kate ever had last names.  Empties or duplicates in other positions result in an incorrect mapping of data.&lt;/p&gt;

&lt;p&gt;We should allow the option to create an &lt;tt&gt;ElementMetadataHandler&lt;/tt&gt; which allows empty and/or duplicate values.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 11 Jun 2013 20:09:18 +0000" id="247" opendate="Tue, 11 Jun 2013 20:07:55 +0000">
		<buginformation>
			<summary>Incorrect Cardinality and Case in IPTC Metadata Definition</summary>
			<description>&lt;p&gt;Some of the fields defined in the &lt;tt&gt;IPTC&lt;/tt&gt; interface have incorrect cardinality and metadata key names with incorrect case.&lt;/p&gt;

&lt;p&gt;The change of key names should be done though composite properties which include deprecated versions of the incorrect names as secondary properties for backwards compatibility.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/metadata/IPTC.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 15 Mar 2015 02:10:31 +0000" id="248" opendate="Tue, 18 Jun 2013 23:50:38 +0000">
		<buginformation>
			<summary>Wasted work in WontBeSerializedError.writeObject()</summary>
			<description>&lt;p&gt;The problem appears in version 1.3 and in revision 1494353.  I&lt;br/&gt;
attached a one-line patch that fixes it.&lt;/p&gt;

&lt;p&gt;In method &quot;WontBeSerializedError.writeObject&quot;, the loop over&lt;br/&gt;
&quot;e.getStackTrace()&quot; should break immediately after &quot;found&quot; is set to&lt;br/&gt;
&quot;true&quot;.  All the iterations after &quot;found&quot; is set to &quot;true&quot; do not&lt;br/&gt;
perform any useful work, at best they just set &quot;found&quot; again to&lt;br/&gt;
&quot;true&quot;.&lt;/p&gt;

&lt;p&gt;Method &quot;embedInTempFile&quot; in class &quot;ExternalEmbedderTest&quot; has a similar&lt;br/&gt;
loop (over &quot;embeddedMetadata.getValues(metadataName)&quot;), and this loop&lt;br/&gt;
breaks immediately after &quot;foundExpectedValue&quot; is set to &quot;true&quot;, just&lt;br/&gt;
like in the proposed patch.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/fork/ForkParserIntegrationTest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 5 Dec 2011 03:45:20 +0000" id="249" opendate="Wed, 14 Apr 2010 15:19:41 +0000">
		<buginformation>
			<summary>textbox content extaction for word documents</summary>
			<description>&lt;p&gt;It looks like Tika does not extract text from textbox compenent of word files.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/WordExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 7 Oct 2011 08:57:12 +0000" id="250" opendate="Thu, 19 Aug 2010 18:28:29 +0000">
		<buginformation>
			<summary>ContainerAwareDetector doesn't support truncated Open XML files</summary>
			<description>&lt;p&gt;When I try to run the detector on a truncated Open XML file I get an exception&lt;/p&gt;

&lt;p&gt;java.util.zip.ZipException: error in opening zip file&lt;br/&gt;
	at java.util.zip.ZipFile.open(Native Method)&lt;br/&gt;
	at java.util.zip.ZipFile.&amp;lt;init&amp;gt;(ZipFile.java:114)&lt;br/&gt;
	at java.util.zip.ZipFile.&amp;lt;init&amp;gt;(ZipFile.java:131)&lt;br/&gt;
	at org.apache.tika.detect.ZipContainerDetector.detect(ZipContainerDetector.java:52)&lt;br/&gt;
	at org.apache.tika.detect.ZipContainerDetector.detect(ZipContainerDetector.java:45)&lt;br/&gt;
	at org.apache.tika.detect.ContainerAwareDetector.detect(ContainerAwareDetector.java:77)&lt;br/&gt;
	at org.apache.tika.detect.ContainerAwareDetector.detect(ContainerAwareDetector.java:59)&lt;br/&gt;
	at org.apache.tika.detect.TestContainerAwareDetector.testTruncatedOOXMLFile(TestContainerAwareDetector.java:168)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:597)&lt;br/&gt;
	at junit.framework.TestCase.runTest(TestCase.java:154)&lt;br/&gt;
	at junit.framework.TestCase.runBare(TestCase.java:127)&lt;br/&gt;
	at junit.framework.TestResult$1.protect(TestResult.java:106)&lt;br/&gt;
	at junit.framework.TestResult.runProtected(TestResult.java:124)&lt;br/&gt;
	at junit.framework.TestResult.run(TestResult.java:109)&lt;br/&gt;
	at junit.framework.TestCase.run(TestCase.java:118)&lt;br/&gt;
	at junit.framework.TestSuite.runTest(TestSuite.java:208)&lt;br/&gt;
	at junit.framework.TestSuite.run(TestSuite.java:203)&lt;br/&gt;
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:130)&lt;br/&gt;
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)&lt;br/&gt;
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)&lt;br/&gt;
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)&lt;br/&gt;
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)&lt;br/&gt;
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)&lt;/p&gt;

&lt;p&gt;This is similar to &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-485&quot; title=&quot;ContainerAwareDetector doesn&amp;#39;t support truncated POI files&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-485&quot;&gt;&lt;del&gt;TIKA-485&lt;/del&gt;&lt;/a&gt;, there should be a try-catch around the zipDetector&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/detect/ContainerAwareDetector.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 31 Oct 2011 21:48:01 +0000" id="251" opendate="Thu, 2 Dec 2010 10:35:30 +0000">
		<buginformation>
			<summary>Improved OSGi bundling</summary>
			<description>&lt;p&gt;I'd like to add proper integration tests for tika-bundle and expose the Tika facade object as a service so other bundles could access it easily like this:&lt;/p&gt;

&lt;p&gt;    @Reference&lt;br/&gt;
    private Tika tika;&lt;/p&gt;

&lt;p&gt;It would also be nice to allow other OSGi bundles to expose their Parser implementations as pluggable services and have the Tika bundle automatically pick up and use them along with all the embedded parsers it contains.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-bundle/pom.xml</file>
			<file>/tika-core/pom.xml</file>
			<file>/tika-core/src/main/java/org/apache/tika/config/package-info.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/config/ServiceLoader.java</file>
			<file>/tika-parsers/pom.xml</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/POIFSContainerDetector.java</file>
			<file>/tika-bundle-it/src/test/java/org/apache/tika/bundle/BundleTest.java</file>
			<file>/tika-parent/pom.xml</file>
			<file>/tika-bundle-it/pom.xml</file>
			<file>/pom.xml</file>
			<file>/CHANGES.txt</file>
			<file>/tika-app/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 26 Oct 2011 14:39:26 +0000" id="252" opendate="Tue, 4 Jan 2011 20:08:12 +0000">
		<buginformation>
			<summary>Lithuanian language identification</summary>
			<description>&lt;p&gt;I am attaching Lithuanian language profile, which allows Tika to support Lithuanian language identification.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/language/lt.ngp</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 4 Apr 2012 10:00:06 +0000" id="253" opendate="Tue, 1 Feb 2011 18:39:12 +0000">
		<buginformation>
			<summary>Tika network server</summary>
			<description>&lt;p&gt;It would be cool to be able to run Tika as a network service that accepts a binary document as input and produces the extracted content (as XHTML, text, or just metadata) as output. A bit like &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-169&quot; title=&quot;Tika Web Service Servlet&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-169&quot;&gt;&lt;del&gt;TIKA-169&lt;/del&gt;&lt;/a&gt;, but without the dependency to a servlet container.&lt;/p&gt;

&lt;p&gt;I'd like to be able to set up and run such a server like this:&lt;/p&gt;

&lt;p&gt;    $ java -jar tika-app.jar --port 1234&lt;/p&gt;

&lt;p&gt;We should also add a NetworkParser class that acts as a local client for such a service. This way a lightweight client could use the full set of Tika parsing functionality even with just the tika-core jar within its classpath.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/NOTICE.txt</file>
			<file>/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java</file>
			<file>/tika-server/pom.xml</file>
			<file>/pom.xml</file>
			<file>/tika-app/pom.xml</file>
			<file>/tika-server/src/main/java/org/apache/tika/server/TikaServerCli.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 4 May 2011 01:06:16 +0000" id="254" opendate="Wed, 16 Mar 2011 14:48:29 +0000">
		<buginformation>
			<summary>Error parsing GIF</summary>
			<description>&lt;p&gt;I am getting an exception parsing the following GIF file, which opens OK in firefox, etc. Although since this is a problem in Sun/Oracle code, I would not expect it to be fixed anytime soon!&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;$ java -jar tika-app/target/tika-app-1.0-SNAPSHOT.jar http://sites.google.com/site/keepaesopen/_/rsrc/1271597999559/system/app/images/icon_gadget_tools_dark.gif
Exception in thread &quot;main&quot; org.apache.tika.exception.TikaException: image/gif parse error
        at org.apache.tika.parser.image.ImageParser.parse(ImageParser.java:91)
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:197)
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:197)
        at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:135)
        at org.apache.tika.cli.TikaCLI$OutputType.process(TikaCLI.java:107)
        at org.apache.tika.cli.TikaCLI.process(TikaCLI.java:302)
        at org.apache.tika.cli.TikaCLI.main(TikaCLI.java:91)
Caused by: javax.imageio.IIOException: Unexpected block type 0!
        at com.sun.imageio.plugins.gif.GIFImageReader.readMetadata(GIFImageReader.java:722)
        at com.sun.imageio.plugins.gif.GIFImageReader.getWidth(GIFImageReader.java:167)
        at org.apache.tika.parser.image.ImageParser.parse(ImageParser.java:75)
        ... 6 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/image/ImageParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 5 Oct 2011 17:13:07 +0000" id="255" opendate="Tue, 19 Apr 2011 22:13:12 +0000">
		<buginformation>
			<summary>Few of RTF files not extracting properly</summary>
			<description>&lt;p&gt;Few of the RTF files dont get extracted properly. &lt;br/&gt;
This is the stack trace: &lt;/p&gt;

&lt;p&gt;org.apache.tika.exception.TikaException: &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-198&quot; title=&quot;Better distinction between IOException and TikaException&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-198&quot;&gt;&lt;del&gt;TIKA-198&lt;/del&gt;&lt;/a&gt;: Illegal IOException from org.apache.tika.parser.rtf.RTFParser@616d071a&lt;br/&gt;
at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:203)&lt;br/&gt;
at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:197)&lt;br/&gt;
at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:135)&lt;/p&gt;

&lt;p&gt;Caused by: java.io.IOException: Too many close-groups in RTF text&lt;br/&gt;
at javax.swing.text.rtf.RTFParser.write(RTFParser.java:156)&lt;br/&gt;
at javax.swing.text.rtf.RTFParser.writeSpecial(RTFParser.java:101)&lt;br/&gt;
at javax.swing.text.rtf.AbstractFilter.write(AbstractFilter.java:158)&lt;br/&gt;
at javax.swing.text.rtf.AbstractFilter.readFromStream(AbstractFilter.java:88)&lt;br/&gt;
at javax.swing.text.rtf.RTFEditorKit.read(RTFEditorKit.java:65)&lt;br/&gt;
at org.apache.tika.parser.rtf.RTFParser.parse(RTFParser.java:112)&lt;br/&gt;
at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:197)&lt;/p&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/rtf/RTFParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 24 Jan 2012 14:51:11 +0000" id="256" opendate="Wed, 20 Apr 2011 07:43:34 +0000">
		<buginformation>
			<summary>tika hangs parsing doc file (attached)</summary>
			<description>&lt;p&gt;Tika hangs parsing the word file:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://dl.dropbox.com/u/2371175/testfile002.doc&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://dl.dropbox.com/u/2371175/testfile002.doc&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The current version of tika (0.9) works fine with the same file&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/io/TaggedInputStream.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/detect/POIFSContainerDetector.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/io/CountingInputStream.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 3 May 2011 01:46:22 +0000" id="257" opendate="Mon, 25 Apr 2011 03:33:54 +0000">
		<buginformation>
			<summary>TikaCLI only reports internal meta models with --list-met-models</summary>
			<description>&lt;p&gt;From TikaCLI.java&lt;/p&gt;

&lt;p&gt;        for (Class&amp;lt;?&amp;gt; modelClass: modelClasses) {&lt;br/&gt;
            // we don't care about internal Tika met classes&lt;br/&gt;
            // if we do, then we can take this conditional out&lt;br/&gt;
            if (modelClass.getSimpleName().contains(&quot;Tika&quot;)) {&lt;br/&gt;
                System.out.println(modelClass.getSimpleName());&lt;/p&gt;

&lt;p&gt;The condition is the reverse from what the comment says and needs to be negated.  The way it's written, it only returns the internal ones.&lt;/p&gt;

&lt;p&gt;java -jar tika-app-0.9.jar --list-met-models&lt;br/&gt;
TikaMetadataKeys&lt;br/&gt;
 PROTECTED&lt;br/&gt;
 RESOURCE_NAME_KEY&lt;br/&gt;
TikaMimeKeys&lt;br/&gt;
 MIME_TYPE_MAGIC&lt;br/&gt;
 TIKA_MIME_FILE&lt;/p&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 6 May 2011 01:24:23 +0000" id="258" opendate="Thu, 5 May 2011 07:30:49 +0000">
		<buginformation>
			<summary>Resources not properly closed</summary>
			<description>&lt;p&gt;We have a thread which parser &amp;gt; 200k files, and we always get &quot;too many open files open&quot; error from operating system. Using lsof I noticed tha apache-tika temp files (created by class temporaryFiles) are not really deleted by operating system, even if delete method returns true.&lt;br/&gt;
Searching in the code, I found that the problem (which does not manifest with all the files) is probably in TikaInputStream#close method. Here opencontainer is set to null, but in case of opencontainer instance of org.apache.poi.poifs.filesystem.NPOIFSFileSystem the problems disappear if I call close() on opencontainer. I modified the NPOIFSFileSystem class to implement java.io.Closeable, and modified TikaInputStream#close method to make &lt;/p&gt;

&lt;p&gt;	if (openContainer instanceof java.io.Closeable) &lt;/p&gt;
{
			((java.io.Closeable) openContainer).close();
		}
&lt;p&gt;        openContainer = null;&lt;/p&gt;

&lt;p&gt;I don't know if this is the best solution, but it seems to solve the problem for me.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/detect/ZipContainerDetector.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/io/TikaInputStream.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 21 Aug 2011 17:38:06 +0000" id="259" opendate="Fri, 27 May 2011 10:35:57 +0000">
		<buginformation>
			<summary>Changes to RFC822Parser to support turning off strict parsing</summary>
			<description>&lt;p&gt;Currently in RFC822Parser if Apache-Mime4J fails while parsing any field, then parsing the whole document will fail. This causes problems on the Enron Corpus - see &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-657&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/TIKA-657&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;RFC822Parser is configured from a MimeEntityConfig object. MimeEntityConfig contains an option for &quot;strict parsing&quot;. Currently MailContentHandler only performs strict parsing, I.E. if a MimeException is encountered when processing any fields in MailContentHandler.field then processing the document fails. However, we may prefer not to have strict parsing I.E. continue even if processing one or more fields fails. This can be achieved by placing a try / catch block around the logic inside MailContentHandler.field(), and only rethrowing the error if strictParsing is enabled, otherwise we log the error.&lt;/p&gt;

&lt;p&gt;I enclose a diff for RFC822Parser and MailContentHandler that does this. I have also made some other minor changes to MailContentHandler: there was some repeated code for handling To:, Cc: and Bcc: fields, so I have replaced that with a single private method, and rewritten stripOutFieldPrefix, to avoid manipulating the String using re-assignment. &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/mail/MailContentHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 10 Oct 2011 22:26:45 +0000" id="260" opendate="Tue, 5 Jul 2011 13:10:52 +0000">
		<buginformation>
			<summary>eight new n-gram language profiles</summary>
			<description>&lt;p&gt;Eight new n-gram language profiles added: Belarusian, Catalan, Esperanto, Galician, Romanian, Slovak, Slovenian, and Ukrainian. &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/language/be.ngp</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 1 Sep 2011 17:39:50 +0000" id="261" opendate="Sun, 31 Jul 2011 09:48:00 +0000">
		<buginformation>
			<summary>Temporary file not removed after detection</summary>
			<description>&lt;p&gt;Temporary files created by Tika are not removed in the case the TikaInputStream has been created using a byte array or BufferedInputStream and using the ZipContainerDetector (in our case for Office 2007 documents).&lt;br/&gt;
The fix for bug &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-654&quot; title=&quot;Resources not properly closed&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-654&quot;&gt;&lt;del&gt;TIKA-654&lt;/del&gt;&lt;/a&gt; solves part of the problem (when using file as input) but when the byte array is being used, TikaInputStream will create a temp file (when getFile() is called). This file will be removed when close() is called, but in the ZipDetector a ZipFile is instantiated which also opens a stream to the same temp file. This stream is not closed and therefor the file can not be deleted when TikaInputStream.close() is called.&lt;/p&gt;

&lt;p&gt;See attached patch for unittest and solution.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/detect/TestContainerAwareDetector.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 5 Oct 2011 16:59:05 +0000" id="262" opendate="Fri, 26 Aug 2011 10:21:44 +0000">
		<buginformation>
			<summary>Automatic checks against backwards-incompatible API changes</summary>
			<description>&lt;p&gt;As we get closer to 1.x we should add tooling like the Maven Clirr plugin &lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; to guard against accidental backwards-incompatible API changes.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; &lt;a href=&quot;http://mojo.codehaus.org/clirr-maven-plugin/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://mojo.codehaus.org/clirr-maven-plugin/&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 3 Apr 2012 16:05:34 +0000" id="263" opendate="Tue, 30 Aug 2011 14:58:35 +0000">
		<buginformation>
			<summary>Upgrade to POI 3.8 as available</summary>
			<description>&lt;p&gt;This issue is to track upgrading the POI dependency to 3.8 Final once it's available, and intermediate beta releases before then&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 27 Oct 2011 16:25:13 +0000" id="264" opendate="Thu, 1 Sep 2011 11:32:32 +0000">
		<buginformation>
			<summary>Drop deprecated methods/classes/interfaces</summary>
			<description>&lt;p&gt;The plan so far has been to clean up our public APIs by dropping all deprecated parts before we do the 1.0 release.&lt;/p&gt;

&lt;p&gt;The best way to do this will probably be to create a 0.x branch (and possibly prepare for a 0.10 release from it) right before going through the codebase and removing all deprecated parts. This should ideally be the last thing we do before releasing Tika 1.0.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/config/TikaConfig.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 3 Oct 2011 18:26:11 +0000" id="265" opendate="Fri, 9 Sep 2011 16:40:14 +0000">
		<buginformation>
			<summary>Word parser doesn't extract optional hyphen correctly</summary>
			<description>&lt;p&gt;We seem not to extract the optional hyphen character correctly in&lt;br/&gt;
the Word parser.&lt;/p&gt;

&lt;p&gt;You can create this char in Word by typing ctrl and -.  It's hidden,&lt;br/&gt;
normally; you have to turn on display of formatting marks to see it.&lt;/p&gt;

&lt;p&gt;Ideally we'd get U+00AD (unicode soft hyphen), I think.&lt;/p&gt;

&lt;p&gt;DOC produces a unicode replacement char, which is wrong.&lt;/p&gt;

&lt;p&gt;DOCX and PDF drop the char (which seems acceptable).  RTF produces&lt;br/&gt;
U+2027 (hyphenation point) which also seems OK (in &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-683&quot; title=&quot;RTF Parser issues with non european characters&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-683&quot;&gt;&lt;del&gt;TIKA-683&lt;/del&gt;&lt;/a&gt; it will&lt;br/&gt;
produce U+00AD).&lt;/p&gt;

&lt;p&gt;PPT and PPTX work correctly (U+00AD).&lt;/p&gt;

&lt;p&gt;So DOC is the only bug I think &amp;#8211; I haven't dug into what's wrong&lt;br/&gt;
yet...&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/TestParsers.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 3 Oct 2011 18:26:11 +0000" id="266" opendate="Mon, 12 Sep 2011 17:44:38 +0000">
		<buginformation>
			<summary>Master slide text isn't extracted</summary>
			<description>&lt;p&gt;It looks like we are not getting text from the master slide for PPT&lt;br/&gt;
and PPTX.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/HSLFExtractor.java</file>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/microsoft/PowerPointParserTest.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 3 Oct 2011 10:54:53 +0000" id="267" opendate="Sat, 17 Sep 2011 13:07:26 +0000">
		<buginformation>
			<summary>Comment/annotation is sometimes not extracted</summary>
			<description>&lt;p&gt;When I add an annotation or comment, I see that comment extracted for PPTX, DOC, DOCX, PDF (in one case) but not for RTF, PPT and PDF (in a different case).&lt;/p&gt;

&lt;p&gt;I think I'll just commit the test with all docs, but leave the ones that don't work commented out; we can uncomment once we fix this bug.&lt;/p&gt;

&lt;p&gt;I'll have a look at the RTF comment, but I'm not yet familiar enough w/ the other parsers to dig into them...&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/TestParsers.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/rtf/TextExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 20 Oct 2011 12:56:05 +0000" id="268" opendate="Mon, 19 Sep 2011 18:02:22 +0000">
		<buginformation>
			<summary>PDF text sometimes has extra space between letters</summary>
			<description>&lt;p&gt;I have a PDF with simple text &quot;Here is some formatted text&quot;, but when&lt;br/&gt;
I extract with Tika I get extra spaces inserted:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;H e re  i s  so me  fo rma tte d  te x t
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When I created the text in this PDF (I used the PDFpen tool on OS X),&lt;br/&gt;
I set the style of the text to &quot;loosen&quot; (ie, increase space slightly&lt;br/&gt;
between the letters), so I think Tika (PDFBox) is trying to &quot;respect&quot;&lt;br/&gt;
that whitespace, but it'd be nice to turn this off (if it won't mess&lt;br/&gt;
up other places where we DO want the whitespace).&lt;/p&gt;

&lt;p&gt;When I copy/paste the text is copied correctly.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 2 Mar 2015 05:09:23 +0000" id="269" opendate="Thu, 22 Sep 2011 10:17:28 +0000">
		<buginformation>
			<summary>Improve the outputed XHTML by HSLFExtractor</summary>
			<description>&lt;p&gt;The XHTML output of HSLFExtractor parser is not pure XHTML, it only inserts the full text into a P&lt;span class=&quot;error&quot;&gt;&amp;#91;aragraph&amp;#93;&lt;/span&gt; tag (including non-html carriage returns).  This behavior comes from the poor capabilities that the POI PowerPointExtractor offers.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/HSLFExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 26 Sep 2011 10:39:28 +0000" id="270" opendate="Mon, 26 Sep 2011 09:56:05 +0000">
		<buginformation>
			<summary>NPE in WordExtractor.handleParagraph()</summary>
			<description>&lt;p&gt;Not sure if it is a duplicate or not..&lt;/p&gt;

&lt;p&gt;java.lang.NullPointerException&lt;br/&gt;
	at org.apache.tika.parser.microsoft.WordExtractor.handleParagraph(WordExtractor.java:161)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.WordExtractor.parse(WordExtractor.java:87)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.OfficeParser.parse(OfficeParser.java:200)&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/WordExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 26 Sep 2011 15:56:45 +0000" id="271" opendate="Mon, 26 Sep 2011 15:52:50 +0000">
		<buginformation>
			<summary>Upgrade to Commons Codec 1.5</summary>
			<description>&lt;p&gt;As noted by Konstantin Gribov on dev@, the POI version we use expects Commons Codec 1.5 while Tika is still using 1.4. Best to upgrade.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 28 Oct 2011 18:03:41 +0000" id="272" opendate="Sat, 1 Oct 2011 10:32:15 +0000">
		<buginformation>
			<summary>OpenOffice parser: master footer text isn't extracted</summary>
			<description>&lt;p&gt;If I edit the footer text on the master slide of an OpenOffice presentation, I see that text rendered on the slide, but it's not extracted by Tika.&lt;/p&gt;

&lt;p&gt;Digging into the document, curiously the footer text is in the styles.xml, under office:master-styles -&amp;gt; style:master-page -&amp;gt; draw:frame -&amp;gt; draw:text-box -&amp;gt; text&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/tongue.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.  I think somehow we're not linking up each slide's master text elements to that slide, similar to &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-712&quot; title=&quot;Master slide text isn&amp;#39;t extracted&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-712&quot;&gt;TIKA-712&lt;/a&gt;.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 26 Nov 2011 19:54:36 +0000" id="273" opendate="Mon, 3 Oct 2011 10:54:07 +0000">
		<buginformation>
			<summary>Tika fails to extract text from PDF annotations</summary>
			<description>&lt;p&gt;Spinoff from &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-717&quot; title=&quot;Comment/annotation is sometimes not extracted&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-717&quot;&gt;&lt;del&gt;TIKA-717&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/TestParsers.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 5 Oct 2011 13:52:09 +0000" id="274" opendate="Mon, 3 Oct 2011 19:10:03 +0000">
		<buginformation>
			<summary>For certain DWG files, the Tika content parser outputs garbage</summary>
			<description>&lt;p&gt;I'm using Solr version 3.4.  After I index the attached file, Solr displays an error message if it is included in the search results because of malformed XML.  When I extract the file using Solr's extractOnly option, I get results back that look corrupted to me (see attached).&lt;/p&gt;

&lt;p&gt;I observed the same behavior with Solr version 3.3.&lt;/p&gt;

&lt;p&gt;The exact URL that I used to extract the content is (before I URL encode it):  &lt;a href=&quot;http://localhost:8983/solr/update/extract?extractOnly=true&amp;amp;literal.type=file&amp;amp;literal.id=9a7ab433616746aaab526d77564b916f&amp;amp;literal.name=3D&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://localhost:8983/solr/update/extract?extractOnly=true&amp;amp;literal.type=file&amp;amp;literal.id=9a7ab433616746aaab526d77564b916f&amp;amp;literal.name=3D&lt;/a&gt; Dacor Modern Kitchen.dwg&amp;amp;resource.name=3D Dacor Modern Kitchen.dwg&amp;amp;literal.createddate=2010-08-19T17:32:48.277Z&amp;amp;literal.modifieddate=2010-08-19T17:32:49.996Z&amp;amp;literal.size=452832&amp;amp;literal.versionnumber=0&amp;amp;literal.ownerid=92a7271bfa3c4639993c4652ef7e922b&amp;amp;literal.creatorid=201008051854838&amp;amp;literal.viewerids=201008051854838&amp;amp;literal.viewerids=201006231721543&amp;amp;literal.viewerids=201011041924210&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 5 Oct 2011 15:14:05 +0000" id="275" opendate="Tue, 4 Oct 2011 01:16:14 +0000">
		<buginformation>
			<summary>&quot;Zip bomb&quot; (XML nesting) detection is too strict</summary>
			<description>&lt;p&gt;I get &quot;zip bomb&quot; errors from many HTML documents, e.g. &lt;a href=&quot;http://www.akhbaar.org/wesima_articles/index-20100101-82736.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.akhbaar.org/wesima_articles/index-20100101-82736.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Is there a way that the element nesting level could be made configurable? 30 elements just doesn't seem to be enough.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/sax/SecureContentHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 5 Oct 2011 10:43:54 +0000" id="276" opendate="Tue, 4 Oct 2011 10:30:45 +0000">
		<buginformation>
			<summary>PDF2XHTML fails to insert &lt;p&gt; nor space around page marker</summary>
			<description>&lt;p&gt;I have a test document (unfortunately not committable) whose page&lt;br/&gt;
numbers are rendered with no separator (&amp;lt;p&amp;gt; nor space) before the next&lt;br/&gt;
word.  So I have words like:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;1Massachusetts&lt;/li&gt;
	&lt;li&gt;2Course&lt;/li&gt;
	&lt;li&gt;3also&lt;/li&gt;
	&lt;li&gt;4The&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;But then when I ran the ExtractText -html command-line from PDFBox, I&lt;br/&gt;
can see that &amp;lt;p&amp;gt; is inserted after these page numbers (spookily, not&lt;br/&gt;
closing the previous &amp;lt;p&amp;gt;; I opened &lt;a href=&quot;https://issues.apache.org/jira/browse/PDFBOX-1130&quot; title=&quot;ExtractText -html doesn&amp;#39;t always close the &amp;lt;p&amp;gt; tags it opens&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PDFBOX-1130&quot;&gt;&lt;del&gt;PDFBOX-1130&lt;/del&gt;&lt;/a&gt; for that).&lt;/p&gt;

&lt;p&gt;So I made a simple change to Tika's PDF2XHTML, to have it override the&lt;br/&gt;
writeStart/EndParagraph, and call handler.start/EndElement(&quot;p&quot;), ie to&lt;br/&gt;
preserve the paragraph structure that PDFBOX detects out to the&lt;br/&gt;
resulting XHTML handler, and this fixes the issue (I now see the page&lt;br/&gt;
number as a separate paragraph, rendered w/ newline in &quot;text&quot; mode&lt;br/&gt;
from TikaCLI).&lt;/p&gt;

&lt;p&gt;Note that this test document is the same document from &lt;a href=&quot;https://issues.apache.org/jira/browse/PDFBOX-1129&quot; title=&quot;Quote glyphs (quoteright, quotedblright, etc.) not mapped to the right Unicode character&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PDFBOX-1129&quot;&gt;&lt;del&gt;PDFBOX-1129&lt;/del&gt;&lt;/a&gt;&lt;br/&gt;
(there are some quote characters that are not extracted correctly).&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDF2XHTML.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 5 Oct 2011 13:11:04 +0000" id="277" opendate="Wed, 5 Oct 2011 13:04:53 +0000">
		<buginformation>
			<summary>Upgrade to Apache parent POM version 10</summary>
			<description>&lt;p&gt;Apache parent POM version 10 was released a few months ago. It upgrades various plugin versions and makes the useAgent option of the GPG plugin configurable.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parent/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 5 Oct 2011 17:08:48 +0000" id="278" opendate="Wed, 5 Oct 2011 17:06:34 +0000">
		<buginformation>
			<summary>Drop support for Java 1.4</summary>
			<description>&lt;p&gt;Since &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-175&quot; title=&quot;Retrotranslate Tika for use in Java 1.4 environments&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-175&quot;&gt;&lt;del&gt;TIKA-175&lt;/del&gt;&lt;/a&gt; we've been supporting clients that still need to run on Java 1.4 environments. I guess nobody is on Java 1.4 anymore, so it should be safe to drop the retrotranslator settings.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parent/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 6 Oct 2011 15:32:57 +0000" id="279" opendate="Thu, 6 Oct 2011 15:31:33 +0000">
		<buginformation>
			<summary>MP3 parser should handle genres not in ID3v1</summary>
			<description>&lt;p&gt;The MP3 parser currently prefers id3v1 genres (for normalisation), but doesn't cope with id3v2 genres not in v1.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/mp3/ID3v22Handler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 26 Oct 2011 10:54:42 +0000" id="280" opendate="Thu, 6 Oct 2011 20:27:47 +0000">
		<buginformation>
			<summary>Support custom mime types</summary>
			<description>&lt;p&gt;As discussed over the summer &amp;lt;&lt;a href=&quot;http://lucene.472066.n3.nabble.com/Appending-Mime-Types-td3266434.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.472066.n3.nabble.com/Appending-Mime-Types-td3266434.html&lt;/a&gt;&amp;gt; there are legitimate cases for wanting to load in extra, custom mimetypes (and their matching rules) to Tika&lt;/p&gt;

&lt;p&gt;Discussions seem to conclude that the built in tika-mimetypes file should be used for common and public mimetypes, and people wanting to support additional public formats should open an issue to have them in the main file. People who want only a very restricted set of mimetypes can use a custom tika config with a limited, smaller mimetypes file&lt;/p&gt;

&lt;p&gt;For people who want to load one or two extra, likely custom mimetypes, we should provide a service loading system to pull in the extra mimetypes. This allows for the regular mimetypes file to be used for most files, and the extra custom ones merged in as needed. It also allows for a custom parser to provide the mimetype detection for the specific custom formats it handles&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/mime/MimeTypesFactory.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/mime/MimeTypes.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 30 Jun 2012 14:16:11 +0000" id="281" opendate="Thu, 6 Oct 2011 21:16:01 +0000">
		<buginformation>
			<summary>Ogg Vorbis and FLAC Parsers</summary>
			<description>&lt;p&gt;As mentioned on the list a few weeks back, I've now finished getting an Ogg Vorbis and FLAC parser working. The code is available at &lt;a href=&quot;https://github.com/Gagravarr/VorbisJava&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/Gagravarr/VorbisJava&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As I see it, there are three options for getting it into Tika:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Roll a release of the whole project from GitHub, upload jars to Maven Central, and add a parser pom dependency&lt;/li&gt;
	&lt;li&gt;Bring the parser code over to Tika, and release (+post to Central) just the core jar&lt;/li&gt;
	&lt;li&gt;Bring the whole lot over to Apache (maybe to Tika, maybe elsewhere?)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This issue is to track the plan, and the inclusion of the parsers into Tika one way or another&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-bundle/pom.xml</file>
			<file>/tika-app/pom.xml</file>
			<file>/tika-parsers/src/test/resources/test-documents/testFLAC.flac</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 10 Oct 2011 18:15:12 +0000" id="282" opendate="Fri, 7 Oct 2011 17:48:22 +0000">
		<buginformation>
			<summary>RTF parser fails to extract the body</summary>
			<description>&lt;p&gt;Using tika-app I'm getting the following result of parsing the attached document:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;&amp;lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot;&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;meta name=&quot;subject&quot; content=&quot;tests&quot;/&amp;gt;
&amp;lt;meta name=&quot;Content-Length&quot; content=&quot;2235&quot;/&amp;gt;
&amp;lt;meta name=&quot;comment&quot; content=&quot;StarWriter&quot;/&amp;gt;
&amp;lt;meta name=&quot;X-Parsed-By&quot; content=&quot;org.apache.tika.parser.DefaultParser&quot;/&amp;gt;
&amp;lt;meta name=&quot;X-Parsed-By&quot; content=&quot;org.apache.tika.parser.rtf.RTFParser&quot;/&amp;gt;
&amp;lt;meta name=&quot;Content-Type&quot; content=&quot;application/rtf&quot;/&amp;gt;
&amp;lt;meta name=&quot;resourceName&quot; content=&quot;test.rtf&quot;/&amp;gt;
&amp;lt;title&amp;gt;test rft document&amp;lt;/title&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body/&amp;gt;&amp;lt;/html&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The expected result would be a non-empty body containing the text &quot;The quick brown fox jumps over the lazy dog&lt;br/&gt;
&quot;.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/rtf/TextExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 7 Oct 2011 21:05:45 +0000" id="283" opendate="Fri, 7 Oct 2011 20:50:33 +0000">
		<buginformation>
			<summary>Avoid using POI's LittleEndian in non-POI parsers</summary>
			<description>&lt;p&gt;Several of our Parsers use org.apache.poi.util.LittleEndian to read in little endian numbers as part of processing&lt;/p&gt;

&lt;p&gt;We should pull over this code from POI, add a big endian version, then use that in our (non POI) parsers instead&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/io/EndianUtils.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 10 Oct 2011 21:40:16 +0000" id="284" opendate="Sun, 9 Oct 2011 04:50:59 +0000">
		<buginformation>
			<summary>JavaDoc of Tika XPathParser should mention descendant:node()</summary>
			<description>&lt;p&gt;I was trying to figure out why my xpath expressions weren't working in Solr Cell (a Tika adapter to Solr) and it turns out that it's because of Tika's more limited XPath support. I was hoping to find documentation on what is supported. The best I found was the javadocs here: &lt;a href=&quot;http://tika.apache.org/0.8/api/org/apache/tika/sax/xpath/XPathParser.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://tika.apache.org/0.8/api/org/apache/tika/sax/xpath/XPathParser.html&lt;/a&gt;  It lists the supported XPath axis but it fails to mention that document:node() is supported too.  As an aside, I believe this is erroneous as there should be two colons.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/sax/BodyContentHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 12 Oct 2011 19:19:17 +0000" id="285" opendate="Wed, 12 Oct 2011 12:55:00 +0000">
		<buginformation>
			<summary>Small improvements to how embedded docs are parsed in AbstractPOIFSExtractor.handleEmbeddedOfficeDoc</summary>
			<description>&lt;p&gt;I noticed some minor things in this method:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;It does too much work (writes the tmpFile out) if the&lt;br/&gt;
    EmbeddedDocumentExtractor didn't want to actually parse file&lt;br/&gt;
    file.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;It writes the tmpFile when it won't use it in the OLE10_NATIVE&lt;br/&gt;
    case (because we use a TikeInputStream from the in-RAM byte[]&lt;br/&gt;
    instead).&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Also I fixed a typo in the method name (embeded -&amp;gt; embedded) &amp;#8211; is&lt;br/&gt;
that OK?  It's a protected method, and a few of the office parsers&lt;br/&gt;
invoke it.&lt;/p&gt;

&lt;p&gt;Finally I cutover to TemporaryResources to track the possible tmpFile&lt;br/&gt;
and open TikaInputStream against it.&lt;/p&gt;

&lt;p&gt;Separately, it's inefficient now that we must serialize a sub-dir&lt;br/&gt;
(DirectoryEntry) in the NPOIFileSystem to a tmp file only to re-parse&lt;br/&gt;
it back to an NPOIFileSystem in OfficeParser; I'd like to look into&lt;br/&gt;
instead (somehow) directly passing the NPOIFileSystem's DirectoryEntry&lt;br/&gt;
to OfficeParser... but that looks like a bigger change.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/AbstractPOIFSExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 13 Oct 2011 19:14:03 +0000" id="286" opendate="Thu, 13 Oct 2011 15:26:12 +0000">
		<buginformation>
			<summary>Typo in timezone used in Metadata.iso8601Format</summary>
			<description>&lt;p&gt;The timezone string is &quot;UTF&quot; when it should be &quot;UTC&quot;.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/metadata/Metadata.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 20 Oct 2011 12:37:00 +0000" id="287" opendate="Fri, 14 Oct 2011 18:27:13 +0000">
		<buginformation>
			<summary>Improve performance when parsing embedded Office docs</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 18 Oct 2011 20:59:04 +0000" id="288" opendate="Tue, 18 Oct 2011 10:56:07 +0000">
		<buginformation>
			<summary>Add getDetector() method to TikaConfig</summary>
			<description>&lt;p&gt;As discussed on the mailing list, we should add a getDetector() method to TikaConfig. This would return a DefaultDetector that was created with the same classloader as the DefaultParser was&lt;/p&gt;

&lt;p&gt;As part of this, we should update the Tika class to get the DefaultDetector from the TikaConfig, rather than creating internally. We should also switch the Tika class to not create its own AutoDetectParser, but instead use the DefaultParser from TikaConfig&lt;/p&gt;

&lt;p&gt;Discussion is:&lt;br/&gt;
&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/tika-dev/201110.mbox/%3Calpine.DEB.2.00.1110171330160.7762@urchin.earth.li%3E&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://mail-archives.apache.org/mod_mbox/tika-dev/201110.mbox/%3Calpine.DEB.2.00.1110171330160.7762@urchin.earth.li%3E&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/Tika.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 29 Jun 2015 08:19:47 +0000" id="289" opendate="Tue, 18 Oct 2011 18:40:15 +0000">
		<buginformation>
			<summary>XMP output from Tika CLI</summary>
			<description>&lt;p&gt;It would be great if the Tika CLI could output metadata also in the XMP format.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/main/appended-resources/META-INF/LICENSE</file>
			<file>/tika-xmp/src/main/java/org/apache/tika/xmp/XMPMetadata.java</file>
			<file>/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java</file>
			<file>/tika-xmp/src/main/java/org/apache/tika/xmp/convert/MSOfficeXMLConverter.java</file>
			<file>/pom.xml</file>
			<file>/tika-xmp/pom.xml</file>
			<file>/tika-app/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 24 Jan 2012 14:49:44 +0000" id="290" opendate="Fri, 21 Oct 2011 14:05:28 +0000">
		<buginformation>
			<summary>NPE XHTMLContentHandler in characters Method</summary>
			<description>&lt;p&gt;The method:&lt;/p&gt;

&lt;p&gt;    public void characters(String characters) throws SAXException &lt;/p&gt;
{
        characters(characters.toCharArray(), 0, characters.length());
    }

&lt;p&gt;does not check for null values.&lt;br/&gt;
On many code references a check is done &quot;before&quot; calling this methd. However on other sides, e.g. HSLFExtractor some values are not checked:&lt;/p&gt;

&lt;p&gt;xhtml.characters( comment.getAuthor() );&lt;/p&gt;

&lt;p&gt;which may be null.&lt;/p&gt;

&lt;p&gt;The simplest fix would be to check for null on the handler and if it is null handle it as NOOP or insert the new UTF-8 &quot;replacement char&quot; to let the user decide.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/sax/XHTMLContentHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 28 Oct 2011 16:09:10 +0000" id="291" opendate="Mon, 24 Oct 2011 13:16:10 +0000">
		<buginformation>
			<summary>Provide version number by CLI argument -V</summary>
			<description>&lt;p&gt;I'd like to get the Apache Tika version number through CLI argument -V or --version. The patch is trivial and basically finished. The only thing missing (because Java is not my native programming language) is the actual version number. Any hints where I can get that from?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java</file>
			<file>/tika-app/pom.xml</file>
			<file>/tika-parent/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 1 Nov 2011 15:21:35 +0000" id="292" opendate="Fri, 28 Oct 2011 16:56:13 +0000">
		<buginformation>
			<summary>Update license metadata</summary>
			<description>&lt;p&gt;I was going through our dependency tree, and realized that we haven't updated the LICENSE files in tika-app and tika-bundle to reflect some of the more recent additions. We need to bring this license metadata up to date before releasing 1.0.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/main/appended-resources/META-INF/LICENSE</file>
			<file>/tika-app/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 2 Nov 2011 13:08:17 +0000" id="293" opendate="Sat, 29 Oct 2011 02:19:28 +0000">
		<buginformation>
			<summary>OpenDocumentMetaParser should use common metadata keys for document statistics</summary>
			<description>&lt;p&gt;The OpenDocumentMetaParser currently outputs a number of document statistics with its own Metadata keys, rather than using the standard ones defined on the Metadata class. It should be updated to output the common ones, and once people have updated then remove the current custom ones&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/odf/OpenDocumentMetaParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 2 Nov 2011 13:01:23 +0000" id="294" opendate="Wed, 2 Nov 2011 10:09:51 +0000">
		<buginformation>
			<summary>Upgrade to Commons Compress 1.3</summary>
			<description>&lt;p&gt;Commons Compress 1.3 was just released. The release announcement summarizes the main changes:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Version 1.3 is adds support for Pack200, read-only support for the Unix dump format and transparent support for Zip64 extensions.  It is the first version to require Java 5.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We should upgrade the dependency in Tika.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 5 Nov 2011 19:31:21 +0000" id="295" opendate="Sun, 6 Nov 2011 10:44:32 +0000">
		<buginformation>
			<summary>.NET version of Tika</summary>
			<description>&lt;p&gt;As a followup to &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-212&quot; title=&quot;Do you have Tika in .NET?&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-212&quot;&gt;&lt;del&gt;TIKA-212&lt;/del&gt;&lt;/a&gt; and inspired by efforts like &lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;, I'd like to set up a .NET version of Tika based on IKVM and NPanday. The goal would be to produce a Tika DLL that contains all the parser libraries and can be used natively in any .NET environment with some API sugar on top to make the Tika facade class work more smoothly with .NET (for example, use System.IO.FileInfo instead of java.io.File).&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; &lt;a href=&quot;http://blogs.dovetailsoftware.com/blogs/kmiller/archive/2010/07/02/using-the-tika-java-library-in-your-net-application-with-ikvm&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://blogs.dovetailsoftware.com/blogs/kmiller/archive/2010/07/02/using-the-tika-java-library-in-your-net-application-with-ikvm&lt;/a&gt; &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-dotnet/pom.xml</file>
			<file>/pom.xml</file>
			<file>/tika-dotnet/.gitignore</file>
			<file>/tika-dotnet/AssemblyInfo.cs</file>
			<file>/tika-app/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 7 Feb 2012 19:17:25 +0000" id="296" opendate="Thu, 2 Feb 2012 15:23:55 +0000">
		<buginformation>
			<summary>Tika add parsing support for ANPA-1312 news wire feeds</summary>
			<description>&lt;p&gt;This submission adds support for ANPA-1312 news wire feeds.&lt;/p&gt;

&lt;p&gt;Those feeds are the formats used by AP, AFP, NYT, Reuters in their daily news wire broadcasts.&lt;/p&gt;

&lt;p&gt;This was a pretty significant development effort, so am happy to share back as a thank you to the TIKA community. &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/iptc/IptcAnpaParser.java</file>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 7 Jul 2012 19:45:25 +0000" id="297" opendate="Fri, 13 Nov 2009 04:10:50 +0000">
		<buginformation>
			<summary>Improve encoding detection speed and accuracy</summary>
			<description>&lt;p&gt;The encoding detection code we took from ICU4J is not very efficient and sometimes produces odd results when more than one encoding matches the given input data. It would be good to refactor the code to be faster for easy-to-detect encodings and to have better heuristics in case multiple matches are found.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/main/appended-resources/META-INF/LICENSE</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/html/HtmlParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 8 Jul 2012 22:47:29 +0000" id="298" opendate="Fri, 21 May 2010 17:57:20 +0000">
		<buginformation>
			<summary>Tika currently misuses the HTTP Content-Encoding header, and does not seem to use the charset part of the Content-Type header properly.</summary>
			<description>&lt;p&gt;Tika currently misuses the HTTP Content-Encoding header, and does not seem to use the charset part of the Content-Type header properly.&lt;/p&gt;

&lt;p&gt;Content-Encoding is not for the charset. It is for values like gzip, deflate, compress, or identity.&lt;/p&gt;

&lt;p&gt;Charset is passed in with the Content-Type. For instance: text/html; charset=iso-8859-1&lt;/p&gt;

&lt;p&gt;Tika should, in my opinion, do the following:&lt;/p&gt;

&lt;p&gt;1. Stop using Content-Encoding, unless it wants me to be able to pass in gzipped content in an input stream.&lt;/p&gt;

&lt;p&gt;2. Parse and understand charset=... declarations if passed in the Metadata object&lt;/p&gt;

&lt;p&gt;3. Return charset=... declarations in the Metadata object if a charset is detected.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/mime/MediaTypeRegistry.java</file>
			<file>/.gitattributes</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 8 Jul 2012 12:05:56 +0000" id="299" opendate="Sat, 24 Jul 2010 01:36:25 +0000">
		<buginformation>
			<summary>Avoid Charset name bottleneck when multiple threads are using HtmlParser</summary>
			<description>&lt;p&gt;As reported by a user on the Nutch list, if there are lots of threads all parsing HTML documents, there's a lock contention issue caused by a JVM-wide lock used when resolving charset names:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Apparently this is a known issue with Java, and a couple articles are&lt;br/&gt;
written about it:&lt;br/&gt;
&lt;a href=&quot;http://paul.vox.com/library/post/the-mysteries-of-java-character-set-perform&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://paul.vox.com/library/post/the-mysteries-of-java-character-set-perform&lt;/a&gt;&lt;br/&gt;
ance.html &lt;br/&gt;
&lt;a href=&quot;http://halfbottle.blogspot.com/2009/07/charset-continued-i-wrote-about.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://halfbottle.blogspot.com/2009/07/charset-continued-i-wrote-about.html&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;There is also a note in java bug database about scaling issues with the&lt;br/&gt;
class...&lt;br/&gt;
Please also note that the current implementation of&lt;br/&gt;
sun.nio.cs.FastCharsetProvider.charsetForName() uses a JVM-wide lock and is&lt;br/&gt;
called very often (e.g. by new String(byte[] data,String encoding)). This&lt;br/&gt;
JVM-wide lock means that Java applications do not scale beyond 4 CPU cores.&lt;/p&gt;

&lt;p&gt;I noted in the case of my stack at this particular point in time.  The&lt;br/&gt;
BLOCKED calls to charsetForName were generated by:&lt;/p&gt;

&lt;p&gt;at org.apache.tika.parser.html.HtmlParser.getEncoding(HtmlParser.java:84) 378&lt;br/&gt;
at org.apache.tika.parser.html.HtmlParser.getEncoding(HtmlParser.java:99) 61&lt;br/&gt;
at org.apache.tika.parser.html.HtmlParser.getEncoding(HtmlParser.java:133) 19 &lt;br/&gt;
at org.apache.nutch.parse.html.HtmlParser.sniffCharacterEncoding(HtmlParser.java:86)  238&lt;br/&gt;
...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We now have a CharsetUtils class in Tika, and we could add a cache for validated names in the isSupported() method.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/config/ServiceLoader.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/utils/CharsetUtils.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/txt/UniversalEncodingDetector.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/detect/EncodingDetector.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 30 Jun 2012 15:40:33 +0000" id="300" opendate="Tue, 7 Sep 2010 22:06:10 +0000">
		<buginformation>
			<summary>Parser for font files</summary>
			<description>&lt;p&gt;The FontBox library used by PDFBox supports various kinds of font information files. These files don't typically contain much useful textual data, but they do have interesting metadata that should be made available also through Tika.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/font/AdobeFontMetricParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 7 Jul 2012 16:05:28 +0000" id="301" opendate="Thu, 25 Nov 2010 19:34:23 +0000">
		<buginformation>
			<summary>Support EMLX file detection</summary>
			<description>&lt;p&gt;Apple Mail generates email files in .emlx format. They roughly resemble standard rfc822 .eml files but are different.&lt;br/&gt;
On the first line they have the content length in bytes,&lt;br/&gt;
then on the second line, normal rfc822 content starts&lt;br/&gt;
and afterwards there is some XML metadata.&lt;/p&gt;

&lt;p&gt;I would suggest to add support for .emlx files to tika-mimetypes.xml. Just copy the message/rfc822 definitions and state that they should appear at offsets 3:10, this should be enough to accomodate the the content length on the first line. Any reasonable email should be longer than 9 bytes. In this case the first line would have two bytes, then the line break, and normal rfc822 headers can start at offset 4. This will work for emails up to 99 MB, (99 999 999 bytes). &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/.gitattributes</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 1 Jul 2012 21:23:32 +0000" id="302" opendate="Thu, 20 Oct 2011 12:36:06 +0000">
		<buginformation>
			<summary>Address TODOs when we upgrade to next POI release (3.8 beta 5)</summary>
			<description>&lt;p&gt;I'm opening a blanket issue to remind us all to address the TODOs in the sources for when we upgrade to the next POI.&lt;/p&gt;

&lt;p&gt;I think this (a single blanket issue) is better than keeping separate issues open even though they are technically fixed?&lt;/p&gt;

&lt;p&gt;For example, I've committed &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-753&quot; title=&quot;Improve performance when parsing embedded Office docs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-753&quot;&gt;&lt;del&gt;TIKA-753&lt;/del&gt;&lt;/a&gt; (speedups for embedded office docs), yet it included some TODOs for further speedups possible once we upgrade POI.  Rather than keeping &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-753&quot; title=&quot;Improve performance when parsing embedded Office docs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-753&quot;&gt;&lt;del&gt;TIKA-753&lt;/del&gt;&lt;/a&gt; (and others like it) open, I think we should resolve them and let this issue cover all the TODOs.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/extractor/ParsingEmbeddedDocumentExtractor.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ExcelExtractor.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/WordExtractor.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/AbstractPOIFSExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 2 Mar 2015 20:13:04 +0000" id="303" opendate="Thu, 20 Oct 2011 12:51:30 +0000">
		<buginformation>
			<summary>Address TODOs when we upgrade to next PDFBox release</summary>
			<description>&lt;p&gt;Like &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-757&quot; title=&quot;Address TODOs when we upgrade to next POI release (3.8 beta 5)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-757&quot;&gt;&lt;del&gt;TIKA-757&lt;/del&gt;&lt;/a&gt; for POI, I'm opening this blanket issue to address any TODOs in the code when we next upgrade PDFBox.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDF2XHTML.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 21 Nov 2011 13:16:06 +0000" id="304" opendate="Mon, 21 Nov 2011 02:42:11 +0000">
		<buginformation>
			<summary>Tika CLI --detect returns incorrect content-type for files with altered extensions</summary>
			<description>&lt;p&gt;From a discussion on the user mailing list on Nov. 11 2011, where the following was requested as a new bug: Tika CLI will return incorrect content type information when called with --detect for files that have had their extensions modified (and nothing else).  MS Word (.doc) documents that have their extension changed to .xls or .ppt will be incorrectly detected as Excel or PowerPoint documents, whereas the --metadata option will determine the content type correctly (as application/msword), based on the actual contents of these mis-named files.  The same also occurs with other types of MS Office 2003 documents, and could possibly occur with a wide range of document types.  To quote Nick B., from the user mailing list: &quot;If you look at the TestMediaTypes class you'll see what you can get with just the mime magic and filenames, and then there's TestContainerAwareDetector which shows the correct detection happening by using the extra detectors available&quot;.   &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/detect/DefaultDetector.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 14 Dec 2011 13:23:14 +0000" id="305" opendate="Fri, 25 Nov 2011 15:46:59 +0000">
		<buginformation>
			<summary>Fix the detection of protected OOXML files</summary>
			<description>&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-437&quot; title=&quot;OfficeParser: support for write-protected xlsx files&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-437&quot;&gt;&lt;del&gt;TIKA-437&lt;/del&gt;&lt;/a&gt; patch allowed Tika to work with OOXML files protected with the default VelvetSweatshop password. I feel there is room for improvement.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;The POIFSContainerDetector lies when it sees such a file. It should be able to mark it as x-tika-ooxml&lt;/li&gt;
	&lt;li&gt;The OOXMLParser can't work with such a file. It should:
	&lt;ol&gt;
		&lt;li&gt;If it's protected with the default password - it should be decrypted and processed normally.&lt;/li&gt;
		&lt;li&gt;If it's protected with a non-default password - the file should be marked as protected, no weird exceptions should appear.&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Therefore I'd like to add an 'if' to POIFSContainerDetector which returns x-tika-ooxml, and some code to OOXMLParser, which would be similar to the code currently residing in OfficeParser. After this improvement both the OfficeParser and the OOXMLParser will treat such files in the same way.&lt;/p&gt;

&lt;p&gt;When I have that, I can add a hack in my application, which will say &quot;If the type is x-tika-ooxml and the name-based detection is a specialization of ooxml, then use the name-based detection&quot;. This will be a workaround for the fact that in MimeTypes, magic always trumps the name. With that, the encrypted DOCX files will appear with the normal DOCX mimetype in my app.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/resources/test-documents/testEXCEL_protected_passtika.xls</file>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 6 Dec 2011 01:14:06 +0000" id="306" opendate="Mon, 5 Dec 2011 11:23:52 +0000">
		<buginformation>
			<summary>mark/reset not supported from POIFSContainerDetector</summary>
			<description>&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;bash-3.2$ touch test.txt
bash-3.2$ zip test.zip test.txt
  adding: test.txt (stored 0%)
bash-3.2$ java -jar tika-app-1.1-SNAPSHOT.jar -z test.zip
Exception in thread &lt;span class=&quot;code-quote&quot;&gt;&quot;main&quot;&lt;/span&gt; org.apache.tika.exception.TikaException: TIKA-198: Illegal IOException from org.apache.tika.parser.pkg.PackageParser@2d58f9d3
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:249)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:243)
	at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)
	at org.apache.tika.cli.TikaCLI$OutputType.process(TikaCLI.java:130)
	at org.apache.tika.cli.TikaCLI.process(TikaCLI.java:397)
	at org.apache.tika.cli.TikaCLI.main(TikaCLI.java:101)
Caused by: java.io.IOException: mark/reset not supported
	at java.io.InputStream.reset(InputStream.java:330)
	at org.apache.tika.parser.microsoft.POIFSContainerDetector.detect(POIFSContainerDetector.java:116)
	at org.apache.tika.detect.CompositeDetector.detect(CompositeDetector.java:61)
	at org.apache.tika.cli.TikaCLI$FileEmbeddedDocumentExtractor.parseEmbedded(TikaCLI.java:676)
	at org.apache.tika.parser.pkg.PackageExtractor.unpack(PackageExtractor.java:167)
	at org.apache.tika.parser.pkg.PackageExtractor.parse(PackageExtractor.java:96)
	at org.apache.tika.parser.pkg.PackageParser.parse(PackageParser.java:64)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:243)
	... 5 more
bash-3.2$ 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pkg/PackageExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 9 Dec 2011 18:49:17 +0000" id="307" opendate="Mon, 5 Dec 2011 12:36:26 +0000">
		<buginformation>
			<summary>ContentHandlerDecorator outputs invalid element</summary>
			<description>&lt;ul&gt;
	&lt;li&gt;Start Tika GUI&lt;/li&gt;
	&lt;li&gt;try opening test-outlook.msg (from tika-parsers test resources)&lt;/li&gt;
	&lt;li&gt;the following exception is thrown:
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;org.apache.tika.exception.TikaException: Unexpected RuntimeException from org.apache.tika.parser.microsoft.OfficeParser@12e14ebc
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:245)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:243)
	at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)
	at org.apache.tika.gui.TikaGUI.handleStream(TikaGUI.java:320)
	at org.apache.tika.gui.TikaGUI.openFile(TikaGUI.java:279)
	at org.apache.tika.gui.TikaGUI.actionPerformed(TikaGUI.java:238)
	at javax.swing.AbstractButton.fireActionPerformed(AbstractButton.java:2028)
	at javax.swing.AbstractButton$Handler.actionPerformed(AbstractButton.java:2351)
	at javax.swing.DefaultButtonModel.fireActionPerformed(DefaultButtonModel.java:387)
	at javax.swing.DefaultButtonModel.setPressed(DefaultButtonModel.java:242)
	at javax.swing.AbstractButton.doClick(AbstractButton.java:389)
	at javax.swing.plaf.basic.BasicMenuItemUI.doClick(BasicMenuItemUI.java:809)
	at com.apple.laf.AquaMenuItemUI.doClick(AquaMenuItemUI.java:137)
	at javax.swing.plaf.basic.BasicMenuItemUI$Handler.mouseReleased(BasicMenuItemUI.java:850)
	at java.awt.Component.processMouseEvent(Component.java:6373)
	at javax.swing.JComponent.processMouseEvent(JComponent.java:3267)
	at java.awt.Component.processEvent(Component.java:6138)
	at java.awt.Container.processEvent(Container.java:2085)
	at java.awt.Component.dispatchEventImpl(Component.java:4735)
	at java.awt.Container.dispatchEventImpl(Container.java:2143)
	at java.awt.Component.dispatchEvent(Component.java:4565)
	at java.awt.LightweightDispatcher.retargetMouseEvent(Container.java:4621)
	at java.awt.LightweightDispatcher.processMouseEvent(Container.java:4282)
	at java.awt.LightweightDispatcher.dispatchEvent(Container.java:4212)
	at java.awt.Container.dispatchEventImpl(Container.java:2129)
	at java.awt.Window.dispatchEventImpl(Window.java:2478)
	at java.awt.Component.dispatchEvent(Component.java:4565)
	at java.awt.EventQueue.dispatchEventImpl(EventQueue.java:679)
	at java.awt.EventQueue.access$000(EventQueue.java:85)
	at java.awt.EventQueue$1.run(EventQueue.java:638)
	at java.awt.EventQueue$1.run(EventQueue.java:636)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.security.AccessControlContext$1.doIntersectionPrivilege(AccessControlContext.java:87)
	at java.security.AccessControlContext$1.doIntersectionPrivilege(AccessControlContext.java:98)
	at java.awt.EventQueue$2.run(EventQueue.java:652)
	at java.awt.EventQueue$2.run(EventQueue.java:650)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.security.AccessControlContext$1.doIntersectionPrivilege(AccessControlContext.java:87)
	at java.awt.EventQueue.dispatchEvent(EventQueue.java:649)
	at java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:296)
	at java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:211)
	at java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:201)
	at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:196)
	at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:188)
	at java.awt.EventDispatchThread.run(EventDispatchThread.java:122)
Caused by: java.lang.NullPointerException
	at com.sun.org.apache.xml.internal.serializer.ToHTMLStream.endElement(ToHTMLStream.java:907)
	at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerHandlerImpl.endElement(TransformerHandlerImpl.java:273)
	at org.apache.tika.sax.ContentHandlerDecorator.endElement(ContentHandlerDecorator.java:136)
	at org.apache.tika.gui.TikaGUI$2.endElement(TikaGUI.java:519)
	at org.apache.tika.sax.TeeContentHandler.endElement(TeeContentHandler.java:94)
	at org.apache.tika.sax.ContentHandlerDecorator.endElement(ContentHandlerDecorator.java:136)
	at org.apache.tika.sax.SecureContentHandler.endElement(SecureContentHandler.java:256)
	at org.apache.tika.sax.ContentHandlerDecorator.endElement(ContentHandlerDecorator.java:136)
	at org.apache.tika.sax.ContentHandlerDecorator.endElement(ContentHandlerDecorator.java:136)
	at org.apache.tika.sax.ContentHandlerDecorator.endElement(ContentHandlerDecorator.java:136)
	at org.apache.tika.sax.SafeContentHandler.endElement(SafeContentHandler.java:273)
	at org.apache.tika.sax.XHTMLContentHandler.endDocument(XHTMLContentHandler.java:213)
	at org.apache.tika.parser.microsoft.OfficeParser.parse(OfficeParser.java:159)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:243)
	... 44 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The same file is parsed without any errors when not in GUI mode.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 12 Dec 2011 03:10:13 +0000" id="308" opendate="Mon, 12 Dec 2011 02:56:12 +0000">
		<buginformation>
			<summary>IndexOutOfBoundsException with TikaGUI</summary>
			<description>&lt;p&gt;From issue Tika-410: the issue's attached test document, &quot;test.doc&quot;, causes an IndexOutOfBoundsException when dropped into a latest build of the tika-app GUI.  The reason is that the org.apache.poi.hwpf.usermodel.Picture class's suggestFileExtension() method, used by Tika's WordExtractor class, can return an empty string.  The requestSave method of the ImageSavingParser inner class (of TikaGUI) is not written to handle the case of an image having no extension.  &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/main/java/org/apache/tika/gui/TikaGUI.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 30 Jul 2012 12:33:11 +0000" id="309" opendate="Tue, 13 Dec 2011 09:53:02 +0000">
		<buginformation>
			<summary>Upgrade metadatExtractor version for OpenJDK 7 support</summary>
			<description>&lt;p&gt;The metadataextractor library (2.4.0-beta-1) is quite old and is depending on some Sun classes thus making it unable to run on openJDK 7 which is now the default JDK on Linux distributions.&lt;br/&gt;
Upgrading the library to the new version 2.5.0-RC3 fixes this issue but the API has changed.&lt;br/&gt;
Appending a patch to the MetadataExtactor class (and the tests) to take advantage of this.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/pom.xml</file>
			<file>/TIKA-915</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 19 Dec 2011 11:18:40 +0000" id="310" opendate="Tue, 13 Dec 2011 16:21:19 +0000">
		<buginformation>
			<summary>Improve the detection of Works Spreadsheet 7.0 files</summary>
			<description>&lt;p&gt;This was originally part of ver3 of my patch submitted to &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-806&quot; title=&quot;MS Word Detection magics are a bit overzealous&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-806&quot;&gt;&lt;del&gt;TIKA-806&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Works Spreadsheet files are weird. Versions up to 3.0 used a Quattro Pro magic, version 4.0 used its own magic, while version 7.0 (probably later ones as well) use an OLE2 structure and an MS Office magic. The 7.0 files also contain an entry labelled &quot;Workbook&quot;. In Tika this makes both MimeTypes (due to the quirk recently discussed in &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-806&quot; title=&quot;MS Word Detection magics are a bit overzealous&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-806&quot;&gt;&lt;del&gt;TIKA-806&lt;/del&gt;&lt;/a&gt;) and the POIFSContainerDetector label them as Excel.&lt;/p&gt;

&lt;p&gt;&quot;Conceptually&quot; they should be vnd.ms-works, but &quot;technically&quot; they are vnd.ms-excel. A special media type seems like a good compromise, similar in vein to the compromise we reached with &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-798&quot; title=&quot;Distinguish between EMF and WMF&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-798&quot;&gt;&lt;del&gt;TIKA-798&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I would like to mark them with a new media type: &quot;application/x-tika-msworks-spreadsheet&quot;. It would be a subclass of vnd.ms-excel so that:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;With pure MimeTypes and no name, ms-excel could be returned.&lt;/li&gt;
	&lt;li&gt;With MimeTypes with name and data, the correct type could be returned&lt;/li&gt;
	&lt;li&gt;With POIFSContainerDetector the correct type could be returned&lt;/li&gt;
	&lt;li&gt;They can also be added to the list of types supported by ExcelParser as it seems to be able to get some content from them&lt;/li&gt;
&lt;/ol&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
			<file>/tika-parsers/src/test/java/org/apache/tika/mime/TestMimeTypes.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 19 Dec 2011 11:28:13 +0000" id="311" opendate="Tue, 13 Dec 2011 18:10:35 +0000">
		<buginformation>
			<summary>Webarchive detection.</summary>
			<description>&lt;p&gt;I'd like to be be able to detect .webarchive files. They are a special case of the Apple Binary Property list format. They are generated by the Safari browser and contain all the files that comprise a web page within a single container file.&lt;/p&gt;

&lt;p&gt;Can anyone supply an example file? All the ones I have are confidential and I don't have a mac myself.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 19 Dec 2011 11:40:41 +0000" id="312" opendate="Tue, 13 Dec 2011 20:32:13 +0000">
		<buginformation>
			<summary>Increase the amount of bytes read by TextDetector</summary>
			<description>&lt;p&gt;In &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-688&quot; title=&quot;Enhance content-type detector to recognize almost plain text&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-688&quot;&gt;&lt;del&gt;TIKA-688&lt;/del&gt;&lt;/a&gt; Jukka implemented a plain text detector. It is fired automatically inside MimeTypes. I find a number of files in my collections, which are binary but are still detected as plain text. They wouldn't be if the plain text detector were allowed to look at more than the initial 512 bytes. I think that the TextDetector should look at MimeTypes.getMinLength bytes. It is given a ByteArrayInputStream backed by an Array. It should read all bytes in that array. &lt;/p&gt;

&lt;p&gt;The performance impact should be negligible (no I/O, no allocations, just pure array lookups), while my experiments show that there are cases when 512 bytes is not enough.&lt;/p&gt;

&lt;p&gt;If anyone objects due to performance reasons, I'll create another patch, which will allow the users to decouple the TextDetector from MimeTypes and supply their own, with different settings.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/detect/TextDetector.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 2 Mar 2015 02:06:11 +0000" id="313" opendate="Tue, 20 Dec 2011 15:50:58 +0000">
		<buginformation>
			<summary>Support detecting old MIcrosoft Works Word Processor formats</summary>
			<description>&lt;p&gt;An issue similar to &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-812&quot; title=&quot;Improve the detection of Works Spreadsheet 7.0 files&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-812&quot;&gt;&lt;del&gt;TIKA-812&lt;/del&gt;&lt;/a&gt;. This time it's about old Works Word Processor formats. They use an OLE2 structure, but the top-level entry is called &quot;MatOST&quot;, they are not supported by the OfficeParser. I would like to:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;Add a magic to tika-mimetypes.xml to mark the file as ms-works if &quot;MatOST&quot; is found. (After &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-806&quot; title=&quot;MS Word Detection magics are a bit overzealous&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-806&quot;&gt;&lt;del&gt;TIKA-806&lt;/del&gt;&lt;/a&gt; we officially like those).&lt;/li&gt;
	&lt;li&gt;Add an 'if' to POIFSContainerDetector to look for MatOST.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;I'm not creating a separate media type for this (like I did in &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-812&quot; title=&quot;Improve the detection of Works Spreadsheet 7.0 files&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-812&quot;&gt;&lt;del&gt;TIKA-812&lt;/del&gt;&lt;/a&gt;) because no parser supports it anyway. In &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-812&quot; title=&quot;Improve the detection of Works Spreadsheet 7.0 files&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-812&quot;&gt;&lt;del&gt;TIKA-812&lt;/del&gt;&lt;/a&gt; it was necessary, because ExcelParser can't work with all vnd.ms-works files but can work with 7.0 spreadsheets. In this case there is no gain in a separate mime type.&lt;/p&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 21 Dec 2011 12:01:42 +0000" id="314" opendate="Tue, 20 Dec 2011 23:06:56 +0000">
		<buginformation>
			<summary>Detect StarOffice files</summary>
			<description>&lt;p&gt;I would like both MimeTypes and the POIFSContainerDetector to be able to detect files created with Star Office Draw, Impress, Writer and Calc.&lt;/p&gt;

&lt;p&gt;I started working on this, but stumbled upon a POI issue, which I posted to poi-user. &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://thread.gmane.org/gmane.comp.jakarta.poi.user/17857&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://thread.gmane.org/gmane.comp.jakarta.poi.user/17857&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Nick? Yegor? I know you're on the Tika list as well. Could you take a look? How to get the raw content of CompObj entry?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 3 Jan 2012 05:13:31 +0000" id="315" opendate="Thu, 22 Dec 2011 04:14:25 +0000">
		<buginformation>
			<summary>TikaException / OfficeXmlFileException with .xlsb files</summary>
			<description>&lt;p&gt;The file testEXCEL.xlsb in the tika-parsers test-documents folder causes a POI OfficeXmlFileException when one tries to open it with TikaGUI or TikaCLI, using a latest build.  The reason: Tika has it configured to be opened with the OfficeParser class, rather than the OOXMLParser class; it is an Office 2007 file, and should be opened with the OOXMLParser class.  Neither the ExcelParserTest class nor the OOXMLParserTest class has anything related to .xlsb files.  Once changes are made to these two parsers so that the OOXMLParser is used (I'll submit a patch shortly for these), the OfficeXmlFileException goes away, and a new POI exception (IllegalArgumentException in the ExtractorFactory class) arises in its place, somewhat related to unsolved POI bug 51921; the creator of this bug mentions a .xlsb file among others.  This exception appears to occur because POI doesn't seem to be able to handle .xlsb files whatsoever.  A cursory search of the source for &quot;xlsb&quot; or its mime type yields nothing relevant, and its project has no .xlsb test files that I can see.   &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/OOXMLParser.java</file>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 30 Jun 2012 15:17:11 +0000" id="316" opendate="Fri, 23 Dec 2011 11:02:51 +0000">
		<buginformation>
			<summary>ForkServer fails to report issues if an exception is not properly serializable</summary>
			<description>&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;        &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
            method.invoke(object, args);
            output.write(DONE);
        } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (InvocationTargetException e) {
            output.write(ERROR);
            &lt;span class=&quot;code-comment&quot;&gt;// warning &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; an exception isn't really serializable, &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; can fail.
&lt;/span&gt;            /**
            File tempFile = File.createTempFile(&lt;span class=&quot;code-quote&quot;&gt;&quot;tika&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;.trace&quot;&lt;/span&gt;);
            PrintWriter pw = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; PrintWriter(tempFile);
            e.printStackTrace(pw);
            pw.close();
            e = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Exception(tempFile.getAbsolutePath());
            */
            ForkObjectInputStream.sendObject(e, output);
        }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/fork/ForkServer.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 29 Dec 2011 16:16:24 +0000" id="317" opendate="Tue, 27 Dec 2011 14:46:54 +0000">
		<buginformation>
			<summary>POI Daily beta6 as of 12/27 breaks ExcelParserTest.testExcelParserFormatting()</summary>
			<description>&lt;p&gt;Attn Nick:&lt;/p&gt;

&lt;p&gt;Changes made to POI(v1221126) for POI-52349 causes the ExcelParserTest.testExcelParserFormatting() junit test case to go out to lunch within POI's DataFormatter.FractionFormat().  Specifically within the nested for loops at line 1000.  These nested loops both have extremely high values that cause the test case to hang for an extremely long duration during the new OfficeParser().parse(input, handler, metadata, context); call at line 73.  This appears to happen due to the Custom Date value in the test file not being identified as a date.&lt;/p&gt;

&lt;p&gt;I'm not sure if the proper resolution lies in modifying TIKA's test case or if a new bug needs to be opened on POI's side, but I wanted to bring this issue to light.&lt;/p&gt;

&lt;p&gt;Regards,&lt;/p&gt;

&lt;p&gt;Jeremy  &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/microsoft/ExcelParserTest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 24 Jan 2012 12:57:46 +0000" id="318" opendate="Wed, 11 Jan 2012 03:32:46 +0000">
		<buginformation>
			<summary>TikaException with testPPT.potm in Tika GUI / CLI</summary>
			<description>&lt;p&gt;Attempting to open the testPPT.potm file found in the parsers' test-documents folder in a latest build of Tika will result in a TikaException, itself 'Caused by: org.apache.xmlbeans.XmlException: error: The document is not a presentation@&lt;a href=&quot;http://schemas.openxmlformats.org/presentationml/2006/main:&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://schemas.openxmlformats.org/presentationml/2006/main:&lt;/a&gt; document element namespace mismatch expected &quot;http://schemas.openxmlformats.org/presentationml/2006/main&quot; got &quot;http://schemas.openxmlformats.org/presentationml/2006/3/main&quot;'.  I opened this file in MS Office 2007, and it said that it was a file created with a beta version of Office, and that it would be updated the next time it was saved to a more up-to-date format.  I made the contents look like that of the other Office 2007 presentation documents in the test-documents folder, and added this file and its mime type to the OOXMLParserTest class, and then had no problems with the .potm file. I'll attach a patch shortly.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/microsoft/ooxml/OOXMLParserTest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 30 Jun 2012 15:41:57 +0000" id="319" opendate="Mon, 23 Jan 2012 13:30:18 +0000">
		<buginformation>
			<summary>Identify and parse the Apple iBooks format</summary>
			<description>&lt;p&gt;With the release of iBooks Author 1.0, Apple have created a new eBook format very similar to ePub. Tika could be extended to identify and parse this new format, re-using the existing ePub code wherever possible.&lt;/p&gt;

&lt;p&gt;I have created an initial patch, which I will attach to this issue.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pkg/ZipContainerDetector.java</file>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/epub/EpubParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 13 May 2012 03:49:46 +0000" id="320" opendate="Mon, 30 Jan 2012 04:00:38 +0000">
		<buginformation>
			<summary>java.io.IOException with TikaGUI and testMP4.m4a</summary>
			<description>&lt;p&gt;Using a latest build: when attempting to drop the new testMP4.m4a file into the Tika GUI, a TikaException / IOException occurs:  &lt;br/&gt;
org.apache.tika.exception.TikaException: Failed to close temporary resources&lt;br/&gt;
	at org.apache.tika.io.TemporaryResources.dispose(TemporaryResources.java:152)&lt;br/&gt;
	at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:127)&lt;br/&gt;
	at org.apache.tika.gui.TikaGUI.handleStream(TikaGUI.java:320)&lt;br/&gt;
	at org.apache.tika.gui.TikaGUI.openFile(TikaGUI.java:279)&lt;br/&gt;
	at org.apache.tika.gui.ParsingTransferHandler.importFiles(ParsingTransferHandler.java:94)&lt;br/&gt;
	at org.apache.tika.gui.ParsingTransferHandler.importData(ParsingTransferHandler.java:77)&lt;br/&gt;
	at javax.swing.TransferHandler.importData(Unknown Source)&lt;br/&gt;
	at javax.swing.TransferHandler$DropHandler.drop(Unknown Source)&lt;br/&gt;
....&lt;br/&gt;
Caused by: java.io.IOException: Could not delete temporary file C:\Users\john\AppData\Local\Temp\apache-tika-693752014807275949.tmp&lt;br/&gt;
	at org.apache.tika.io.TemporaryResources$1.close(TemporaryResources.java:70)&lt;br/&gt;
	at org.apache.tika.io.TemporaryResources.close(TemporaryResources.java:121)&lt;br/&gt;
	at org.apache.tika.io.TemporaryResources.dispose(TemporaryResources.java:150)&lt;br/&gt;
	... 40 more&lt;/p&gt;

&lt;p&gt;I know that the parser for this file is new and its external source parser has some potential bugs, but this exception does not occur when using Tika CLI to detect / parse the test file.  &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/mp4/MP4Parser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 31 Jan 2012 14:39:24 +0000" id="321" opendate="Tue, 31 Jan 2012 14:20:34 +0000">
		<buginformation>
			<summary>No text extraction for Word macroenabled template</summary>
			<description>&lt;p&gt;POI cat extract text from this file, but Tika does not. Mimetype detected by Tika, &quot;application/vnd.ms-word.template.macroenabledtemplate&quot; is not correct too (I think that right type is &quot;application/vnd.ms-word.template.macroEnabled.12&quot;&lt;/p&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pkg/ZipContainerDetector.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 30 Jun 2012 17:40:53 +0000" id="322" opendate="Fri, 17 Feb 2012 00:09:02 +0000">
		<buginformation>
			<summary>MailContentHandler should not create AutoDetectParser on each call</summary>
			<description>&lt;p&gt;MailContentHandler is called from RFC822Parser, and it creates AutoDetectParser on each call to parse(...). The process of creating AutoDetectParser involves reading TikaConfig (not cached), which in turn involves parsing XML config files. Apart from the fact that this process is wasteful and heavy, in addition in a highly concurrent setup it leads to multiple threads blocking on SAX parser creation.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/mail/MailContentHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 30 Jun 2012 16:04:30 +0000" id="323" opendate="Fri, 17 Feb 2012 01:17:41 +0000">
		<buginformation>
			<summary>MimeTypes.forName should avoid method-level synchronization</summary>
			<description>&lt;p&gt;MimeTypes.forName can avoid method-level sync, instead it could push down the sync block only if a modification of Map&amp;lt;MediaType,MimeType&amp;gt; types is required. In most cases this should be enough to free the common case (i.e. when a media type is already registered) from synchronization.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/mime/MimeTypes.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 11 Mar 2012 17:32:38 +0000" id="324" opendate="Wed, 7 Mar 2012 19:43:20 +0000">
		<buginformation>
			<summary>Allow to use call parseToString with a additional parameter of MaxStringLength, so it can be changed per call</summary>
			<description>&lt;p&gt;It would be great to be able to call parseToString with an additional parameter of the maxStringLength, instead of having to set it on the Tika instance. This allows to set it per parse call. Sample code:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; parseToString(InputStream stream, Metadata metadata, &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; maxStringLength)
        &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException, TikaException {
    WriteOutContentHandler handler =
        &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; WriteOutContentHandler(maxStringLength);
    &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
        ParseContext context = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ParseContext();
        context.set(Parser.class, parser);
        parser.parse(
                stream, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; BodyContentHandler(handler), metadata, context);
    } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (SAXException e) {
        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!handler.isWriteLimitReached(e)) {
            &lt;span class=&quot;code-comment&quot;&gt;// This should never happen with BodyContentHandler...
&lt;/span&gt;            &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; TikaException(&lt;span class=&quot;code-quote&quot;&gt;&quot;Unexpected SAX processing failure&quot;&lt;/span&gt;, e);
        }
    } &lt;span class=&quot;code-keyword&quot;&gt;finally&lt;/span&gt; {
        stream.close();
    }
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; handler.toString();
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 21 Mar 2012 11:42:26 +0000" id="325" opendate="Sun, 18 Mar 2012 20:58:45 +0000">
		<buginformation>
			<summary>Embedded document not extracted (regression)</summary>
			<description>&lt;p&gt;Testing the 1.1 rc, I believe I found a regression, hence the priority.&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;dbonniot-t520 /tmp/1.0 java -jar ../tika-app-1.0.jar -z ../coffee.xls 
Extracting 'file0.wmf' (application/x-msmetafile)
Extracting 'file1.wmf' (application/x-msmetafile)
Extracting 'file2.wmf' (application/x-msmetafile)
Extracting 'file3.wmf' (application/x-msmetafile)
Extracting 'file4.png' (image/png)
Extracting 'MBD002B040A.wps' (application/vnd.ms-works)
Extracting 'file5.bin' (application/octet-stream)
Extracting 'MBD00262FE3.unknown' (application/x-tika-msoffice)

dbonniot-t520 /tmp/1.0 cd ../1.1
dbonniot-t520 /tmp/1.1 java -jar ../tika-app-1.1.jar -z ../coffee.xls 
Extracting 'file0.emf' (application/x-emf)
Extracting 'file1.emf' (application/x-emf)
Extracting 'file2.emf' (application/x-emf)
Extracting 'file3.emf' (application/x-emf)
Extracting 'file4.png' (image/png)
Extracting 'MBD002B040A.wps' (application/vnd.ms-works)
Extracting 'file5' (application/x-tika-msoffice-embedded)
Extracting 'MBD00262FE3.unknown' (application/x-tika-msoffice)

dbonniot-t520 /tmp/1.1 ls -l ../1.0/file5.bin ../1.1/file5 
-rw-r--r-- 1 dbonniot dbonniot 2519 2012-03-18 21:51 ../1.0/file5.bin
-rw-r--r-- 1 dbonniot dbonniot    0 2012-03-18 21:51 ../1.1/file5
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Notice how 1.0 could extract the data for file5, but 1.1 creates an empty file instead.&lt;/p&gt;

&lt;p&gt;By the way, I do see improvements in 1.1 as well, congrats for that!&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 22 Mar 2012 14:45:14 +0000" id="326" opendate="Thu, 22 Mar 2012 14:41:49 +0000">
		<buginformation>
			<summary>IllegalArgumentException: No part found for relationship</summary>
			<description>&lt;p&gt;Exception on parsing XLSX file:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Exception in thread &quot;main&quot; org.apache.tika.exception.TikaException: Error creating OOXML extractor
	at org.apache.tika.parser.microsoft.ooxml.OOXMLExtractorFactory.parse(OOXMLExtractorFactory.java:123)
	at org.apache.tika.parser.microsoft.ooxml.OOXMLParser.parse(OOXMLParser.java:82)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)
	at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)
	at org.apache.tika.cli.TikaCLI$OutputType.process(TikaCLI.java:130)
	at org.apache.tika.cli.TikaCLI.process(TikaCLI.java:397)
	at org.apache.tika.cli.TikaCLI.main(TikaCLI.java:101)
Caused by: java.lang.IllegalArgumentException: No part found for relationship id=rId1 - container=org.apache.poi.openxml4j.opc.ZipPackage@1c2ecbe - relationshipType=http://schemas.openxmlformats.org/officeDocument/2006/relationships/image - source=/xl/drawings/drawing1.xml - target=/xl/drawings/NULL,targetMode=INTERNAL
	at org.apache.poi.openxml4j.opc.PackagePart.getRelatedPart(PackagePart.java:486)
	at org.apache.tika.parser.microsoft.ooxml.AbstractOOXMLExtractor.handleEmbeddedParts(AbstractOOXMLExtractor.java:119)
	at org.apache.tika.parser.microsoft.ooxml.AbstractOOXMLExtractor.getXHTML(AbstractOOXMLExtractor.java:108)
	at org.apache.tika.parser.microsoft.ooxml.OOXMLExtractorFactory.parse(OOXMLExtractorFactory.java:110)
	... 7 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;File is not 100% correct (I think it was generated by some third party code), but MS Office and Libreoffice opens it fine.&lt;/p&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/AbstractOOXMLExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 23 Mar 2012 12:09:39 +0000" id="327" opendate="Fri, 23 Mar 2012 11:48:44 +0000">
		<buginformation>
			<summary>Extract embedded images in PPT</summary>
			<description>&lt;p&gt;POI can extract embedded images from PPT files, but this feature is not used in Tika (currently we can extract only embedded OLE-files).&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/HSLFExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 27 Mar 2012 17:32:45 +0000" id="328" opendate="Tue, 27 Mar 2012 15:46:55 +0000">
		<buginformation>
			<summary>Dynamic loading of Parser and Detector services</summary>
			<description>&lt;p&gt;When deployed in an OSGi container, calling &lt;tt&gt;new Tika()&lt;/tt&gt; will dynamically load all available &lt;tt&gt;Parser&lt;/tt&gt; and &lt;tt&gt;Detector&lt;/tt&gt; services. This works fine for the typical use case where the facade instance is only used for a single specific task (parsing a single document, etc.).&lt;/p&gt;

&lt;p&gt;However, if a client instead uses a singe, long-lived Tika instance, the list of referenced services never gets updated even if the set of services in the container changes over time. To address this problem we should make Tika reload the set of available services each time they're needed. The performance overhead of doing this should be minimal compared to the typical parsing or type detection tasks.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/config/ServiceLoader.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 28 Mar 2012 15:27:12 +0000" id="329" opendate="Wed, 28 Mar 2012 15:23:14 +0000">
		<buginformation>
			<summary>OOXMLExtractorFactory can leave files open</summary>
			<description>&lt;p&gt;As identified in an Alfresco bug (ALF-13106), OOXMLExtractorFactory doesn't currently allow the closing of OPCPackage instances created from Files. This is because the OPCPackage isn't associated with the TikaInputStream, so the close doesn't propogate&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/OOXMLExtractorFactory.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 5 Apr 2012 13:40:07 +0000" id="330" opendate="Thu, 5 Apr 2012 13:15:16 +0000">
		<buginformation>
			<summary>Improve detection of Android Packages (APK)</summary>
			<description>&lt;p&gt;APK files are not correctly detected, updates are needed for the mimetypes file and the zip detector&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pkg/ZipContainerDetector.java</file>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
			<file>/tika-parsers/src/test/resources/test-documents/testAPK.apk</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 19 Apr 2012 12:43:50 +0000" id="331" opendate="Thu, 19 Apr 2012 10:12:40 +0000">
		<buginformation>
			<summary>OSGi deployment without declarative services</summary>
			<description>&lt;p&gt;The current OSGi bundles require declarative services for registering the parser and detector services. It would be nice if that dependency wasn't needed.&lt;/p&gt;

&lt;p&gt;Also, the service listener in tika-core won't pick up parser or detector services that have been registered already before the tika-core activator is run.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-bundle/pom.xml</file>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 2 Mar 2015 04:19:53 +0000" id="332" opendate="Fri, 20 Apr 2012 12:04:19 +0000">
		<buginformation>
			<summary>UTF-8 encoded XML is detected as text/plain because of UTF-8 BOM</summary>
			<description>&lt;p&gt;Detection of XML fails when encoded as UTF-8. The UTF-8 BOM: 0xEF,0xBB,0xBF causes the XML detector to fail when trying to match &quot;&amp;lt;?xml&quot; at the beginning of the input stream.&lt;/p&gt;

			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 30 Jun 2012 13:29:25 +0000" id="333" opendate="Mon, 23 Apr 2012 14:03:25 +0000">
		<buginformation>
			<summary>Tika fails to detect ISO9660 disk images</summary>
			<description>&lt;p&gt;I have been testing Tika's ability to identify ISO9660 disk image file systems, and discovered two problems. Firstly, the offset match matcher was wrong (37633 instead of 32769). Secondly, and more seriously, it was impossible for that signaure to ever match, because the default buffer size was far too small. It is currently set to 8KB, and as this signature is some 36KB into the file, Tika could never find the match. The attached patch fixes the magic, and extends the buffer to 64KB.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/mime/MimeTypes.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 30 Jun 2012 13:23:16 +0000" id="334" opendate="Mon, 30 Apr 2012 12:10:47 +0000">
		<buginformation>
			<summary>Adding XMP specification part one namespaces and properties</summary>
			<description>&lt;p&gt;The attached patch adds the namespaces and properties form XMP specification part one as interfaces to metadata package.&lt;br/&gt;
The interfaces are not added to metadata class. The idea is that class will contain aliases to those core properties later on.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/metadata/Property.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/metadata/XMP.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 10 May 2012 11:52:54 +0000" id="335" opendate="Mon, 7 May 2012 13:23:22 +0000">
		<buginformation>
			<summary>MagicMime detection of msdos executables does not work</summary>
			<description>&lt;p&gt;Mime detection does not work as expected (at least from me) in contrast e.g. to sourceforge mime-util detection or &quot;file&quot; utility.&lt;br/&gt;
For example using putty ms-dos executable does result in wrong detections:&lt;/p&gt;

&lt;p&gt;krah@sf050:~$ java -jar /tmp/tika-app-1.1.jar --detect /tmp/putty&lt;br/&gt;
application/octet-stream&lt;br/&gt;
krah@sf050:~$ java -jar /tmp/tika-app-1.1.jar --detect /tmp/putty.jpg&lt;br/&gt;
image/jpeg&lt;br/&gt;
krah@sf050:~$ java -jar /tmp/tika-app-1.1.jar --detect /tmp/putty.exe&lt;br/&gt;
application/x-msdownload&lt;/p&gt;


&lt;p&gt;Its everytime the same binary resource only with different names.&lt;br/&gt;
In contrast using &quot;file&quot; does output:&lt;/p&gt;

&lt;p&gt;krah@sf050:~$ file /tmp/putty&lt;br/&gt;
/tmp/putty: PE32 executable for MS Windows (GUI) Intel 80386 32-bit&lt;br/&gt;
krah@sf050:~$ file /tmp/putty.jpg&lt;br/&gt;
/tmp/putty.jpg: PE32 executable for MS Windows (GUI) Intel 80386 32-bit&lt;br/&gt;
krah@sf050:~$ file /tmp/putty.exe&lt;br/&gt;
/tmp/putty.exe: PE32 executable for MS Windows (GUI) Intel 80386 32-bit&lt;/p&gt;

&lt;p&gt;So magic mime detection should be able to detect that this is actually an executable.&lt;/p&gt;

&lt;p&gt;E.g. for a PDF it does work:&lt;/p&gt;

&lt;p&gt;krah@sf050:~$ java -jar /tmp/tika-app-1.1.jar --detect /tmp/print.pdf&lt;br/&gt;
application/pdf&lt;br/&gt;
krah@sf050:~$ java -jar /tmp/tika-app-1.1.jar --detect /tmp/print&lt;br/&gt;
application/pdf&lt;br/&gt;
krah@sf050:~$ java -jar /tmp/tika-app-1.1.jar --detect /tmp/print.jpg &lt;br/&gt;
application/pdf&lt;/p&gt;

&lt;p&gt;Here Tika detects what is expected.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 16 May 2012 22:05:50 +0000" id="336" opendate="Wed, 9 May 2012 14:23:04 +0000">
		<buginformation>
			<summary>NullPointerException processing XPS file</summary>
			<description>&lt;p&gt;Steps to  reproduce:&lt;/p&gt;

&lt;p&gt;a) Using an application that natively outputs XPS documents, i.e. Microsoft Word 2007 when using Save As XPS. (doc_xps.xps)&lt;br/&gt;
b) Using the &quot;Print-to-XPS&quot; driver Microsoft provides, called the Microsoft XPS Document Writer. (xps_print.xps)&lt;/p&gt;

&lt;p&gt;Both files gives error and content is not indexed.&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;&amp;gt;  java -jar /opt/ems/lib/tika-app-1.1.jar --text /tmp/doc_xps.xps
Exception in thread &quot;main&quot; org.apache.tika.exception.TikaException: Unexpected RuntimeException from org.apache.tika.parser.microsoft.ooxml.OOXMLParser@3f6dadf9
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:244)
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)
        at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)
        at org.apache.tika.cli.TikaCLI$OutputType.process(TikaCLI.java:130)
        at org.apache.tika.cli.TikaCLI.process(TikaCLI.java:397)
        at org.apache.tika.cli.TikaCLI.main(TikaCLI.java:101)
Caused by: java.lang.NullPointerException
        at org.apache.tika.parser.microsoft.ooxml.OOXMLExtractorFactory.parse(OOXMLExtractorFactory.java:82)
        at org.apache.tika.parser.microsoft.ooxml.OOXMLParser.parse(OOXMLParser.java:82)
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)
        ... 5 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/OOXMLExtractorFactory.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 16 May 2012 22:05:50 +0000" id="337" opendate="Sun, 13 May 2012 13:45:40 +0000">
		<buginformation>
			<summary>Parser for executables (metadata)</summary>
			<description>&lt;p&gt;Based on the investigations for &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-913&quot; title=&quot;MagicMime detection of msdos executables does not work&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-913&quot;&gt;&lt;del&gt;TIKA-913&lt;/del&gt;&lt;/a&gt;, it should be fairly easy to implement a parser to extract metadata from executables (PE and ELF). This could give us a similar level of information to that returned by file, eg architecture, platform, endian-ness etc&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/resources/test-documents/testFreeBSD-x86-64</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/executable/ExecutableParser.java</file>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
			<file>/tika-parsers/src/test/resources/test-documents/testC.c</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 30 Oct 2012 18:28:15 +0000" id="338" opendate="Wed, 16 May 2012 16:04:05 +0000">
		<buginformation>
			<summary>Remove DublinCore From Metadata and Deprecate String Properties</summary>
			<description>&lt;p&gt;In an effort to separate Tika's core metadata definitions from the particular standards used in the implementation the Metadata class should not implement DublinCore, those properties should be deprecated in the Metadata class, and the definitions in DublinCore should be made Property objects rather than Strings.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 16 May 2012 20:42:58 +0000" id="339" opendate="Wed, 16 May 2012 20:38:41 +0000">
		<buginformation>
			<summary>Data Typed Metadata.set(...) Value Methods Should Call Metadata.set(Property...)</summary>
			<description>&lt;p&gt;The property value setter methods for int, double, and Date types should call the underlying set method with a Property key rather than String.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/metadata/Metadata.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 16 May 2012 21:43:14 +0000" id="340" opendate="Wed, 16 May 2012 21:23:56 +0000">
		<buginformation>
			<summary>Composite Properties</summary>
			<description>&lt;p&gt;A composite property type would be useful for maintaining backwards compatibility when migrating old properties or aliasing a single primary property to several secondary fields.&lt;/p&gt;

&lt;p&gt;To achieve this the Property class could allow for defining a composite type which aliases a primary Property and an optional list of secondaryExtractProperties which can be used to set additional metadata properties with the same value as the primary.&lt;/p&gt;

&lt;p&gt;The name of the composite property would be taken from its primary property.&lt;/p&gt;

&lt;p&gt;The primary and secondary children of a composite property must not be composite properties themselves or a PropertyTypeException will be thrown.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/metadata/Metadata.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 16 May 2012 21:43:14 +0000" id="341" opendate="Wed, 16 May 2012 22:06:24 +0000">
		<buginformation>
			<summary>Separation of Tika Core Properties From Metadata Processing</summary>
			<description>&lt;p&gt;The Metadata class is a bit overloaded with both processing and core Tika properties defined in the same place.&lt;/p&gt;

&lt;p&gt;Separating the core properties into a TikaCoreProperties class which contains only composite properties which reference other standards like DublinCore will allow the Metadata class to focus on processing and ease the transition from the now deprecated String properties that were directly included in Metadata via the implements clause.&lt;/p&gt;

&lt;p&gt;This will also allow us to cherry pick only the properties we want from a standard as Tika core properties rather than having to include all the properties in a standard's interface, some of which may be more specific to a particular content type than we want.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/asm/XHTMLClassVisitor.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/dwg/DWGParser.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/metadata/Metadata.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/metadata/TikaCoreProperties.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 30 Jun 2012 11:40:05 +0000" id="342" opendate="Thu, 17 May 2012 16:17:15 +0000">
		<buginformation>
			<summary>Consistent, namespaced definitions for office file related metadata</summary>
			<description>&lt;p&gt;Currently, we have the MSOffice metadata definitions, which is a mixture of Properties and Strings, none of them namespaced. Despite the name, the keys apply to a wide range of Office Documents (not just MS ones), and the keys are taken from a mixture of sources.&lt;/p&gt;

&lt;p&gt;Similar to &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-925&quot; title=&quot;Remove DublinCore From Metadata and Deprecate String Properties&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-925&quot;&gt;&lt;del&gt;TIKA-925&lt;/del&gt;&lt;/a&gt; / &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-928&quot; title=&quot;Separation of Tika Core Properties From Metadata Processing&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-928&quot;&gt;TIKA-928&lt;/a&gt;, we should replace these with prefixed versions drawn from a few well known externally defined namespaces, then deprecate the old ones.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/SummaryExtractor.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/metadata/Office.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/metadata/MSOffice.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 16 May 2012 16:34:05 +0000" id="343" opendate="Sat, 5 May 2012 10:40:58 +0000">
		<buginformation>
			<summary>Tika's PDFParser fails to parse documents embedded in a PDF Package</summary>
			<description>&lt;p&gt;In working on &lt;a href=&quot;https://issues.apache.org/jira/browse/PDFBOX-1297&quot; title=&quot;ExtractText fails to extract text from packaged PDFs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PDFBOX-1297&quot;&gt;&lt;del&gt;PDFBOX-1297&lt;/del&gt;&lt;/a&gt;, I realized Tika's PDFParser also doesn't&lt;br/&gt;
visit documents embedded with a PDF document (ie a PDF package).&lt;/p&gt;

&lt;p&gt;Tika can actually handle this better than ExtractText since it can&lt;br/&gt;
recurse on any embedded document type (not just PDFs) and parse them&lt;br/&gt;
as well, vs ExtractText which only extracts when the embedded&lt;br/&gt;
documents are also PDF.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;Update: Moving this issue to Tika.&amp;#93;&lt;/span&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDFParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 29 Jun 2012 21:21:16 +0000" id="344" opendate="Wed, 23 May 2012 14:57:02 +0000">
		<buginformation>
			<summary>Upgrade to Commons Compress 1.4.1</summary>
			<description>&lt;p&gt;There's a denial of service vulnerability (CVE-2012-2098) in Commons Compress versions up to 1.4 (we currently use 1.3) that can be triggered with a specially crafted bzip2 document.&lt;/p&gt;

&lt;p&gt;Tika already has higher-level features (ForkParser, etc.) for dealing with problems like this, but it would in any case be good to upgrade our Commons Compress dependency to the new 1.4.1 release that fixes the vulnerability.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pkg/CompressorParser.java</file>
			<file>/tika-app/src/main/appended-resources/META-INF/LICENSE</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 30 Jun 2012 12:15:11 +0000" id="345" opendate="Thu, 24 May 2012 21:25:57 +0000">
		<buginformation>
			<summary>Tika in server mode stops responding and reports NPE over and over in logs</summary>
			<description>&lt;p&gt;We run tika in server mode via:&lt;/p&gt;

&lt;p&gt;/usr/java/jdk/bin/java -Dlog4j.app.name=-server -Djavax.xml.soap.MessageFactory=com.sun.xml.messaging.saaj.soap.ver1_1.SOAPMessageFactory1_1Impl -Dfile.encoding=UTF-8 -Djava.net.preferIPv4Stack=true -server -Xms256M -Xmx768M -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/var/log/oom/content-extractor-8983.dump.1 -server -Xms500M -Xmx500M -jar /opt/tika/tika-app-1.1.jar --text --encoding=UTF-8 --server 8983&lt;/p&gt;

&lt;p&gt;Our client talks to this over port 8983. We pass data via the socket and get the responses back. However, sometimes, tika will get into a bad state and stop responding. &lt;br/&gt;
When this happens, we see this in the logs over and over. &lt;/p&gt;

&lt;p&gt;2012-05-24_20:12:33.88573 Caused by: java.lang.NullPointerException&lt;br/&gt;
2012-05-24_20:12:33.88576       at org.apache.tika.sax.XHTMLContentHandler.lazyEndHead(XHTMLContentHandler.java:157)&lt;br/&gt;
2012-05-24_20:12:33.88580       at org.apache.tika.sax.XHTMLContentHandler.startElement(XHTMLContentHandler.java:237)&lt;br/&gt;
2012-05-24_20:12:33.88584       at org.apache.tika.sax.XHTMLContentHandler.startElement(XHTMLContentHandler.java:274)&lt;br/&gt;
2012-05-24_20:12:33.88589       at org.apache.tika.parser.microsoft.WordExtractor.handleParagraph(WordExtractor.java:186)&lt;br/&gt;
2012-05-24_20:12:33.88593       at org.apache.tika.parser.microsoft.WordExtractor.parse(WordExtractor.java:97)&lt;br/&gt;
2012-05-24_20:12:33.88597       at org.apache.tika.parser.microsoft.OfficeParser.parse(OfficeParser.java:185)&lt;br/&gt;
2012-05-24_20:12:33.88602       at org.apache.tika.parser.microsoft.OfficeParser.parse(OfficeParser.java:160)&lt;br/&gt;
2012-05-24_20:12:33.88606       at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)&lt;br/&gt;
2012-05-24_20:12:33.88611       ... 4 more&lt;br/&gt;
2012-05-24_20:12:49.28441 org.apache.tika.exception.TikaException: Unexpected RuntimeException from org.apache.tika.parser.microsoft.OfficeParse&lt;br/&gt;
r@6906daba&lt;br/&gt;
2012-05-24_20:12:49.28458       at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:244)&lt;br/&gt;
2012-05-24_20:12:49.28466       at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)&lt;br/&gt;
2012-05-24_20:12:49.28477       at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)&lt;br/&gt;
2012-05-24_20:12:49.28489       at org.apache.tika.cli.TikaCLI$OutputType.process(TikaCLI.java:130)&lt;br/&gt;
2012-05-24_20:12:49.28497       at org.apache.tika.cli.TikaCLI$TikaServer$1.run(TikaCLI.java:735)&lt;br/&gt;
2012-05-24_20:12:49.28509 Caused by: java.lang.NullPointerException&lt;br/&gt;
2012-05-24_20:12:49.28516       at org.apache.tika.sax.XHTMLContentHandler.lazyEndHead(XHTMLContentHandler.java:157)&lt;br/&gt;
2012-05-24_20:12:49.28524       at org.apache.tika.sax.XHTMLContentHandler.startElement(XHTMLContentHandler.java:237)&lt;br/&gt;
2012-05-24_20:12:49.28532       at org.apache.tika.sax.XHTMLContentHandler.startElement(XHTMLContentHandler.java:274)&lt;br/&gt;
2012-05-24_20:12:49.28541       at org.apache.tika.parser.microsoft.WordExtractor.handleParagraph(WordExtractor.java:186)&lt;br/&gt;
2012-05-24_20:12:49.28550       at org.apache.tika.parser.microsoft.WordExtractor.parse(WordExtractor.java:97)&lt;br/&gt;
2012-05-24_20:12:49.28558       at org.apache.tika.parser.microsoft.OfficeParser.parse(OfficeParser.java:185)&lt;br/&gt;
2012-05-24_20:12:49.28565       at org.apache.tika.parser.microsoft.OfficeParser.parse(OfficeParser.java:160)&lt;br/&gt;
2012-05-24_20:12:49.28577       at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)&lt;br/&gt;
2012-05-24_20:12:49.28585       ... 4 more&lt;/p&gt;

&lt;p&gt;We have tried to figure out what causes this with no success. We only know that once the server gets into this state, there is no recourse but to restart the tika service.&lt;/p&gt;

&lt;p&gt;Other instances of tika we have running in the test environment continue to work. There is some combination of content or work that causes&lt;br/&gt;
tika to destabilize. Our working theory is that perhaps tika server is not thread safe and that may be causing this behavior.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 6 Feb 2015 19:43:33 +0000" id="346" opendate="Tue, 29 May 2012 08:36:08 +0000">
		<buginformation>
			<summary>encoding of ZipArchiveInputStream</summary>
			<description>&lt;p&gt;When extracting from the zip files which are zipped at Windows OS(Japanese), the file name extracted from zip is garbled.&lt;/p&gt;

&lt;p&gt;ZipArchiveInputStream has three constructors. Modifying like the below, the file name was not garbled. I specified the encoding - SJIS.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-style: solid;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeHeader panelHeader&quot; style=&quot;border-bottom-width: 1px;border-bottom-style: solid;&quot;&gt;&lt;b&gt;PackageExtractor&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void parse(InputStream stream)
 :
 &lt;span class=&quot;code-comment&quot;&gt;//unpack(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ZipArchiveInputStream(stream), xhtml);  
&lt;/span&gt; unpack(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ZipArchiveInputStream(stream,&lt;span class=&quot;code-quote&quot;&gt;&quot;SJIS&quot;&lt;/span&gt;,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), xhtml); 
 :
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In first constructor &lt;del&gt;the platform's default encoding&lt;/del&gt; UTF-8 is used.  In my case the encoding of my computer is UTF-8, the encoding of zip file is SJIS, so the file name was garbled. We will get garbled file name if there is a difference of  encoding between &lt;del&gt;platform&lt;/del&gt; this constructor and zip file.&lt;/p&gt;

&lt;p&gt;I want Tika to parse zip by giving some kind of encoding parameter per file, Where should I give the encoding, somewhere in Metadata or ParseContext? Please support this. I am using Tika via Solr(SolrCell), so when posting zip file to Solr I want to add encoding parameter to the request.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pkg/PackageParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 13 Jun 2012 15:36:59 +0000" id="347" opendate="Wed, 13 Jun 2012 15:04:15 +0000">
		<buginformation>
			<summary>Windows Media Video file detected as Windows Media Audio</summary>
			<description>&lt;p&gt;Attached file is detected as &quot;audio/x-ms-wma&quot; instead of &quot;video/x-ms-wmv&quot;.&lt;/p&gt;

&lt;p&gt;Expected result:&lt;br/&gt;
$ java -jar tika-app-1.1.jar -d test.wmv &lt;br/&gt;
video/x-ms-wmv&lt;/p&gt;

&lt;p&gt;Actual result:&lt;br/&gt;
$ java -jar tika-app-1.1.jar -d test.wmv &lt;br/&gt;
audio/x-ms-wma&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 21 Jun 2012 17:19:47 +0000" id="348" opendate="Thu, 21 Jun 2012 08:17:09 +0000">
		<buginformation>
			<summary>Support detecting 7-zip format</summary>
			<description>&lt;p&gt;Tika now detects 7-zip archive as application/octect-stream.&lt;br/&gt;
Adding this fragment in tika-mimetypes.xml registry Tika mime magic detector will correctly recognize 7z files:&lt;/p&gt;

&lt;p&gt;&amp;lt;mime-type type=&quot;application/x-7z-compressed&quot;&amp;gt;&lt;br/&gt;
        &amp;lt;acronym&amp;gt;7zip&amp;lt;/acronym&amp;gt;&lt;br/&gt;
        &amp;lt;_comment&amp;gt;7-zip archive&amp;lt;/_comment&amp;gt;&lt;br/&gt;
        &amp;lt;magic priority=&quot;50&quot;&amp;gt;&lt;br/&gt;
            &amp;lt;!-- Magic: '7', 'z', 0xBC, 0xAF, 0x27, 0x1C --&amp;gt;&lt;br/&gt;
            &amp;lt;match value=&quot;7z&quot; type=&quot;string&quot; offset=&quot;0:1&quot; &amp;gt;&lt;br/&gt;
                &amp;lt;match value=&quot;0xBCAF271C&quot; type=&quot;string&quot; offset=&quot;2:5&quot; /&amp;gt;&lt;br/&gt;
            &amp;lt;/match&amp;gt;           &lt;br/&gt;
        &amp;lt;/magic&amp;gt;&lt;br/&gt;
        &amp;lt;glob pattern=&quot;*.7z&quot; /&amp;gt;&lt;br/&gt;
    &amp;lt;/mime-type&amp;gt;&lt;/p&gt;

			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/resources/test-documents/test-documents.7z</file>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 8 May 2014 16:12:05 +0000" id="349" opendate="Thu, 21 Jun 2012 08:43:07 +0000">
		<buginformation>
			<summary>Detecting KML / KMZ files</summary>
			<description>&lt;p&gt;KML format is subtype of application/xml with a &quot;kml&quot; root node and (an optional?) &quot;http://www.opengis.net/kml/2.2&quot; namespace.&lt;/p&gt;

&lt;p&gt;  &amp;lt;mime-type type=&quot;application/vnd.google-earth.kml+xml&quot;&amp;gt;    &lt;br/&gt;
    &amp;lt;root-XML localName=&quot;kml&quot;/&amp;gt;&lt;br/&gt;
    &amp;lt;root-XML namespaceURI=&quot;http://www.opengis.net/kml/2.2&quot; localName=&quot;kml&quot;/&amp;gt;    &lt;br/&gt;
    &amp;lt;acronym&amp;gt;KML&amp;lt;/acronym&amp;gt;&lt;br/&gt;
    &amp;lt;_comment&amp;gt;Keyhole Markup Language&amp;lt;/_comment&amp;gt;&lt;br/&gt;
    &amp;lt;glob pattern=&quot;*.kml&quot;/&amp;gt;    &lt;br/&gt;
    &amp;lt;sub-class-of type=&quot;application/xml&quot;/&amp;gt;&lt;br/&gt;
  &amp;lt;/mime-type&amp;gt;	&lt;/p&gt;

&lt;p&gt;KMZ files (&lt;a href=&quot;https://developers.google.com/kml/documentation/kmzarchives&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://developers.google.com/kml/documentation/kmzarchives&lt;/a&gt;) are zip archives with a KML file inside (the file should be called doc.kml) and one or more folder. A naive approach consists in adding a further check in ZipContainerDetector (find attached). &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/resources/test-documents/testKML.kml</file>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
			<file>/CHANGES.txt</file>
			<file>/tika-parsers/src/test/java/org/apache/tika/detect/TestContainerAwareDetector.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 30 Jun 2012 11:53:40 +0000" id="350" opendate="Tue, 26 Jun 2012 22:50:12 +0000">
		<buginformation>
			<summary>Add parameter to tika-app to supply password for decryption</summary>
			<description>&lt;p&gt;Now that &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-850&quot; title=&quot;Consistent way to supply document passwords to parsers&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-850&quot;&gt;&lt;del&gt;TIKA-850&lt;/del&gt;&lt;/a&gt; is in, we need a way to pass a password into tika-app.jar so that we can parse encrypted docs from commandline or GUI.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 9 Aug 2012 17:42:15 +0000" id="351" opendate="Tue, 3 Jul 2012 23:31:58 +0000">
		<buginformation>
			<summary>Embedded PDF extracted incorrectly as MS Works file from Word 97-2003 doc</summary>
			<description>&lt;p&gt;This is just like &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-704&quot; title=&quot;PDF and Outlook docs embedded in MS Word documents not parsed&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-704&quot;&gt;&lt;del&gt;TIKA-704&lt;/del&gt;&lt;/a&gt;, except that issue was for an OOXML Word&lt;br/&gt;
doc but this is for the older Word 97-2003 format.&lt;/p&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/test/java/org/apache/tika/cli/TikaCLITest.java</file>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/AbstractPOIFSExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 4 Jul 2012 15:08:18 +0000" id="352" opendate="Wed, 4 Jul 2012 15:03:14 +0000">
		<buginformation>
			<summary>Mimetype magic needed for mapping formats such as XMind Pro and MindMapper</summary>
			<description>&lt;p&gt;As originally reported against Alfresco in ALF-14716, Tika is missing mimetype entries for XMind Pro and MindMapper, which both use zip-based file formats. Entries for these should ideally be added to Tika&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 6 Jul 2012 21:28:09 +0000" id="353" opendate="Fri, 6 Jul 2012 21:15:49 +0000">
		<buginformation>
			<summary>Bundle activation policy for Eclipse</summary>
			<description>&lt;p&gt;As suggested on users@ by Kevin Milburn, it would be useful if the Tika bundles set the &lt;tt&gt;Bundle-ActivationPolicy&lt;/tt&gt; manifest entry to &lt;tt&gt;lazy&lt;/tt&gt; for smoother integration with the Eclipse platform.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-bundle/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 16 Jan 2012 12:28:14 +0000" id="354" opendate="Mon, 22 Oct 2007 21:38:52 +0000">
		<buginformation>
			<summary>MimeTypes should allow modification of MIME types</summary>
			<description>&lt;p&gt;The current MimeTypes class doesn't allow already added MIME types to be modified as the magic and pattern maps are only updated when a type is added. This makes it hard to collect information like extra magic rules or glob patterns from multiple sources.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/src/main/java/org/apache/tika/mime/Magic.java</file>
			<file>/src/main/java/org/apache/tika/mime/MimeTypes.java</file>
			<file>/src/main/java/org/apache/tika/mime/MediaType.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/mime/Magic.java</file>
			<file>/src/main/java/org/apache/tika/mime/MimeType.java</file>
			<file>/TIKA-89</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 12 Jan 2012 15:00:51 +0000" id="355" opendate="Tue, 23 Aug 2011 07:27:36 +0000">
		<buginformation>
			<summary>Custom properties on xlsx, docx, pptx</summary>
			<description>&lt;p&gt;Parser on office Xfiles do not get custom properties.&lt;/p&gt;

&lt;p&gt;In class MetadataExtractor, method extract, only core and extended properties are retrieve.&lt;br/&gt;
I added something like this:&lt;/p&gt;

&lt;p&gt;extractMetadata(extractor.getCustomProperties(), metadata);&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;	/**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Add this method to read custom properties on document.&lt;/li&gt;
	&lt;li&gt;&lt;/li&gt;
	&lt;li&gt;@param properties All custom properties.&lt;/li&gt;
	&lt;li&gt;@param metadata Metadata to complete with read properties.&lt;br/&gt;
	 */&lt;br/&gt;
	private void extractMetadata(CustomProperties properties, Metadata metadata) {&lt;br/&gt;
		org.openxmlformats.schemas.officeDocument.x2006.customProperties.CTProperties propsHolder = properties.getUnderlyingProperties();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;		String value = null;&lt;br/&gt;
		DateUtils dateUtils = DateUtils.getInstance();&lt;br/&gt;
		BigDecimal bigDecimal;&lt;/p&gt;

&lt;p&gt;		for (CTProperty property : propsHolder.getPropertyList()) {&lt;br/&gt;
			/* Parse each property */&lt;br/&gt;
			if (property.isSetLpwstr()) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {				value = property.getLpwstr();			}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt; else if (property.isSetFiletime()) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {				value = dateUtils.convertDate(property.getFiletime(), null);			}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt; else if (property.isSetDate()) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {				value = dateUtils.convertDate(property.getDate(), null);			}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt; else if (property.isSetDecimal()) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {				bigDecimal = property.getDecimal();				value = null == bigDecimal ? null }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt; else if (property.isSetBool()) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {				value = BooleanUtils.toStringTrueFalse(property.getBool());			}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt; else if (property.isSetInt()) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {				value = Integer.toString(property.getInt());			}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt; else if (property.isSetLpstr()) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {				value = property.getLpstr();			}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt; else if (property.isSetI4()) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {				/* Number in Excel for example.... Why i4 ? Ask microsoft. */				value = Integer.toString(property.getI4());			}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt; else &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {				/* For other type, do nothing. */				continue;			}&lt;/span&gt; &lt;/div&gt;

&lt;p&gt;			/* Add the custom prefix, as done in old office format. */&lt;br/&gt;
			addProperty(metadata, &quot;custom:&quot; + property.getName(), value);&lt;br/&gt;
		}&lt;br/&gt;
	}&lt;/p&gt;&lt;/blockquote&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/microsoft/ExcelParserTest.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/MetadataExtractor.java</file>
			<file>/tika-parsers/src/test/resources/test-documents/testEXCEL_custom_props.xls</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 27 Nov 2011 22:59:21 +0000" id="356" opendate="Tue, 23 Aug 2011 18:16:04 +0000">
		<buginformation>
			<summary>Tika reports the content type of AR archives as &quot;text/plain&quot;</summary>
			<description>&lt;p&gt;The Tika.detect(InputStream) method returns &quot;text/plain&quot; for AR archives created with the Linux &quot;Create Archive&quot; option of Nautilus (available via right-clicking on a file).&lt;/p&gt;

&lt;p&gt;The Apache Commons Compress &quot;autodetection&quot; code of the ArchiveStreamFactory looks at the first 12 bytes of the stream and correctly identifies the type as AR.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/resources/test-documents/testARofSND.ar</file>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
			<file>/tika-parsers/src/test/java/org/apache/tika/mime/TestMimeTypes.java</file>
			<file>/tika-parsers/src/test/resources/test-documents/test-documents.cpio</file>
			<file>/tika-core/src/main/java/org/apache/tika/detect/MagicDetector.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 6 Nov 2011 11:15:16 +0000" id="357" opendate="Tue, 13 Sep 2011 16:43:55 +0000">
		<buginformation>
			<summary>Word art isn't extracted for various doc types</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/microsoft/ooxml/OOXMLParserTest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 3 Oct 2011 18:20:22 +0000" id="358" opendate="Wed, 28 Sep 2011 02:11:10 +0000">
		<buginformation>
			<summary>[PATCH] RTF TextExtractor processGroupEnd() NoSuchElementException</summary>
			<description>&lt;p&gt;Parsing some RTF documents attempt to perform a removeLast() on the groupStates() list when the list is empty.  Added a check to not perform the logic when the list is empty, thus causing the restore group state to not be performed. Text extraction now completes without further down-stream errors.&lt;/p&gt;

&lt;p&gt;Unable to include sample file due to sensitive nature of file contents.&lt;/p&gt;

&lt;p&gt;StackTrace (TIKA-0.9)&lt;/p&gt;

&lt;p&gt;Caused by: java.util.NoSuchElementException&lt;br/&gt;
	at java.util.LinkedList.remove(LinkedList.java:788)&lt;br/&gt;
	at java.util.LinkedList.removeLast(LinkedList.java:144)&lt;br/&gt;
	at org.apache.tika.parser.rtf.TextExtractor.processGroupEnd(TextExtractor.java:1010)&lt;br/&gt;
	at org.apache.tika.parser.rtf.TextExtractor.extract(TextExtractor.java:352)&lt;br/&gt;
	at org.apache.tika.parser.rtf.RTFParser.parse(RTFParser.java:53)&lt;br/&gt;
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)&lt;br/&gt;
	... 45 more&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 4 Nov 2011 16:28:51 +0000" id="359" opendate="Tue, 1 Nov 2011 22:55:36 +0000">
		<buginformation>
			<summary>Enable controlling of PDFBOX's setSuppressDuplicateOverlappingText from PDFParser</summary>
			<description>&lt;p&gt;Given that there are some problems with how overlapping text is&lt;br/&gt;
removed (slow performance: &lt;a href=&quot;https://issues.apache.org/jira/browse/PDFBOX-956&quot; title=&quot;Poor text extraction performance in PDFTextStripper.java&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PDFBOX-956&quot;&gt;&lt;del&gt;PDFBOX-956&lt;/del&gt;&lt;/a&gt;; some chars incorrectly skipped:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/PDFBOX-1155&quot; title=&quot;setSuppressDuplicateOverlappingText sometimes removes characters that it shouldn&amp;#39;t&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PDFBOX-1155&quot;&gt;&lt;del&gt;PDFBOX-1155&lt;/del&gt;&lt;/a&gt;), I think we should make this controllable from Tika's&lt;br/&gt;
PDFParser and I think it's best to default it to off for now.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 18 Jan 2013 22:29:13 +0000" id="360" opendate="Tue, 8 Nov 2011 02:31:51 +0000">
		<buginformation>
			<summary>Embed Capabilities</summary>
			<description>&lt;p&gt;This patch defines and implements the concept of embedding tika metadata into a file stream, the reverse of extraction.&lt;/p&gt;

&lt;p&gt;In the tika-core project an interface defining an Embedder and a generic sed ExternalEmbedder implementation meant to be extended or configured are added.  These classes are essentially a reverse flow of the existing Parser and ExternalParser classes.&lt;/p&gt;

&lt;p&gt;In the tika-parsers project an ExternalEmbedderTest unit test is added which uses the default ExternalEmbedder (calls sed) to embed a value placed in Metadata.DESCRIPTION then verify the operation by parsing the resulting stream.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/embedder/Embedder.java</file>
			<file>/tika-parsers/src/test/java/org/apache/tika/embedder/ExternalEmbedderTest.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/embedder/ExternalEmbedder.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 8 Nov 2011 23:22:28 +0000" id="361" opendate="Tue, 8 Nov 2011 16:47:02 +0000">
		<buginformation>
			<summary>RTF parser incorrectly applies fonts to complete group</summary>
			<description>&lt;p&gt;Tika's RTF parser processes the following rtf document incorrectly, applying the wrong character encoding to the parsed characters:&lt;/p&gt;

{\rtf1\ansi\ansicpg1252\fromtext \fbidis \deff0
{\fonttbl
{\f0\fswiss\fcharset0 Arial;}
{\f1\fswiss\fcharset204 Arial;}
&lt;p&gt;}&lt;/p&gt;
{\f1\fs20 \'d3\'e2\'e0\'e6\'e0\'e5\'ec\'fb\'e9 \'ea\'eb\'e8\'e5\'ed\'f2!\f0}
&lt;p&gt;\par&lt;br/&gt;
}&lt;/p&gt;

&lt;p&gt;This document contains russian characters (\f1), but tika decodes these as latin due to the \f0 directive at the end of the group. The RTF parser should probably flush its pendingBytes buffer before processing directives such as these.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 26 Nov 2011 19:57:41 +0000" id="362" opendate="Wed, 9 Nov 2011 13:03:10 +0000">
		<buginformation>
			<summary>NullPointerException in tika-app, parsing PDF content</summary>
			<description>&lt;p&gt;I try to extract text from some pdf files with the tika app. In version 0.10 the error &lt;br/&gt;
ERROR - Error: Could not parse predefined CMAP file for '--UCS2'&lt;br/&gt;
is printed on the command line, but text extraction works and is correct.&lt;/p&gt;

&lt;p&gt;In version 1.0 I get the same error message on the command line, but also receive an exception and no text is extracted:&lt;br/&gt;
org.apache.tika.exception.TikaException: Unexpected RuntimeException from org.apache.tika.parser.pdf.PDFParser@62bc36ff&lt;br/&gt;
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:244)&lt;br/&gt;
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)&lt;br/&gt;
	at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)&lt;br/&gt;
	at org.apache.tika.gui.TikaGUI.handleStream(TikaGUI.java:320)&lt;br/&gt;
	at org.apache.tika.gui.TikaGUI.openFile(TikaGUI.java:279)&lt;br/&gt;
	at org.apache.tika.gui.TikaGUI.actionPerformed(TikaGUI.java:238)&lt;br/&gt;
	at javax.swing.AbstractButton.fireActionPerformed(AbstractButton.java:1995)&lt;br/&gt;
	at javax.swing.AbstractButton$Handler.actionPerformed(AbstractButton.java:2318)&lt;br/&gt;
	at javax.swing.DefaultButtonModel.fireActionPerformed(DefaultButtonModel.java:387)&lt;br/&gt;
	at javax.swing.DefaultButtonModel.setPressed(DefaultButtonModel.java:242)&lt;br/&gt;
	at javax.swing.AbstractButton.doClick(AbstractButton.java:357)&lt;br/&gt;
	at javax.swing.plaf.basic.BasicMenuItemUI.doClick(BasicMenuItemUI.java:809)&lt;br/&gt;
	at javax.swing.plaf.basic.BasicMenuItemUI$Handler.mouseReleased(BasicMenuItemUI.java:850)&lt;br/&gt;
	at java.awt.Component.processMouseEvent(Component.java:6288)&lt;br/&gt;
	at javax.swing.JComponent.processMouseEvent(JComponent.java:3267)&lt;br/&gt;
	at java.awt.Component.processEvent(Component.java:6053)&lt;br/&gt;
	at java.awt.Container.processEvent(Container.java:2041)&lt;br/&gt;
	at java.awt.Component.dispatchEventImpl(Component.java:4651)&lt;br/&gt;
	at java.awt.Container.dispatchEventImpl(Container.java:2099)&lt;br/&gt;
	at java.awt.Component.dispatchEvent(Component.java:4481)&lt;br/&gt;
	at java.awt.LightweightDispatcher.retargetMouseEvent(Container.java:4577)&lt;br/&gt;
	at java.awt.LightweightDispatcher.processMouseEvent(Container.java:4238)&lt;br/&gt;
	at java.awt.LightweightDispatcher.dispatchEvent(Container.java:4168)&lt;br/&gt;
	at java.awt.Container.dispatchEventImpl(Container.java:2085)&lt;br/&gt;
	at java.awt.Window.dispatchEventImpl(Window.java:2478)&lt;br/&gt;
	at java.awt.Component.dispatchEvent(Component.java:4481)&lt;br/&gt;
	at java.awt.EventQueue.dispatchEventImpl(EventQueue.java:643)&lt;br/&gt;
	at java.awt.EventQueue.access$000(EventQueue.java:84)&lt;br/&gt;
	at java.awt.EventQueue$1.run(EventQueue.java:602)&lt;br/&gt;
	at java.awt.EventQueue$1.run(EventQueue.java:600)&lt;br/&gt;
	at java.security.AccessController.doPrivileged(Native Method)&lt;br/&gt;
	at java.security.AccessControlContext$1.doIntersectionPrivilege(AccessControlContext.java:87)&lt;br/&gt;
	at java.security.AccessControlContext$1.doIntersectionPrivilege(AccessControlContext.java:98)&lt;br/&gt;
	at java.awt.EventQueue$2.run(EventQueue.java:616)&lt;br/&gt;
	at java.awt.EventQueue$2.run(EventQueue.java:614)&lt;br/&gt;
	at java.security.AccessController.doPrivileged(Native Method)&lt;br/&gt;
	at java.security.AccessControlContext$1.doIntersectionPrivilege(AccessControlContext.java:87)&lt;br/&gt;
	at java.awt.EventQueue.dispatchEvent(EventQueue.java:613)&lt;br/&gt;
	at java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:269)&lt;br/&gt;
	at java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:184)&lt;br/&gt;
	at java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:174)&lt;br/&gt;
	at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:169)&lt;br/&gt;
	at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:161)&lt;br/&gt;
	at java.awt.EventDispatchThread.run(EventDispatchThread.java:122)&lt;br/&gt;
Caused by: java.lang.NullPointerException&lt;br/&gt;
	at com.sun.org.apache.xml.internal.serializer.ToHTMLStream.endElement(ToHTMLStream.java:907)&lt;br/&gt;
	at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerHandlerImpl.endElement(TransformerHandlerImpl.java:273)&lt;br/&gt;
	at org.apache.tika.sax.ContentHandlerDecorator.endElement(ContentHandlerDecorator.java:136)&lt;br/&gt;
	at org.apache.tika.gui.TikaGUI$2.endElement(TikaGUI.java:519)&lt;br/&gt;
	at org.apache.tika.sax.TeeContentHandler.endElement(TeeContentHandler.java:94)&lt;br/&gt;
	at org.apache.tika.sax.ContentHandlerDecorator.endElement(ContentHandlerDecorator.java:136)&lt;br/&gt;
	at org.apache.tika.sax.SecureContentHandler.endElement(SecureContentHandler.java:256)&lt;br/&gt;
	at org.apache.tika.sax.ContentHandlerDecorator.endElement(ContentHandlerDecorator.java:136)&lt;br/&gt;
	at org.apache.tika.sax.ContentHandlerDecorator.endElement(ContentHandlerDecorator.java:136)&lt;br/&gt;
	at org.apache.tika.sax.ContentHandlerDecorator.endElement(ContentHandlerDecorator.java:136)&lt;br/&gt;
	at org.apache.tika.sax.SafeContentHandler.endElement(SafeContentHandler.java:273)&lt;br/&gt;
	at org.apache.tika.sax.XHTMLContentHandler.endDocument(XHTMLContentHandler.java:216)&lt;br/&gt;
	at org.apache.tika.parser.pdf.PDF2XHTML.endDocument(PDF2XHTML.java:112)&lt;br/&gt;
	at org.apache.pdfbox.util.PDFTextStripper.writeText(PDFTextStripper.java:323)&lt;br/&gt;
	at org.apache.tika.parser.pdf.PDF2XHTML.process(PDF2XHTML.java:61)&lt;br/&gt;
	at org.apache.tika.parser.pdf.PDFParser.parse(PDFParser.java:96)&lt;br/&gt;
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)&lt;br/&gt;
	... 43 more&lt;/p&gt;

&lt;p&gt;I tried the same pdf files (and can switch forth and back between version 0.10 and 1.0, this behavior is stable) and it looks like the exact same pdfbox version is inside the tika-app-0.10.jar and tika-app-1.0.jar. It would be great if version 1.0 could do what 0.10 can. Sorry that I cannot provide the pdf.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 15 Nov 2011 09:42:50 +0000" id="363" opendate="Thu, 10 Nov 2011 13:37:39 +0000">
		<buginformation>
			<summary>Detection of Microsoft Works 2000 Word Processor files</summary>
			<description>&lt;p&gt;In older versions of Tika, our Microsoft Works 2000 Word Processor example file would get recognized properly by the POIFSContainerDetector. Now it isn't. Some debugging revealed that the improvements from &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-704&quot; title=&quot;PDF and Outlook docs embedded in MS Word documents not parsed&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-704&quot;&gt;&lt;del&gt;TIKA-704&lt;/del&gt;&lt;/a&gt; broke the detection of that particular file. The detection is based on top-level names obtained from the root DirectoryNode. In case of this file there are two strings in that set: &quot;CONTENTS&quot; and &quot;\u0001CompObj&quot;. In older versions &quot;CONTENTS&quot; was enough to recognize a file as &quot;application/vnd.ms-works&quot;. Now it looks like this:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;if (names.contains(&quot;CONTENTS&quot;) &amp;amp;&amp;amp; names.contains(&quot;SPELLING&quot;)) {
   return WPS;
} else if (names.contains(&quot;CONTENTS&quot;)) {
   // CONTENTS without SPELLING normally means some sort of
   //  embedded non-office file inside an OLE2 document
   // This is most commonly triggered on nested directories
   return OLE;
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now I have a file with CONTENTS, but without SPELLING, and it's a normal WPS file. I did a workaround like this:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;if ( names.contains(&quot;CONTENTS&quot;) &amp;amp;&amp;amp; 
    (names.contains(&quot;SPELLING&quot;) || names.contains(&quot;\u0001CompObj&quot;))) {
   return WPS;
} else if (names.contains(&quot;CONTENTS&quot;)) {
   // CONTENTS without SPELLING normally means some sort of
   //  embedded non-office file inside an OLE2 document
   // This is most commonly triggered on nested directories
   return OLE;
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So &quot;CONTENTS&quot; has to be supplemented by &quot;SPELLING&quot; or &quot;\u0001CompObj&quot;. I don't know the meaning of this and I don't know if that second string also occurs in those &quot;embedded non-office files inside an OLE2 documents&quot;, referred to in that comment. The workaround solves the problem for me, the Tika build tests pass and regression tests of my apps pass as well.&lt;/p&gt;

&lt;p&gt;Jukka, do you have more than one WPS file, and all of them have both CONTENTS and SPELLING names in that collection? Is the &quot;\u0001CompObj&quot; string characteristic to this format, or is it a generic thing which also occurs on those &quot;non-office files&quot; or &quot;nested directories&quot;. If yes, just close this as wontfix. &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/POIFSContainerDetector.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 11 Nov 2011 12:30:00 +0000" id="364" opendate="Thu, 10 Nov 2011 16:52:11 +0000">
		<buginformation>
			<summary>Optimize loading of the media type registry</summary>
			<description>&lt;p&gt;Parsing of our pretty large media type registry takes quite a while (hundreds of milliseconds), which can be a problem for some applications. There's a lot of ways in which we could optimize the loading of the type registry.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/mime/MimeTypesReader.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/mime/MediaType.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/detect/MagicDetector.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 11 Nov 2011 18:27:42 +0000" id="365" opendate="Fri, 11 Nov 2011 15:31:46 +0000">
		<buginformation>
			<summary>RTF parser should ignore most control words in ignore groups</summary>
			<description>&lt;p&gt;The RTF parser should ignore control words like \par, \line and \tab when these occur in groups where ignore==true. This greatly improves the layout and readability of extracted text. The testRTFHyperlink.rtf file from the test documents is a good example of this.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 17 Nov 2011 20:34:25 +0000" id="366" opendate="Fri, 11 Nov 2011 20:09:23 +0000">
		<buginformation>
			<summary>Add support for parsing binary data in RTF files</summary>
			<description>&lt;p&gt;The current RTF parser doesn't process \bin control words yet. These control words are followed by a specific amount of binary data. Because of this, the RTF parser trips over some of these bytes in a number of (classified) documents.&lt;/p&gt;

&lt;p&gt;I've implemented processing of the \bin control word, but it required of the core parsing algorithm. IMHO, it also improved readability of the code. I hope you will accept this patch. Please let me know if the patch requires modifications.&lt;/p&gt;

&lt;p&gt;Apart from the \bin code word, this patch also makes the parser stop after reading the document-closing '}' character. In a number of files (again, classified), the parser would include non-readable characters that appeared after this closing brace.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 21 Nov 2011 01:25:48 +0000" id="367" opendate="Fri, 18 Nov 2011 12:36:03 +0000">
		<buginformation>
			<summary>Mimetype entry for DITA</summary>
			<description>&lt;p&gt;Currently, we don't have mimetype entries for DITA. There is a provisional mimetype&lt;/p&gt;

&lt;p&gt;According to &lt;a href=&quot;http://docs.oasis-open.org/dita/v1.2/cs01/spec/non-normative/DITA-mime-type.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://docs.oasis-open.org/dita/v1.2/cs01/spec/non-normative/DITA-mime-type.html&lt;/a&gt; there is a provisional mimetype of &quot;application/dita+xml&quot; for DITA files&lt;/p&gt;

&lt;p&gt;There are then three kinds of DITA file, which apparently all use the same mimetype:&lt;br/&gt;
 DITA Topic - .dita&lt;br/&gt;
 DITA Map - .ditamap&lt;br/&gt;
 DITA Conditional Processing Profile - .ditaval&lt;/p&gt;

&lt;p&gt;DITA is XML based, so we should be able to do XML detection in addition to filename matching&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/resources/test-documents/testDITA.dita</file>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 21 Nov 2011 01:04:58 +0000" id="368" opendate="Mon, 21 Nov 2011 01:02:23 +0000">
		<buginformation>
			<summary>TikaCLI should include a --list-detectors option similar to --list-parsers</summary>
			<description>&lt;p&gt;Now that Detectors can be found and loaded using the service pattern used by parsers, we should add a similar option to the TikaCLI to print out the available detectors&lt;/p&gt;

&lt;p&gt;TikaCLI already has a --list-parsers option, so this should logically be --list-detectors&lt;/p&gt;

&lt;p&gt;With parsers, we have the option to also list information on the supported mimetypes. This probably doesn't make sense for detectors, so we can go with a simpler pattern&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 23 Nov 2011 14:11:42 +0000" id="369" opendate="Wed, 23 Nov 2011 13:22:26 +0000">
		<buginformation>
			<summary>CharsetDetector text buffer is too small to small to correctly detect UTF-8 in HTML page</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/txt/CharsetDetector.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 23 Nov 2011 14:11:42 +0000" id="370" opendate="Fri, 25 Nov 2011 05:37:16 +0000">
		<buginformation>
			<summary>DWG parser infinite loop on possibly corrupt file</summary>
			<description>&lt;p&gt;When parsing some dwg items, it is possible that the parser may cause itself to go into an infinite loop.&lt;/p&gt;

&lt;p&gt;Attached is the file causing the problem.&lt;/p&gt;

&lt;p&gt;Here is a possible patch that will at least proceed until an error is thrown.&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt; &lt;pre&gt;=== modified file 'tika-parsers/src/main/java/org/apache/tika/parser/dwg/DWGParser.java'
--- tika-parsers/src/main/java/org/apache/tika/parser/dwg/DWGParser.java        2011-11-24 11:30:33 +0000
+++ tika-parsers/src/main/java/org/apache/tika/parser/dwg/DWGParser.java        2011-11-25 05:27:41 +0000
@@ -274,8 +274,10 @@
             return false;
         }
         while (toSkip &amp;gt; 0) {
-            byte[] skip = new byte[Math.min((int) toSkip, 0x4000)];
-            IOUtils.readFully(stream, skip);
+            byte[] skip = new byte[(int) Math.min(toSkip, 0x4000)];
+            if (IOUtils.readFully(stream, skip) == -1) {
+               return false; //invalid skip
+            }
             toSkip -= skip.length;
         }
         return true;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/dwg/DWGParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 25 Nov 2011 15:29:25 +0000" id="371" opendate="Fri, 25 Nov 2011 14:17:48 +0000">
		<buginformation>
			<summary>Microsoft Project (MPP) basic support</summary>
			<description>&lt;p&gt;The Microsoft Project file format (MPP) could fairly easily be better supported by Tika. Gaps to fill are:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Correct mimetype definition (it's OLE2 based)&lt;/li&gt;
	&lt;li&gt;OLE2 detection for MPP&lt;/li&gt;
	&lt;li&gt;Common OLE2 metadata extraction&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;For fuller support (such as text contents), we'd probably want a parser which used MPXJ. However, as MPXJ is LGPL, it'd need to be an external 3rd party parser. (MPXJ is based on top of POI, but it's under a more copyleft license. POI itself doesn't have MPP support)&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/POIFSContainerDetector.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/OfficeParser.java</file>
			<file>/tika-parsers/src/test/resources/test-documents/testPROJECT2003.mpp</file>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
			<file>/tika-parsers/src/test/resources/test-documents/testMPP2003.mpp</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 28 Nov 2011 13:59:55 +0000" id="372" opendate="Fri, 25 Nov 2011 14:43:12 +0000">
		<buginformation>
			<summary>Reduce duplication between POIFSDocumentType (in OfficeParser) and POIFSContainerDetector</summary>
			<description>&lt;p&gt;For historical reasons, we now have two parts of Tika that handle trying to identify the type of an OLE2 based file.&lt;/p&gt;

&lt;p&gt;POIFSDocumentType is able to detect a few kinds of files that POIFSContainerDetector is not able to (eg Encrypted and OLE Native), mostly which may not map well onto mimetypes. POIFSDocumentType also lacks some of the logic in the main detector, and only does the office parser supported files&lt;/p&gt;

&lt;p&gt;We should probably try to reduce the duplication. One option is to add the extra few types into the Detector some how, the other is to use the detector first and do additional specific checks after&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 26 Sep 2013 15:27:14 +0000" id="373" opendate="Sat, 26 Nov 2011 16:08:49 +0000">
		<buginformation>
			<summary>NoSuchMethodException &quot;CTMarkupImpl.&lt;init&gt;(org.apache.xmlbeans.SchemaType, boolean)&quot; processing a OOXML document</summary>
			<description>&lt;p&gt;Parsing some OOXML documents, this stacktrace is logged many times:&lt;/p&gt;

&lt;p&gt;java.lang.NoSuchMethodException: org.openxmlformats.schemas.wordprocessingml.x2006.main.impl.CTMarkupImpl.&amp;lt;init&amp;gt;(org.apache.xmlbeans.SchemaType, boolean)&lt;br/&gt;
	at java.lang.Class.getConstructor0(Class.java:2723)&lt;br/&gt;
	at java.lang.Class.getDeclaredConstructor(Class.java:2002)&lt;br/&gt;
	at org.apache.xmlbeans.impl.schema.SchemaTypeImpl.getJavaImplConstructor2(SchemaTypeImpl.java:1749)&lt;br/&gt;
	at org.apache.xmlbeans.impl.schema.SchemaTypeImpl.createUnattachedSubclass(SchemaTypeImpl.java:1886)&lt;br/&gt;
	at org.apache.xmlbeans.impl.schema.SchemaTypeImpl.createUnattachedNode(SchemaTypeImpl.java:1875)&lt;br/&gt;
	at org.apache.xmlbeans.impl.schema.SchemaTypeImpl.createElementType(SchemaTypeImpl.java:1021)&lt;br/&gt;
	at org.apache.xmlbeans.impl.values.XmlObjectBase.create_element_user(XmlObjectBase.java:893)&lt;br/&gt;
	at org.apache.xmlbeans.impl.store.Xobj.getUser(Xobj.java:1657)&lt;br/&gt;
	at org.apache.xmlbeans.impl.store.Cur.getUser(Cur.java:2654)&lt;br/&gt;
	at org.apache.xmlbeans.impl.store.Cur.getObject(Cur.java:2647)&lt;br/&gt;
	at org.apache.xmlbeans.impl.store.Cursor._getObject(Cursor.java:995)&lt;br/&gt;
	at org.apache.xmlbeans.impl.store.Cursor.getObject(Cursor.java:2904)&lt;br/&gt;
	at org.apache.poi.xwpf.usermodel.XWPFParagraph.&amp;lt;init&amp;gt;(XWPFParagraph.java:83)&lt;br/&gt;
	at org.apache.poi.xwpf.usermodel.XWPFDocument.onDocumentRead(XWPFDocument.java:145)&lt;br/&gt;
	at org.apache.poi.POIXMLDocument.load(POIXMLDocument.java:159)&lt;br/&gt;
	at org.apache.poi.xwpf.usermodel.XWPFDocument.&amp;lt;init&amp;gt;(XWPFDocument.java:115)&lt;br/&gt;
	at org.apache.poi.xwpf.extractor.XWPFWordExtractor.&amp;lt;init&amp;gt;(XWPFWordExtractor.java:53)&lt;br/&gt;
	at org.apache.poi.extractor.ExtractorFactory.createExtractor(ExtractorFactory.java:180)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.ooxml.OOXMLExtractorFactory.parse(OOXMLExtractorFactory.java:63)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.ooxml.OOXMLParser.parse(OOXMLParser.java:69)&lt;br/&gt;
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)&lt;br/&gt;
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)&lt;br/&gt;
	at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)&lt;/p&gt;


&lt;p&gt;Looking at the poi code java is right here, there is no constructor with a SchemaType and a boolean, only with SchemaType.&lt;br/&gt;
My guess is this one was missed during upgrade to poi beta4, but only a guess, anyway needs a fix &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/microsoft/ooxml/OOXMLParserTest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 29 Dec 2011 09:12:19 +0000" id="374" opendate="Sun, 27 Nov 2011 08:45:52 +0000">
		<buginformation>
			<summary>Invalid ASCII character (65533) when retriving MP3 metadata</summary>
			<description>&lt;p&gt;When extracting metadata from certain mp3's (the id3 version appears to be 2.4) I'm seeing invalid characters at the end of the parsed fields. For example:&lt;/p&gt;

&lt;p&gt;American M�&lt;/p&gt;

&lt;p&gt;which should be:&lt;/p&gt;

&lt;p&gt;American Me&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/resources/test-documents/testMP3i18n.mp3</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/mp3/ID3v22Handler.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/mp3/CompositeTagHandler.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/mp3/ID3v2Frame.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 28 Nov 2011 00:27:40 +0000" id="375" opendate="Mon, 28 Nov 2011 00:25:32 +0000">
		<buginformation>
			<summary>Mime magic logic for Little16 is incorrect</summary>
			<description>&lt;p&gt;The mime magic logic for big16 and little16 seems to be the same:&lt;/p&gt;

&lt;p&gt;         } else if (type.equals(&quot;host16&quot;) || type.equals(&quot;little16&quot;)) {&lt;br/&gt;
             int i = Integer.parseInt(tmpVal, radix);&lt;br/&gt;
             decoded = new byte[] &lt;/p&gt;
{ (byte) (i &amp;gt;&amp;gt; 8), (byte) (i &amp;amp; 0x00FF) };&lt;br/&gt;
         } else if (type.equals(&quot;big16&quot;)) {&lt;br/&gt;
             int i = Integer.parseInt(tmpVal, radix);&lt;br/&gt;
             decoded = new byte[] { (byte) (i &amp;gt;&amp;gt; 8), (byte) (i &amp;amp; 0x00FF) }
&lt;p&gt;;&lt;/p&gt;

&lt;p&gt;It looks like both are calculating the big endian version, which is breaking CPIO detection&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/detect/MagicDetector.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 13 Dec 2011 16:21:39 +0000" id="376" opendate="Fri, 2 Dec 2011 13:42:33 +0000">
		<buginformation>
			<summary>Distinguish between EMF and WMF</summary>
			<description>&lt;p&gt;I'd like MimeTypes to distinguish between EMF and WMF. These are different formats with different magics.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 7 Nov 2012 13:00:13 +0000" id="377" opendate="Sat, 3 Dec 2011 02:26:35 +0000">
		<buginformation>
			<summary>ForkParser does not populate metadata object after completing a parse</summary>
			<description>&lt;p&gt;ForkParser does not add any new metadata to the metadata object passed to the parse method.&lt;br/&gt;
See these two links which describe the problem:&lt;br/&gt;
&lt;a href=&quot;http://stackoverflow.com/questions/8349898/why-is-my-tika-metadata-object-not-being-populated-when-using-forkparser&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://stackoverflow.com/questions/8349898/why-is-my-tika-metadata-object-not-being-populated-when-using-forkparser&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/tika-user/201112.mbox/%3CCAOFYJNZ7hsATwfFbMmECjYY9Cii5BuJH7qGfXC08Yrqx%2BDmTvA%40mail.gmail.com%3E&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://mail-archives.apache.org/mod_mbox/tika-user/201112.mbox/%3CCAOFYJNZ7hsATwfFbMmECjYY9Cii5BuJH7qGfXC08Yrqx%2BDmTvA%40mail.gmail.com%3E&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/fork/ForkParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 13 Dec 2011 04:14:45 +0000" id="378" opendate="Thu, 8 Dec 2011 08:58:09 +0000">
		<buginformation>
			<summary>Outlook parser to mark the message body in some special way</summary>
			<description>&lt;p&gt;Currently, the Outlook parser does not mark the message body in any special way. It would be great if Outlook parser can mark the message body with something specific or provide some way of retrieving just the body of the message, so that Solr can use it through its ExtractingRequestHandler.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/OutlookExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 16 Jan 2012 10:40:49 +0000" id="379" opendate="Fri, 9 Dec 2011 14:47:06 +0000">
		<buginformation>
			<summary>improvements in XSLFPowerPointExtractorDecorator </summary>
			<description>&lt;p&gt;I'm finalizing the XSLF api in Apache POI and would like to improve the corresponding part on the Tika side. &lt;/p&gt;

&lt;p&gt;Current XSLFPowerPointExtractorDecorator  is too low-level and doesn't correlate much with the XSLF usermodel. My patch eliminates direct manipulations with low-level XmlBeans and uses high-level usermodel objects instead. &lt;/p&gt;

&lt;p&gt;Cheers,&lt;br/&gt;
Yegor   &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/XSLFPowerPointExtractorDecorator.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 23 Dec 2011 23:34:37 +0000" id="380" opendate="Mon, 12 Dec 2011 02:13:39 +0000">
		<buginformation>
			<summary>Fork Parser doesn't work for PDF files</summary>
			<description>&lt;p&gt;There seems to be something wrong with the fork parser and PDF files. &lt;/p&gt;

&lt;p&gt;If you run tika-app with the --text option against tika-parsers/src/test/resources/test-documents/testPDF.pdf then you get the text of the pdf back. However, with &quot;-f --text&quot; no text is returned (but you get no errors either)&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/test/java/org/apache/tika/fork/ForkParserTest.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/fork/ForkClient.java</file>
			<file>/tika-parent/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 30 Jun 2012 15:05:46 +0000" id="381" opendate="Mon, 12 Dec 2011 19:27:45 +0000">
		<buginformation>
			<summary>Upgrade to PDFbox 1.7.0 as available</summary>
			<description>&lt;p&gt;This isssue is to track upgrading the PDFbox dependency to 1.7.0 Final once it's available, and the daily build before then&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 3 Apr 2012 16:04:08 +0000" id="382" opendate="Mon, 19 Dec 2011 15:57:13 +0000">
		<buginformation>
			<summary>(XLS/XLSX) Improperly formatted date/time in text content.</summary>
			<description>&lt;p&gt;Improperly formated text content for XLS and XLSX files.&lt;/p&gt;

&lt;p&gt;The date and time are not formatted as date/time data but rather floating point numbers.  This occurs for cells with the content as &quot;=now()&quot; or &quot;=today()&quot;.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ExcelExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 27 Sep 2013 16:51:40 +0000" id="383" opendate="Mon, 19 Dec 2011 16:07:09 +0000">
		<buginformation>
			<summary>(PPT/PPTX) Missing date/time in text content.</summary>
			<description>&lt;p&gt;Missing date/time text in text content for PPT and PPTX files.&lt;/p&gt;

&lt;p&gt;The date and time are missing from the text content.  This occurs when one chooses the following with MS-PowerPoint 2010:&lt;br/&gt;
1) &quot;Insert&quot;&lt;br/&gt;
2) &quot;Date &amp;amp; Time&quot;&lt;br/&gt;
3) &quot;Update automatically&quot;&lt;br/&gt;
4) save to PPT or PPTX&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/sax/ToXMLContentHandler.java</file>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/microsoft/PowerPointParserTest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 10 Feb 2012 14:21:12 +0000" id="384" opendate="Mon, 19 Dec 2011 18:24:00 +0000">
		<buginformation>
			<summary>Allow PDFBox to be used with RandomAccessFile vs RandomAccessBuffer to allow for a memory vs performance tradeoff</summary>
			<description>&lt;p&gt;After upgrading to Tika 0.10, began having OOM errors processing large amounts of PDFs in parallel. The heap dump indicated that all the memory was getting used up by PDFBox RandomAccessBuffers. After digging around, it looks like PDFBox now defaults to using RAM vs temporary files for PDF extraction. This can be overridden to use RandomAccessFiless. &lt;/p&gt;

&lt;p&gt;I propose that Tika controls file vs buffer based on the inputstream type received. If the TikaInputStream is a file, RandomAccessFile should be used and for other stream types, RandomAccessBuffer can be used. &lt;/p&gt;

&lt;p&gt;I believe the code to control this is here:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/tika/blob/trunk/tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDFParser.java&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/tika/blob/trunk/tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDFParser.java&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;At ~line 87:&lt;br/&gt;
PDDocument pdfDocument =&lt;br/&gt;
            PDDocument.load(new CloseShieldInputStream(stream), true);&lt;/p&gt;

&lt;p&gt;Not sure if this is the best approach and am curious if there are other ideas on how to control this and keep the interface clean. &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDFParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 28 Dec 2013 22:46:15 +0000" id="385" opendate="Tue, 20 Dec 2011 09:00:27 +0000">
		<buginformation>
			<summary>Locator is unset for HTML parser</summary>
			<description>&lt;p&gt;The HtmlParser does not call setDocumentLocator(Locator locator) on the user's content handler.&lt;/p&gt;

&lt;p&gt;Patch and unit test attached.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/sax/TextContentHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 21 Dec 2011 03:03:54 +0000" id="386" opendate="Tue, 20 Dec 2011 18:42:48 +0000">
		<buginformation>
			<summary>MediaType fails to parse charset that has quoted value</summary>
			<description>&lt;p&gt;If a mime type is&lt;/p&gt;

&lt;p&gt;text/html; charset=&quot;UTF-8&quot;&lt;/p&gt;

&lt;p&gt;the value is incorrectly &quot;UTF-8&quot; not UTF-8&lt;/p&gt;

&lt;p&gt;patch available at &lt;a href=&quot;https://github.com/osi/tika/commit/b77814874ebff8f412ebb2f2adc52c6465d603c4&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/osi/tika/commit/b77814874ebff8f412ebb2f2adc52c6465d603c4&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;i have a CLA on file.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/mime/MediaType.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 23 Dec 2011 22:54:27 +0000" id="387" opendate="Fri, 23 Dec 2011 11:26:37 +0000">
		<buginformation>
			<summary>TaggedIOException can be passed non Serializable objects</summary>
			<description>&lt;p&gt;TaggedIOException can contain tags. It's used to contain TaggedInputStream which isn't serializable.&lt;/p&gt;

&lt;p&gt;This can cause the ForkServer to fail when trying to report issues. See &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-827&quot; title=&quot;ForkServer fails to report issues if an exception is not properly serializable&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-827&quot;&gt;&lt;del&gt;TIKA-827&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;2 solutions&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;make the tag transient&lt;/li&gt;
	&lt;li&gt;replace the InputStream instance in the tag by a serializable object specific to the input stream.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I opt for the first one as I really don't think we need more complexity&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/io/TaggedInputStream.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 26 Dec 2011 04:05:39 +0000" id="388" opendate="Fri, 23 Dec 2011 11:28:45 +0000">
		<buginformation>
			<summary>Tika lacks preconditions on its input, causing some potential misuse of the API</summary>
			<description>&lt;p&gt;In particular:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;ForkParser accepts a ForkParser as underlying parser&lt;/li&gt;
	&lt;li&gt;InputStream can be null&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Those 2 issues were found in broken integration tests for &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-808&quot; title=&quot;Fork Parser doesn&amp;#39;t work for PDF files&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-808&quot;&gt;&lt;del&gt;TIKA-808&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/fork/ForkParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 16 May 2012 21:47:30 +0000" id="389" opendate="Fri, 23 Dec 2011 11:42:10 +0000">
		<buginformation>
			<summary>Tika.parseToString() causes ForkParser to try to serialize itself</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/fork/ForkClient.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 27 Dec 2011 02:46:14 +0000" id="390" opendate="Fri, 23 Dec 2011 12:08:35 +0000">
		<buginformation>
			<summary>ForkClient doesn't report error due to widening conversion issue</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/fork/ForkParserIntegrationTest.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/fork/ForkClient.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 30 Jun 2012 16:31:27 +0000" id="391" opendate="Fri, 23 Dec 2011 13:35:05 +0000">
		<buginformation>
			<summary>ForkParser is unfriendly to code that prints things to its output</summary>
			<description>&lt;p&gt;When given a java command that causes java to write something to the output, like a debugging instruction, tika fails.&lt;/p&gt;

&lt;p&gt;I attach 2 patches that solve the issue in different way. Both use the same unit test&lt;/p&gt;

&lt;p&gt;But I don't know it this is worth the complexity. At least to start a discussion.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/fork/ForkClient.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 13 Jan 2012 15:02:51 +0000" id="392" opendate="Thu, 12 Jan 2012 14:58:02 +0000">
		<buginformation>
			<summary>OOXML parser content type setting</summary>
			<description>&lt;p&gt;At the moment, some of the OOXML parsers set a content type, and others don't. Those that do always set the base type, no matter the contents&lt;/p&gt;

&lt;p&gt;Instead, the detection logic should be re-used where possible within the parsers, with overriding only needed for things like protected files&lt;/p&gt;

&lt;p&gt;(Once fixed, a few parts of the ooxml tests around types that are currently commented out can be re-enabled)&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pkg/ZipContainerDetector.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/AbstractOOXMLExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 18 Jan 2012 14:27:38 +0000" id="393" opendate="Mon, 16 Jan 2012 12:18:54 +0000">
		<buginformation>
			<summary>User supplied parsers should be preferred</summary>
			<description>&lt;p&gt;Currently, user supplied Detectors are preferred over built in ones, via logic in DefaultDetector. This allows users to easily add their own detectors which are used in preference, as well as making it easy to override the built in ones.&lt;/p&gt;

&lt;p&gt;However, there is no such logic for Parsers. Instead, the last parser in the DefaultParser / CompositeParser list for a given mimetype will be used (the map only holds one entry, so last in wins). This makes it hard for users to override the parser for a type that the builtin parsers support, as it isn't predictable where in the list parsers will go&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/parser/DefaultParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 17 May 2012 20:36:55 +0000" id="394" opendate="Mon, 16 Jan 2012 18:45:02 +0000">
		<buginformation>
			<summary>IPTC Properties Should be Defined Completely and Independently of the Drew Library</summary>
			<description>&lt;p&gt;All of the IPTC XMP specification should be defined in tika-core and should not be reliant on the Drew Noakes library as it is incomplete in its support of the standard and the properties are not defined in proper namespaces or prefixed.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/metadata/DublinCore.java</file>
			<file>/LICENSE.txt</file>
			<file>/tika-core/src/main/java/org/apache/tika/metadata/IPTC.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 23 Jan 2012 15:48:42 +0000" id="395" opendate="Mon, 16 Jan 2012 18:59:18 +0000">
		<buginformation>
			<summary>Support for Date without a Time Component</summary>
			<description>&lt;p&gt;Should be able to support parsing of dates without a time component, i.e. 2011:08:31&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/metadata/Metadata.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 23 Jan 2012 15:46:43 +0000" id="396" opendate="Mon, 16 Jan 2012 19:03:36 +0000">
		<buginformation>
			<summary>Ability to Define an Internal Text Bag Property</summary>
			<description>&lt;p&gt;Should be able to define an internal text bag type.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/metadata/Property.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 23 Jan 2012 16:08:25 +0000" id="397" opendate="Mon, 16 Jan 2012 19:15:44 +0000">
		<buginformation>
			<summary>Check for Existing Value in Multi-Value Fields in XML Metadata Handler</summary>
			<description>&lt;p&gt;The XML Abstract metdata handler should check for an existing value for multi-valued fields as well as simple text fields.&lt;/p&gt;

&lt;p&gt;Similar metadata may be stored in multiple fields in the source and a developer may choose to map several source fields to the same tika field, in which case no check is made for duplicates of existing delimited values.&lt;/p&gt;

&lt;p&gt;For example, a developer may want to dump any values contained in legacy IPTC keywords and dc:subject into tika keywords.  If IPTC keywords = &lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;rock&amp;#39;,&amp;#39;tree&amp;#39;,&amp;#39;dog&amp;#39;&amp;#93;&lt;/span&gt; and dc:subject = &lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;rock&amp;#39;,&amp;#39;tree&amp;#39;,&amp;#39;K9&amp;#39;&amp;#93;&lt;/span&gt; then currently tika keywords = &lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;rock&amp;#39;,&amp;#39;tree&amp;#39;,&amp;#39;dog&amp;#39;,&amp;#39;rock&amp;#39;,&amp;#39;tree&amp;#39;,&amp;#39;K9&amp;#39;&amp;#93;&lt;/span&gt; instead of the desired &lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;rock&amp;#39;,&amp;#39;tree&amp;#39;,&amp;#39;dog&amp;#39;,&amp;#39;K9&amp;#39;&amp;#93;&lt;/span&gt;.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/xml/AbstractMetadataHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 23 Jan 2012 15:48:11 +0000" id="398" opendate="Mon, 16 Jan 2012 19:39:25 +0000">
		<buginformation>
			<summary>Ability to Parse RDF Bag Elements in XML</summary>
			<description>&lt;p&gt;Should be able to parse RDF bag elements as multi-valued metadata fields, i.e:&lt;br/&gt;
 &amp;lt;IPTC:Contact&amp;gt;&lt;br/&gt;
  &amp;lt;rdf:Bag&amp;gt;&lt;br/&gt;
   &amp;lt;rdf:li&amp;gt;Ray&amp;lt;/rdf:li&amp;gt;&lt;br/&gt;
   &amp;lt;rdf:li&amp;gt;Gavin&amp;lt;/rdf:li&amp;gt;&lt;br/&gt;
  &amp;lt;/rdf:Bag&amp;gt;&lt;br/&gt;
 &amp;lt;/IPTC:Contact&amp;gt;&lt;/p&gt;

&lt;p&gt;Unfortunately in some libraries and command lines a single value in that same value may be presented as:&lt;br/&gt;
 &amp;lt;IPTC:Contact&amp;gt;Ray&amp;lt;/IPTC:Contact&amp;gt;&lt;/p&gt;

&lt;p&gt;ElementMetadataHandler should have a constructor which accepts a metadata Property in addition to the constructor which accepts a metadata name and be able to parse either format above if that Property.PropertyType is a bag.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/xml/ElementMetadataHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 30 Jun 2012 15:57:14 +0000" id="399" opendate="Tue, 17 Jan 2012 11:06:46 +0000">
		<buginformation>
			<summary>Add regular expression support to the MagicDetector</summary>
			<description>&lt;p&gt;Following on from &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-86&quot; title=&quot;Support magic(5) files&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-86&quot;&gt;&lt;del&gt;TIKA-86&lt;/del&gt;&lt;/a&gt;, we would like to add support for regular expressions to the MagicDetector. This would allow more signatures to be re-used from more sources (e.g. the file(1) command). As part of the SCAPE Project, we have added this functionality to our own Tika branch (e.g. &lt;a href=&quot;https://github.com/openplanets/tika/commit/b8de9e77c8b432788e3f73a4dbccca8ea044b503&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/openplanets/tika/commit/b8de9e77c8b432788e3f73a4dbccca8ea044b503&lt;/a&gt;) and are working to tidy this up to make a clean patch we can submit here.&lt;/p&gt;

&lt;p&gt;BTW, are there any patch submission guidelines or coding standards we should check our work against first?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/detect/MagicDetector.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 16 Feb 2012 11:21:22 +0000" id="400" opendate="Tue, 24 Jan 2012 14:40:05 +0000">
		<buginformation>
			<summary>Consistent way to supply document passwords to parsers</summary>
			<description>&lt;p&gt;Currently, PDF document passwords are supplied to the parser via a special key on the Metadata object, while the Office Parser has a TODO and only supports the default password&lt;/p&gt;

&lt;p&gt;We should update all the parsers that support encrypted documents (currently PDF, Office OLE2 and Office OOXML) to receive the password in a consistent way&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/parser/PasswordProvider.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 27 Jan 2012 14:53:41 +0000" id="401" opendate="Fri, 27 Jan 2012 13:10:14 +0000">
		<buginformation>
			<summary>M4V and M4A detection invalid</summary>
			<description>&lt;p&gt;When the mime type of an M4V file is detected using its name only, it returns video/x-m4v.  When it is detected using the InputStream (hence utilising the MagicDetector), it incorrectly returns video/quicktime.&lt;/p&gt;

&lt;p&gt;Using the sample M4V file from Apple's &lt;a href=&quot;http://support.apple.com/kb/HT1425&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;knowledge base&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeHeader panelHeader&quot; style=&quot;border-bottom-width: 1px;&quot;&gt;&lt;b&gt;TikaTest.java&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;TikaTest {

	&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; void main(&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;[] args) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; Exception {
		&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; userHome = &lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.getProperty(&lt;span class=&quot;code-quote&quot;&gt;&quot;user.home&quot;&lt;/span&gt;);

		File file = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; File(userHome + &lt;span class=&quot;code-quote&quot;&gt;&quot;/Desktop/sample_iPod.m4v&quot;&lt;/span&gt;);

		InputStream is = TikaInputStream.get(file);

		Detector detector = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; DefaultDetector(
			MimeTypes.getDefaultMimeTypes());

		Metadata metadata = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Metadata();

		metadata.set(Metadata.RESOURCE_NAME_KEY, file.getName());

		&lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.out.println(&lt;span class=&quot;code-quote&quot;&gt;&quot;File + filename: &quot;&lt;/span&gt; + detector.detect(is, metadata));

		&lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.out.println(&lt;span class=&quot;code-quote&quot;&gt;&quot;File only:       &quot;&lt;/span&gt; + detector.detect(is, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Metadata()));

		&lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.out.println(&lt;span class=&quot;code-quote&quot;&gt;&quot;Filename only:   &quot;&lt;/span&gt; + detector.detect(&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;, metadata));
	}

}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Renders the output:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;File + filename: video/quicktime
File only:       video/quicktime
Filename only:   video/x-m4v
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Moreover, if the same test is run against an M4A file, the results are even more incorrect:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;File + filename: video/quicktime
File only:       video/quicktime
Filename only:   application/octet-stream
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 27 Jan 2012 14:53:41 +0000" id="402" opendate="Sat, 28 Jan 2012 18:00:13 +0000">
		<buginformation>
			<summary>Quicktime / MP4 Metadata Parser</summary>
			<description>&lt;p&gt;From the investigations done for &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-851&quot; title=&quot;M4V and M4A detection invalid&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-851&quot;&gt;&lt;del&gt;TIKA-851&lt;/del&gt;&lt;/a&gt;, it looks like a parser for the Quicktime format, and MP4 (which is an extension to it) shouldn't be too hard to do. This should be able to output some of the media metadata, such duration, dimensions, and MP4 audio tags&lt;/p&gt;

&lt;p&gt;Information resources on the format are linked from &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-851&quot; title=&quot;M4V and M4A detection invalid&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-851&quot;&gt;&lt;del&gt;TIKA-851&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/main/appended-resources/META-INF/LICENSE</file>
			<file>/tika-bundle/pom.xml</file>
			<file>/tika-parsers/pom.xml</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/mp4/MP4Parser.java</file>
			<file>/tika-parsers/src/test/resources/test-documents/testMP4.m4a</file>
			<file>/tika-core/src/main/java/org/apache/tika/metadata/XMPDM.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 27 Apr 2012 14:01:25 +0000" id="403" opendate="Tue, 14 Feb 2012 01:17:15 +0000">
		<buginformation>
			<summary>Parse links in PDF</summary>
			<description>&lt;p&gt;Currently the XHTML doesn't contain links, although PDFBox parses them. I'm new to Tika and haven't done java for 6 years, but someone more experienced could probably do this in a few hours. &lt;/p&gt;

&lt;p&gt;The PDF2XHTML method loops through the annotations. &lt;/p&gt;

&lt;p&gt;See: &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;136: &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt;(&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt; o : page.getAnnotations()) {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt; I found some code for dealing with links in annotations:&lt;br/&gt;
&lt;a href=&quot;http://stackoverflow.com/questions/7174709/pdfbox-not-recognizing-a-link&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://stackoverflow.com/questions/7174709/pdfbox-not-recognizing-a-link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It involves checking the class. &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt;( annotation &lt;span class=&quot;code-keyword&quot;&gt;instanceof&lt;/span&gt; PDAnnotationLink ) {
                PDAnnotationLink link = (PDAnnotationLink)annotation;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I hope this helps someone.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDF2XHTML.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 17 Feb 2012 15:10:44 +0000" id="404" opendate="Fri, 17 Feb 2012 01:12:30 +0000">
		<buginformation>
			<summary>Metadata.formatDate causes blocking in concurrent use</summary>
			<description>&lt;p&gt;Currently this is a synchronized method that uses a single instance of DateFormat. Instead it could use a pool of ThreadLocal DateFormat instances and avoid the sync blocking.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/metadata/Metadata.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 17 Feb 2012 13:42:33 +0000" id="405" opendate="Fri, 17 Feb 2012 12:37:26 +0000">
		<buginformation>
			<summary>Invalid configuration file causes OutOfMemoryException</summary>
			<description>&lt;p&gt;I tried to override a built-in parser according to the method described in issue &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-527&quot; title=&quot;Allow override mapping mime&amp;lt;--&amp;gt;parsers through config&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-527&quot;&gt;&lt;del&gt;TIKA-527&lt;/del&gt;&lt;/a&gt;. During testing this approach I used an incomplete configuration file (as far as I learned from a discussion on the mailing list also mimetypes and a detector should be specified):&lt;/p&gt;

&lt;p&gt;$ cat tika-config.xml&lt;br/&gt;
&amp;lt;properties&amp;gt;&lt;br/&gt;
&amp;lt;parsers&amp;gt;&lt;br/&gt;
&amp;lt;parser class=&quot;org.apache.tika.parser.DefaultParser&quot;/&amp;gt;&lt;br/&gt;
&amp;lt;/parsers&amp;gt;&lt;br/&gt;
&amp;lt;/properties&amp;gt;&lt;/p&gt;

&lt;p&gt;Using this configuration file causes an OutOfMemoryException:&lt;/p&gt;

&lt;p&gt;$ java -Dtika.config=tika-config.xml -jar tika-app-1.0.jar --list-parsers&lt;br/&gt;
Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: GC overhead limit exceeded&lt;br/&gt;
        at java.util.Arrays.copyOfRange(Arrays.java:3209)&lt;br/&gt;
        at java.lang.String.&amp;lt;init&amp;gt;(String.java:216)&lt;br/&gt;
        at java.lang.StringBuilder.toString(StringBuilder.java:430)&lt;br/&gt;
        at org.apache.tika.mime.MediaType.toString(MediaType.java:237)&lt;br/&gt;
        at org.apache.tika.detect.MagicDetector.&amp;lt;init&amp;gt;(MagicDetector.java:142)&lt;br/&gt;
        at org.apache.tika.mime.MimeTypesReader.readMatch(MimeTypesReader.java:254)&lt;br/&gt;
        at org.apache.tika.mime.MimeTypesReader.readMatches(MimeTypesReader.java:202)&lt;br/&gt;
        at org.apache.tika.mime.MimeTypesReader.readMagic(MimeTypesReader.java:186)&lt;br/&gt;
        at org.apache.tika.mime.MimeTypesReader.readMimeType(MimeTypesReader.java:152)&lt;br/&gt;
        at org.apache.tika.mime.MimeTypesReader.read(MimeTypesReader.java:124)&lt;br/&gt;
        at org.apache.tika.mime.MimeTypesReader.read(MimeTypesReader.java:107)&lt;br/&gt;
        at org.apache.tika.mime.MimeTypesFactory.create(MimeTypesFactory.java:63)&lt;br/&gt;
        at org.apache.tika.mime.MimeTypesFactory.create(MimeTypesFactory.java:91)&lt;br/&gt;
        at org.apache.tika.mime.MimeTypesFactory.create(MimeTypesFactory.java:147)&lt;br/&gt;
        at org.apache.tika.mime.MimeTypes.getDefaultMimeTypes(MimeTypes.java:455)&lt;br/&gt;
        at org.apache.tika.config.TikaConfig.typesFromDomElement(TikaConfig.java:273)&lt;br/&gt;
        at org.apache.tika.config.TikaConfig.&amp;lt;init&amp;gt;(TikaConfig.java:161)&lt;br/&gt;
        at org.apache.tika.config.TikaConfig.getDefaultConfig(TikaConfig.java:237)&lt;br/&gt;
        at org.apache.tika.mime.MediaTypeRegistry.getDefaultRegistry(MediaTypeRegistry.java:42)&lt;br/&gt;
        at org.apache.tika.parser.DefaultParser.&amp;lt;init&amp;gt;(DefaultParser.java:52)&lt;br/&gt;
        at sun.reflect.GeneratedConstructorAccessor4.newInstance(Unknown Source)&lt;br/&gt;
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)&lt;br/&gt;
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)&lt;br/&gt;
        at java.lang.Class.newInstance0(Class.java:355)&lt;br/&gt;
        at java.lang.Class.newInstance(Class.java:308)&lt;br/&gt;
        at org.apache.tika.config.TikaConfig.parserFromDomElement(TikaConfig.java:288)&lt;br/&gt;
        at org.apache.tika.config.TikaConfig.&amp;lt;init&amp;gt;(TikaConfig.java:162)&lt;br/&gt;
        at org.apache.tika.config.TikaConfig.getDefaultConfig(TikaConfig.java:237)&lt;br/&gt;
        at org.apache.tika.mime.MediaTypeRegistry.getDefaultRegistry(MediaTypeRegistry.java:42)&lt;br/&gt;
        at org.apache.tika.parser.DefaultParser.&amp;lt;init&amp;gt;(DefaultParser.java:52)&lt;br/&gt;
        at sun.reflect.GeneratedConstructorAccessor4.newInstance(Unknown Source)&lt;br/&gt;
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27) &lt;/p&gt;

&lt;p&gt;Expected behavior: If the configuration file is not valid, and appropriate exception should be produced.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/config/TikaConfig.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/config/ServiceLoader.java</file>
			<file>/tika-core/src/test/java/org/apache/tika/config/TikaConfigTest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 13 Mar 2012 14:00:05 +0000" id="406" opendate="Tue, 13 Mar 2012 09:42:13 +0000">
		<buginformation>
			<summary>Temporary file leak in ImageParser</summary>
			<description>&lt;p&gt;The stream obtained through ImageIO.createImageInputStream is not closed (org.apache.tika.parser.image.ImageParser.parse(InputStream, ContentHandler, Metadata, ParseContext)). When performing a lot of parse operations, the process can easily run out of file descriptors.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 30 Jun 2012 13:02:20 +0000" id="407" opendate="Wed, 14 Mar 2012 21:24:02 +0000">
		<buginformation>
			<summary>Signed pdf parsing</summary>
			<description>&lt;p&gt;Is there an estimated date for implementing default parsing for signed documents, like signed pdf files (pk7s format), for example?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 2 Mar 2015 04:24:05 +0000" id="408" opendate="Fri, 27 Apr 2012 16:37:00 +0000">
		<buginformation>
			<summary>NPE thrown with password protected Pages file</summary>
			<description>&lt;p&gt;When trying to view a password-protected Pages file in Tika GUI, you get an NPE:&lt;/p&gt;

&lt;p&gt;org.apache.tika.exception.TikaException: Unexpected RuntimeException from org.apache.tika.parser.iwork.IWorkPackageParser@30583058&lt;br/&gt;
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:244)&lt;br/&gt;
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)&lt;br/&gt;
	at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)&lt;br/&gt;
	at org.apache.tika.gui.TikaGUI.handleStream(TikaGUI.java:320)&lt;br/&gt;
	at org.apache.tika.gui.TikaGUI.openFile(TikaGUI.java:279)&lt;br/&gt;
	at org.apache.tika.gui.ParsingTransferHandler.importFiles(ParsingTransferHandler.java:94)&lt;br/&gt;
	at org.apache.tika.gui.ParsingTransferHandler.importData(ParsingTransferHandler.java:77)&lt;br/&gt;
	at javax.swing.TransferHandler.importData(TransferHandler.java:756)&lt;br/&gt;
	at javax.swing.TransferHandler$DropHandler.drop(TransferHandler.java:1479)&lt;br/&gt;
	at java.awt.dnd.DropTarget.drop(DropTarget.java:445)&lt;br/&gt;
	at javax.swing.TransferHandler$SwingDropTarget.drop(TransferHandler.java:1204)&lt;br/&gt;
	at sun.awt.dnd.SunDropTargetContextPeer.processDropMessage(SunDropTargetContextPeer.java:531)&lt;br/&gt;
	at sun.awt.dnd.SunDropTargetContextPeer$EventDispatcher.dispatchDropEvent(SunDropTargetContextPeer.java:844)&lt;br/&gt;
	at sun.awt.dnd.SunDropTargetContextPeer$EventDispatcher.dispatchEvent(SunDropTargetContextPeer.java:768)&lt;br/&gt;
	at sun.awt.dnd.SunDropTargetEvent.dispatch(SunDropTargetEvent.java:42)&lt;br/&gt;
	at java.awt.Component.dispatchEventImpl(Component.java:4498)&lt;br/&gt;
	at java.awt.Container.dispatchEventImpl(Container.java:2110)&lt;br/&gt;
	at java.awt.Component.dispatchEvent(Component.java:4471)&lt;br/&gt;
	at java.awt.LightweightDispatcher.retargetMouseEvent(Container.java:4588)&lt;br/&gt;
	at java.awt.LightweightDispatcher.processDropTargetEvent(Container.java:4323)&lt;br/&gt;
	at java.awt.LightweightDispatcher.dispatchEvent(Container.java:4174)&lt;br/&gt;
	at java.awt.Container.dispatchEventImpl(Container.java:2096)&lt;br/&gt;
	at java.awt.Window.dispatchEventImpl(Window.java:2490)&lt;br/&gt;
	at java.awt.Component.dispatchEvent(Component.java:4471)&lt;br/&gt;
	at java.awt.EventQueue.dispatchEvent(EventQueue.java:610)&lt;br/&gt;
	at java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:280)&lt;br/&gt;
	at java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:195)&lt;br/&gt;
	at java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:185)&lt;br/&gt;
	at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:180)&lt;br/&gt;
	at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:172)&lt;br/&gt;
	at java.awt.EventDispatchThread.run(EventDispatchThread.java:133)&lt;br/&gt;
Caused by: java.lang.NullPointerException&lt;br/&gt;
	at org.apache.tika.parser.iwork.IWorkPackageParser$IWORKDocumentType.detectType(IWorkPackageParser.java:125)&lt;br/&gt;
	at org.apache.tika.parser.iwork.IWorkPackageParser$IWORKDocumentType.access$000(IWorkPackageParser.java:71)&lt;br/&gt;
	at org.apache.tika.parser.iwork.IWorkPackageParser.parse(IWorkPackageParser.java:166)&lt;br/&gt;
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)&lt;br/&gt;
	... 30 more&lt;/p&gt;

&lt;p&gt;I tried viewing the contents in 7-zip, but it tells me it can't understand the compression format.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 18 May 2012 09:53:08 +0000" id="409" opendate="Fri, 27 Apr 2012 16:46:41 +0000">
		<buginformation>
			<summary>Pages documents created in Layout mode not supported</summary>
			<description>&lt;p&gt;Pages supports Layout editing mode, which provides free-form editing as opposed to standard line-by-line word processing. You use text boxes and other embedded objects to add content. Tika only extracts metadata from these documents, not the actual content.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 8 Jul 2012 22:50:07 +0000" id="410" opendate="Fri, 27 Apr 2012 17:09:07 +0000">
		<buginformation>
			<summary>Headers, footers, and footnotes not extracted from Pages documents</summary>
			<description>&lt;p&gt;Tika does not extract anything from the header or footer area and also does not extract footnotes.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/iwork/AutoPageNumberUtilsTest.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/iwork/AutoPageNumberUtils.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/iwork/PagesContentHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 27 Apr 2012 23:56:45 +0000" id="411" opendate="Fri, 27 Apr 2012 17:25:39 +0000">
		<buginformation>
			<summary>Comments embedded in Pages documents not supported</summary>
			<description>&lt;p&gt;Comments added to a Pages document are not extracted. This also applies to documents annotated on iWork.com.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/iwork/PagesContentHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 18 May 2012 09:53:42 +0000" id="412" opendate="Tue, 1 May 2012 19:45:20 +0000">
		<buginformation>
			<summary>Text contained in text boxes or shapes in Keynote docs runs together</summary>
			<description>&lt;p&gt;Tika grabs the text in the various boxes/shapes and combines it into one word.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 16 May 2012 22:05:50 +0000" id="413" opendate="Tue, 15 May 2012 19:23:13 +0000">
		<buginformation>
			<summary>iWork Charts not being parsed in all products (Pages, Numbers, Keynote)</summary>
			<description>&lt;p&gt;Charts titles, axis', and other textual information is all being ignored by the TIKA parser.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 11 Sep 2012 15:03:06 +0000" id="414" opendate="Tue, 15 May 2012 19:25:57 +0000">
		<buginformation>
			<summary>iWork Numbers sheetnames not being parsed into metadata</summary>
			<description>&lt;p&gt;iWork Number's have individual sheetnames.  Currently only the first sheet is going extracted.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 25 May 2012 18:14:12 +0000" id="415" opendate="Tue, 15 May 2012 19:46:05 +0000">
		<buginformation>
			<summary>iWork keynote content on master slides are not being parsed </summary>
			<description>&lt;p&gt;iWork Keynote slides can contain tables, however these are being dropped entirely by the Tika parser.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/iwork/IWorkParserTest.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 21 May 2012 18:03:17 +0000" id="416" opendate="Tue, 15 May 2012 20:13:17 +0000">
		<buginformation>
			<summary>iWork number table titles not being parsed</summary>
			<description>&lt;p&gt;iWork numbers table titles are being ignored by TIKA parser.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 14 Dec 2012 03:09:35 +0000" id="417" opendate="Thu, 13 Dec 2012 15:24:52 +0000">
		<buginformation>
			<summary>Can't parse Word files with no format set</summary>
			<description>&lt;p&gt;When we were using Solr for indexing we came over this Tika bug.&lt;br/&gt;
While parsing a doc or docx file that contains text without any format set (format inside Microsoft Word) the parser will throw exceptions.&lt;br/&gt;
By setting a format to the text the file can be correctly parsed without unexpected errors.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/WordExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 14 Dec 2018 22:25:14 +0000" id="418" opendate="Wed, 26 Dec 2018 04:25:05 +0000">
		<buginformation>
			<summary>Tika includes 2 vulnerable components</summary>
			<description>&lt;p&gt;Maven audit plugin reports 2 vulnerable components:&lt;/p&gt;

&lt;p&gt;com.google.guava:guava:jar:17.0:compile&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;CVE-2018-10237&amp;#93;&lt;/span&gt; Deserialization of Untrusted Data (5.9); &lt;a href=&quot;https://ossindex.sonatype.org/vuln/24585a7f-eb6b-4d8d-a2a9-a6f16cc7c1d0&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://ossindex.sonatype.org/vuln/24585a7f-eb6b-4d8d-a2a9-a6f16cc7c1d0&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;com.google.protobuf:protobuf-java:jar:2.5.0:compile&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;CVE-2015-5237&amp;#93;&lt;/span&gt; Improper Restriction of Operations within the Bounds of a Memory Buffer (8.8); &lt;a href=&quot;https://ossindex.sonatype.org/vuln/d47d20ab-eb2a-4cfd-8064-bbf6283649cb&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://ossindex.sonatype.org/vuln/d47d20ab-eb2a-4cfd-8064-bbf6283649cb&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Maybe it worth to add &lt;tt&gt;audit&lt;/tt&gt; plugin to the build/release?&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;mvn org.sonatype.ossindex.maven:ossindex-maven-plugin:audit -f pom.xml&lt;/tt&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-dl/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 7 Jan 2019 16:08:43 +0000" id="419" opendate="Mon, 7 Jan 2019 12:42:05 +0000">
		<buginformation>
			<summary>.docx text extract leaves out rich text content-control inside of a text box</summary>
			<description>&lt;p&gt;When parsing a Microsoft Word .docx, Rich Text Content Control nested inside of a Text Box remain unextracted.&lt;/p&gt;

&lt;p&gt;I have attached a .docx file that can be tested against. &lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;&quot;&lt;em&gt;rich-text-content-control_inside-text-box&lt;/em&gt;&quot; remains unextracted while &quot;rich-text-content-control &quot; and &quot;&lt;em&gt;simple text&lt;/em&gt;&quot; are extracted without any problem. ** &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 7 Jan 2019 16:08:43 +0000" id="420" opendate="Mon, 7 Jan 2019 17:22:36 +0000">
		<buginformation>
			<summary>Skip h2 1.4.197 in ossindex-maven-plugin in tika-eval </summary>
			<description>&lt;p&gt;The build is now failing because of two recently indexed vulnerabilities in h2 1.4.197, which is used by tika-eval.  In reviewing at least one of the cves (CVE-2018-10054), it looks like versions before 1.4.197 are also vulnerable (unless &quot;create alias&quot; wasn't added until 1.4.197...which I doubt).  There is no actual &quot;fix version&quot; available, afaict.  For now, let's skip h2.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-eval/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 7 Mar 2018 20:39:11 +0000" id="421" opendate="Thu, 14 Dec 2017 08:21:03 +0000">
		<buginformation>
			<summary>Typos in tika-mimetypes.xml</summary>
			<description>&lt;p&gt;Are these mimetypes in tika-mimetypes.xml&lt;/p&gt;


&lt;p&gt;audio/x-adbcm instead audio/x-adpcm&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-xml&quot;&gt; &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;mime-type type=&lt;span class=&quot;code-quote&quot;&gt;&quot;audio/x-adbcm&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;p&gt;audio/x-dec-adbcm  instead audio/x-dec-adpcm&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-xml&quot;&gt; &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;mime-type type=&lt;span class=&quot;code-quote&quot;&gt;&quot;audio/x-dec-adbcm&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;intended?&lt;br/&gt;
Couldn't find these mimetypes.&lt;/p&gt;

&lt;p&gt;Regards&lt;/p&gt;

&lt;p&gt;Andreas&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 6 Sep 2018 15:34:26 +0000" id="422" opendate="Tue, 23 Jan 2018 19:40:31 +0000">
		<buginformation>
			<summary>Upgrade to POI 4.0.0 when available</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/tika-batch/src/test/java/org/apache/tika/batch/fs/BatchDriverTest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 1 May 2018 17:37:41 +0000" id="423" opendate="Tue, 24 Apr 2018 04:19:53 +0000">
		<buginformation>
			<summary>ENVI Header metadata fields can span more than one line</summary>
			<description>&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tpalsulich&quot; class=&quot;user-hover&quot; rel=&quot;tpalsulich&quot;&gt;Tyler Palsulich&lt;/a&gt; was correct when &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1357?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&amp;amp;focusedCommentId=14046140#comment-14046140&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;he stated&lt;/a&gt; &quot;...See below for how to read and output line by line (copy &amp;amp; paste between the xml start/end in EnviHeaderParser). I have a hunch this isn't really what we want &amp;#8211; what if a metadata field has a newline in it? What if the line is too long to fit into a string? On the other hand, with nice input, it's much nicer output.&quot;&lt;/p&gt;

&lt;p&gt;As it turns out ENVI header metadata fields can span more than one line. An example is as follows&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
1.    ENVI
2.    description = {
3.      Georeferenced Image built from input GLT. [Wed Jun 10 04:37:54 2015] [Wed
4.      Jun 10 04:48:52 2015]}
5.    samples = 739
6.    lines = 14674
7.    bands = 432
8.    header offset = 0
9.    file type = ENVI Standard
10.    data type = 4
11.    interleave = bil
12.    sensor type = Unknown
13.    &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt; order = 0
14.    map info = { UTM , 1.000 , 1.000 , 724522.127 , 4074620.759 , 1.1000000000e+00 , 1.1000000000e+00 , 12 , North , WGS-84 , units=Meters , rotation=75.00000000 }
15.    wavelength units = Nanometers
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The case here is when a metadata field value is contained within curly brackets. The examples above are clearly L2-L4 where the value is spread over three lines and L14 where the value is contained within the one line.&lt;/p&gt;

&lt;p&gt;This requires a patch to fix the &lt;a href=&quot;https://github.com/apache/tika/blob/9130bbc1fa6d69419b2ad294917260d6b1cced08/tika-parsers/src/main/java/org/apache/tika/parser/envi/EnviHeaderParser.java&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;EnviHeaderParser&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/output.txt</file>
			<file>/tika-parsers/src/test/java/org/apache/tika/config/TikaEncodingDetectorTest.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/envi/EnviHeaderParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 24 Sep 2018 14:20:23 +0000" id="424" opendate="Wed, 2 May 2018 02:03:06 +0000">
		<buginformation>
			<summary>Update freedesktop.org shared-mime-info-spec hyperlink in MimeTypesReader.java</summary>
			<description>&lt;p&gt;When attempting to upgrade TIKA over in OODT, I noticed that the hyerlink for the &lt;a href=&quot;https://freedesktop.org/wiki/Specifications/shared-mime-info-spec/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://freedesktop.org/wiki/Specifications/shared-mime-info-spec/&lt;/a&gt; is broken in MimeTypesReader.java.&lt;br/&gt;
This issue will simply update the hyperlink.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/mime/MimeTypesReader.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 17 May 2018 20:17:12 +0000" id="425" opendate="Thu, 17 May 2018 12:12:42 +0000">
		<buginformation>
			<summary>Improve RecursiveParserWrapper API</summary>
			<description>&lt;p&gt;The RecursiveParserWrapper stores the data in the wrapper, which makes it not thread safe, and, um, different from the other parsers, API-wise.&lt;/p&gt;

&lt;p&gt;Let's create a RecursiveParserWrapperHandler that puts more of the handling in the handler.  &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 23 May 2018 00:14:19 +0000" id="426" opendate="Fri, 18 May 2018 12:14:37 +0000">
		<buginformation>
			<summary>Reuse SAXParsers where possible</summary>
			<description>&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=wastl-nagel&quot; class=&quot;user-hover&quot; rel=&quot;wastl-nagel&quot;&gt;Sebastian Nagel&lt;/a&gt; pointed out on the &lt;a href=&quot;https://lists.apache.org/thread.html/54913f29cd83bba175f77a2d6a4902bb3a5cba2fa495bbfd6012024a@%3Cuser.tika.apache.org%3E&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;user list&lt;/a&gt; that there can be thread contention when creating SAXParsers.  &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jukkaz&quot; class=&quot;user-hover&quot; rel=&quot;jukkaz&quot;&gt;Jukka Zitting&lt;/a&gt; recommended pooling SAXParsers.&lt;/p&gt;

&lt;p&gt;I noticed a modest improvement in speed when parsing docx files with our SAX based parser, and I suspect that &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=wastl-nagel&quot; class=&quot;user-hover&quot; rel=&quot;wastl-nagel&quot;&gt;Sebastian Nagel&lt;/a&gt; will find an even greater improvement during detection because we're currently creating a new SAXParser on every call to detect an XML root.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/pom.xml</file>
			<file>/CHANGES.txt</file>
			<file>/tika-core/src/main/java/org/apache/tika/parser/NetworkParser.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/parser/ParseContext.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/detect/XmlRootExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 7 Jun 2018 14:53:45 +0000" id="427" opendate="Tue, 22 May 2018 14:47:54 +0000">
		<buginformation>
			<summary>mime detection based on resource name detects resources as &quot;text/x-php&quot; instead of &quot;text/html&quot; </summary>
			<description>&lt;p&gt;When using tika to detect a mime type given only an URL containing &quot;.php&quot; and a content-type hint of &quot;text/html&quot;, it guesses &quot;text/x-php&quot;, whereas one could expect &quot;text/html&quot;.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
TikaConfig tika = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; TikaConfig();
Metadata metadata = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Metadata();
&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; url = &lt;span class=&quot;code-quote&quot;&gt;&quot;https:&lt;span class=&quot;code-comment&quot;&gt;//www.facebook.com/home.php&quot;&lt;/span&gt;;
&lt;/span&gt;metadata.set(Metadata.RESOURCE_NAME_KEY, url);
metadata.set(Metadata.CONTENT_TYPE, &lt;span class=&quot;code-quote&quot;&gt;&quot;text/html&quot;&lt;/span&gt;);
MediaType type = tika.getDetector().detect(&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;, metadata);
&lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.out.println(url + &lt;span class=&quot;code-quote&quot;&gt;&quot; is of type &quot;&lt;/span&gt; + type.toString());
&lt;span class=&quot;code-comment&quot;&gt;// Prints https://www.facebook.com/home.php is of type text/x-php&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/mime/MimeType.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 31 May 2018 20:13:26 +0000" id="428" opendate="Fri, 25 May 2018 15:56:38 +0000">
		<buginformation>
			<summary>Allow users to specify a directory of jars for classloading in ForkParser</summary>
			<description>&lt;p&gt;The ForkParser now builds the parser in the parent process and serializes it to the child process.  It would be neat to make it easier for users of the ForkParser to depend solely on tika-core and put all of our dependency nastiness in a separate directory that will be used by the the fork server (child process) to build the underlying parser.&lt;/p&gt;

&lt;p&gt;This would allow, e.g. Solr, to point to a directory with the tika-app.jar and remove all of our dependencies (except tika-core) from their dependencies. &lt;/p&gt;

&lt;p&gt;I propose that we allow users to initialize ForkParser with a Path that contains all the jars necessary to build the Parser, and, optionally, a ParserFactory.&lt;/p&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/fork/ForkClient.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 31 May 2018 20:13:49 +0000" id="429" opendate="Tue, 29 May 2018 21:23:56 +0000">
		<buginformation>
			<summary>Allow the RecursiveParserWrapper to work with the ForkParser</summary>
			<description>&lt;p&gt;It doesn’t. It should.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/fork/ForkClient.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 1 Jun 2018 13:47:08 +0000" id="430" opendate="Thu, 31 May 2018 20:15:42 +0000">
		<buginformation>
			<summary>Allow users to specify timeout for parsing and/or waiting in ForkParser</summary>
			<description>&lt;p&gt;We currently have a &lt;tt&gt;pulse&lt;/tt&gt; in the ForkServer.  This roughly allows for timeouts, but, depending on the timing, the parser could run for nearly 2x as long as the &lt;tt&gt;pulse&lt;/tt&gt;.  The other downside to our current reliance on &lt;tt&gt;pulse&lt;/tt&gt; is that a parser could theoretically go forever as long as it outputs a byte within the &lt;tt&gt;pulse&lt;/tt&gt; window.&lt;/p&gt;

&lt;p&gt;Let's let users explicitly declare a max parse time, and a max wait time – how long should the server remain alive while waiting for a parse request.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 1 Jun 2018 14:15:23 +0000" id="431" opendate="Fri, 1 Jun 2018 10:48:46 +0000">
		<buginformation>
			<summary>Add System.exit() and heavy gc hang to MockParser</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/tika-batch/src/test/java/org/apache/tika/batch/fs/BatchDriverTest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 5 Sep 2018 20:00:59 +0000" id="432" opendate="Fri, 1 Jun 2018 14:05:56 +0000">
		<buginformation>
			<summary>Add magic numbers of Olympus ORF Files</summary>
			<description>&lt;p&gt;For Olympus Raw Files (.ORF) there aren't any magic numbers specified in the tika-mimetypes.xml file. When I adjust the xml to look like following it works:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&amp;lt;mime-type type=&lt;span class=&quot;code-quote&quot;&gt;&quot;image/x-raw-olympus&quot;&lt;/span&gt;&amp;gt;
  &amp;lt;_comment&amp;gt;Olympus raw image&amp;lt;/_comment&amp;gt;
  &amp;lt;magic priority=&lt;span class=&quot;code-quote&quot;&gt;&quot;50&quot;&lt;/span&gt;&amp;gt;
    &amp;lt;match offset=&lt;span class=&quot;code-quote&quot;&gt;&quot;0&quot;&lt;/span&gt; type=&lt;span class=&quot;code-quote&quot;&gt;&quot;string&quot;&lt;/span&gt; value=&lt;span class=&quot;code-quote&quot;&gt;&quot;\x49\x49\x52\x4F&quot;&lt;/span&gt;/&amp;gt;
  &amp;lt;/magic&amp;gt;
  &amp;lt;glob pattern=&lt;span class=&quot;code-quote&quot;&gt;&quot;*.orf&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;/mime-type&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;I tested this with files from my Olympus E-PL7, my Olympus E-M5 Mark II and some Olympus E-M1 Mark II raw files I found on the internet (the most recent camera). The content type changes from octet stream to image/x-raw-olympus as expected.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 5 Sep 2018 20:00:59 +0000" id="433" opendate="Fri, 1 Jun 2018 14:19:22 +0000">
		<buginformation>
			<summary>Add a parameter to ForkParser to close a client after it has processed x files</summary>
			<description>&lt;p&gt;If a parser has a slowly growing memory leak, it would be helpful to close out the client/server and restart it after &lt;tt&gt;X&lt;/tt&gt; number of files.  We can leave the default to -1, which means never restart based on number of files handled.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/fork/ForkClient.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 5 Sep 2018 20:00:59 +0000" id="434" opendate="Mon, 4 Jun 2018 13:17:18 +0000">
		<buginformation>
			<summary>Prep Tika for Java 10</summary>
			<description>&lt;p&gt;At the very least, we need to fix some split packages...some of which are my fault...sorry!&lt;/p&gt;

&lt;p&gt;I'm guessing we can add this to the todo list for 2.0.0?&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-dl/pom.xml</file>
			<file>/tika-bundle/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 7 Jun 2018 20:20:34 +0000" id="435" opendate="Mon, 4 Jun 2018 14:40:29 +0000">
		<buginformation>
			<summary>Upgrade commons-compress to 1.17</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parent/pom.xml</file>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 7 Jun 2018 20:25:26 +0000" id="436" opendate="Thu, 7 Jun 2018 18:25:46 +0000">
		<buginformation>
			<summary>Add a streaming out option for the Json serialization</summary>
			<description>&lt;p&gt;Depending on the configuration of the ForkParser, it might be useful for that and also for tika-batch to write out each embedded file once the parse for that embedded file has completed, rather than caching the entire output in memory.&lt;/p&gt;

&lt;p&gt;The downside to this is that the main document will now show up at the bottom of the list of metadata objects.  We can re-arrange when we deserialize, but anyone not using our deserialization will see this change in order.  Given that this is a breaking change, I'll make it optional.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/main/resources/tika-app-batch-config.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 14 Aug 2018 15:53:00 +0000" id="437" opendate="Wed, 13 Jun 2018 20:06:54 +0000">
		<buginformation>
			<summary>Upgrade jmatio to 1.4 </summary>
			<description>&lt;p&gt;jmatio 1.3 includes an upgrade to clean MappedByteBuffers in Java 8-&amp;gt;11-ea, thanks to a copy/paste from Lucene.&lt;/p&gt;

&lt;p&gt;jmatio 1.4 will include one that actually works. Thank you, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=thetaphi&quot; class=&quot;user-hover&quot; rel=&quot;thetaphi&quot;&gt;Uwe Schindler&lt;/a&gt;!&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 14 Jun 2018 20:32:52 +0000" id="438" opendate="Thu, 14 Jun 2018 14:43:43 +0000">
		<buginformation>
			<summary>Fix 'can't overwrite cause' exception in TaggedSAXException in Java 11-ea</summary>
			<description>&lt;p&gt;&lt;a href=&quot;https://github.com/elastic/elasticsearch/issues/31305&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/elastic/elasticsearch/issues/31305&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I can reproduce this with Java 11-ea in elastic's test harness and in a standalone unit test within tika-core.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/sax/TaggedSAXException.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 2 Jul 2018 19:21:39 +0000" id="439" opendate="Thu, 14 Jun 2018 19:00:18 +0000">
		<buginformation>
			<summary>Tika JAX-RS PDF parser option / custom config issue</summary>
			<description>&lt;p&gt;PDF parsing using a config file behaves differently in Tika app than in Tika server. Tika server reads the custom config file, but the PDF parsing options are not being set. &lt;/p&gt;

&lt;p&gt;Here is an excerpt of output from the app:&lt;/p&gt;

&lt;p&gt;&amp;lt;p&amp;gt;WINS No: B29017 APACHE 27-38 UNIT 1H Date: 5/4/2017&lt;/p&gt;

&lt;p&gt;&amp;lt;/p&amp;gt;&lt;/p&gt;

&lt;p&gt;&amp;lt;p&amp;gt;AFE No: 1704555 Daily Completion and Workover Report DOL: &lt;/p&gt;

&lt;p&gt;&amp;lt;/p&amp;gt;&lt;/p&gt;

&lt;p&gt;However, with the same configuration file the output from tika server is:&lt;/p&gt;

&lt;p&gt;&amp;lt;p&amp;gt;Daily Completion and Workover Report&lt;/p&gt;

&lt;p&gt;&amp;lt;/p&amp;gt;&lt;/p&gt;

&lt;p&gt;&amp;lt;p&amp;gt;WINS No: &lt;/p&gt;

&lt;p&gt;&amp;lt;/p&amp;gt;&lt;/p&gt;

&lt;p&gt;&amp;lt;p&amp;gt;AFE No: &lt;/p&gt;

&lt;p&gt;&amp;lt;/p&amp;gt;&lt;/p&gt;

&lt;p&gt;&amp;lt;p&amp;gt;Date: &lt;/p&gt;

&lt;p&gt;&amp;lt;/p&amp;gt;&lt;/p&gt;

&lt;p&gt;&amp;lt;p&amp;gt;DOL: &lt;/p&gt;

&lt;p&gt;&amp;lt;/p&amp;gt;&lt;/p&gt;

&lt;p&gt;&amp;lt;p&amp;gt;APACHE 27-38 UNIT B29017&lt;/p&gt;

&lt;p&gt;&amp;lt;/p&amp;gt;&lt;/p&gt;

&lt;p&gt;&amp;lt;p&amp;gt;1704555&lt;/p&gt;

&lt;p&gt;&amp;lt;/p&amp;gt;&lt;/p&gt;

&lt;p&gt;&amp;lt;p&amp;gt;5/4/2017&lt;/p&gt;

&lt;p&gt;&amp;lt;/p&amp;gt;&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;The tika config is:&lt;/p&gt;

&lt;p&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;&lt;br/&gt;
&amp;lt;properties&amp;gt;&lt;br/&gt;
 &amp;lt;parsers&amp;gt;&lt;br/&gt;
 &amp;lt;parser class=&quot;org.apache.tika.parser.pdf.PDFParser&quot;&amp;gt;&lt;br/&gt;
 &amp;lt;params&amp;gt;&lt;br/&gt;
 &amp;lt;param name=&quot;sortByPosition&quot; type=&quot;bool&quot;&amp;gt;true&amp;lt;/param&amp;gt;&lt;br/&gt;
 &amp;lt;/params&amp;gt;&lt;br/&gt;
 &amp;lt;/parser&amp;gt;&lt;br/&gt;
 &amp;lt;/parsers&amp;gt;&lt;br/&gt;
&amp;lt;/properties&amp;gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-server/src/main/java/org/apache/tika/server/resource/TikaResource.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 2 Jul 2018 19:21:39 +0000" id="440" opendate="Fri, 15 Jun 2018 13:39:45 +0000">
		<buginformation>
			<summary>Allow the build to go on even if models aren't available</summary>
			<description>&lt;p&gt;sourceforge.net was recently down for maintenance, and ModelGetter.groovy caused the build to fail with an IOException.  Let's add a try/catch block so that if there's a network problem grabbing the models, the build can go on.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/resources/org/apache/tika/parser/ner/opennlp/ModelGetter.groovy</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 14 Aug 2018 15:52:33 +0000" id="441" opendate="Fri, 15 Jun 2018 20:02:31 +0000">
		<buginformation>
			<summary>Upgrade dl4j to 1.0.0-beta2</summary>
			<description>&lt;p&gt;Let's try to upgrade dl4j.  I think I got us most of the way there, but I got this error when reading the json config file.  Can someone with more knowledge of layer specs help (&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=thammegowda&quot; class=&quot;user-hover&quot; rel=&quot;thammegowda&quot;&gt;Thamme Gowda&lt;/a&gt;, perhaps &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;)?&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.deeplearning4j.exception.DL4JInvalidConfigException: Invalid configuration for layer (idx=-1, name=convolution2d_2, type=ConvolutionLayer) for width dimension:  Invalid input configuration for kernel width. Require 0 &amp;lt; kW &amp;lt;= inWidth + 2*padW; got (kW=3, inWidth=1, padW=0)
Input type = InputTypeConvolutional(h=149,w=1,c=32), kernel = [3, 3], strides = [1, 1], padding = [0, 0], layer size (output channels) = 32, convolution mode = Truncate
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-dl/pom.xml</file>
			<file>/tika-dl/src/main/java/org/apache/tika/dl/imagerec/DL4JInceptionV3Net.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 3 Aug 2018 15:37:44 +0000" id="442" opendate="Tue, 19 Jun 2018 15:35:57 +0000">
		<buginformation>
			<summary>HtmlEncodingDetector doesn't follow the specification</summary>
			<description>&lt;p&gt;This bug is linked to &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-2671&quot; title=&quot;HtmlEncodingDetector doesnt take provided metadata into account&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-2671&quot;&gt;TIKA-2671&lt;/a&gt;, but does not concern metadata, but rather the bytes-based detection itself.&lt;/p&gt;

&lt;p&gt;While reading the specification, I collected a list of sample cases where HtmlEncodingDetector differs from the specification, and thus fails at detecting the right charset.&lt;/p&gt;

&lt;p&gt;I am attaching the test cases to this issue: &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/html/charsetdetector/FullStandardEncodingDetector.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/html/charsetdetector/PreScanner.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/html/charsetdetector/CharsetAliases.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/html/HtmlEncodingDetector.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/html/StrictHtmlEncodingDetector.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/html/charsetdetector/CharsetDetectionResult.java</file>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/html/StandardHtmlEncodingDetectorTest.java</file>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/html/HtmlEncodingDetectorTest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 6 Jul 2018 13:23:25 +0000" id="443" opendate="Wed, 20 Jun 2018 15:04:42 +0000">
		<buginformation>
			<summary>OpenDocumentParser should fail on invalid zip files</summary>
			<description>&lt;p&gt;The OpenDocumentParser assumes a zip file as container. However, if it is called on an invalid zip stream from a remote URL (see &lt;a href=&quot;https://issues.apache.org/jira/browse/NUTCH-2603&quot; title=&quot;Bring back legacy pre-Tika parsers and use them as back up parsers&quot; class=&quot;issue-link&quot; data-issue-key=&quot;NUTCH-2603&quot;&gt;NUTCH-2603&lt;/a&gt;), the parser signals success and returns a document with no/empty content. The behavior is different when called on a local file: while the &lt;a href=&quot;https://docs.oracle.com/javase/8/docs/api/java/util/zip/ZipFile.html#ZipFile-java.io.File-&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;constructor of ZipFile&lt;/a&gt; fails on invalid input, the &lt;a href=&quot;https://docs.oracle.com/javase/8/docs/api/java/util/zip/ZipInputStream.html#ZipInputStream-java.io.InputStream-&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;constructor of ZipInputStream&lt;/a&gt; silently ignores the input.&lt;/p&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/odf/OpenDocumentParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 21 Jun 2018 20:15:56 +0000" id="444" opendate="Wed, 20 Jun 2018 19:35:21 +0000">
		<buginformation>
			<summary>ConcurrentModificationException in org.apache.tika.mime.MediaTypeRegistry.getAliases</summary>
			<description>&lt;p&gt;When using Tika parser on multiple threads and calling MediaTypeRegistry.getAliases, we get ConcurrentModificationException:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;java.util.ConcurrentModificationException
 at java.util.HashMap$HashIterator.nextNode(HashMap.java:1429)
 at java.util.HashMap$EntryIterator.next(HashMap.java:1463)
 at java.util.HashMap$EntryIterator.next(HashMap.java:1461)
 at org.apache.tika.mime.MediaTypeRegistry.getAliases(MediaTypeRegistry.java:78)&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;It will go away, if we use ConcurrentMap here:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/tika/blob/master/tika-core/src/main/java/org/apache/tika/mime/MediaTypeRegistry.java#L49&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/tika/blob/master/tika-core/src/main/java/org/apache/tika/mime/MediaTypeRegistry.java#L49&lt;/a&gt;&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;Here is code snippet to recreate the problem:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; stop = &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;;
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; void main(&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;[] args) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; MimeTypeException {
 MimeTypes mimeTypes = MimeTypes.getDefaultMimeTypes();
 Executors.newSingleThreadExecutor().execute(() -&amp;gt; {
 &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
 &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; i=0; i &amp;lt; 1000 &amp;amp;&amp;amp; !stop; i++) {
 &lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.out.println(mimeTypes.forName(&lt;span class=&quot;code-quote&quot;&gt;&quot;abc&quot;&lt;/span&gt; + i + &lt;span class=&quot;code-quote&quot;&gt;&quot;/abc&quot;&lt;/span&gt;));
 }
 } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (MimeTypeException e) {
 e.printStackTrace();
 }
 });
 &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; i=0; i &amp;lt; 1000 &amp;amp;&amp;amp; !stop; i++) {
 &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
 &lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.out.println(mimeTypes.getMediaTypeRegistry().getAliases(MediaType.APPLICATION_ZIP));
 } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (ConcurrentModificationException ex) {
 stop = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;;
 ex.printStackTrace();
 }
 }
}&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt; &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/mime/MediaTypeRegistry.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 2 Jul 2018 15:28:19 +0000" id="445" opendate="Mon, 2 Jul 2018 14:44:56 +0000">
		<buginformation>
			<summary>Upgrade jempbox to 1.8.15</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 13 Jul 2018 21:24:29 +0000" id="446" opendate="Fri, 13 Jul 2018 20:14:42 +0000">
		<buginformation>
			<summary>Avoid potential to overwrite attachments</summary>
			<description>&lt;p&gt;In a few places in our code base, when we extract attachments and write them out as files, we aren't correctly handling the chance that two embedded files could have the same name.  Let's fix this.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/test/java/org/apache/tika/cli/TikaCLITest.java</file>
			<file>/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 25 Jul 2018 19:11:52 +0000" id="447" opendate="Tue, 17 Jul 2018 22:08:37 +0000">
		<buginformation>
			<summary>MBOX not recognized when unknown X-headers are present</summary>
			<description>&lt;p&gt;This is a spin off from &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-2578&quot; title=&quot;Mails not recognized when unknown X-headers are present&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-2578&quot;&gt;&lt;del&gt;TIKA-2578&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I have mbox files that are not being recognized as such because they have X- headers at the top.&lt;/p&gt;

&lt;p&gt;Current config:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;  &amp;lt;mime-type type=&quot;application/mbox&quot;&amp;gt;
    &amp;lt;!-- MBOX files start with &quot;From [sender] [date]&quot; --&amp;gt;
    &amp;lt;!-- To avoid false matches, check for other headers after that --&amp;gt;
    &amp;lt;magic priority=&quot;70&quot;&amp;gt;
      &amp;lt;match value=&quot;From &quot; type=&quot;string&quot; offset=&quot;0&quot;&amp;gt;
         &amp;lt;match value=&quot;\nFrom: &quot; type=&quot;string&quot; offset=&quot;32:256&quot;/&amp;gt;
         &amp;lt;match value=&quot;\nDate: &quot; type=&quot;string&quot; offset=&quot;32:256&quot;/&amp;gt;
         &amp;lt;match value=&quot;\nSubject: &quot; type=&quot;string&quot; offset=&quot;32:256&quot;/&amp;gt;
         &amp;lt;match value=&quot;\nDelivered-To: &quot; type=&quot;string&quot; offset=&quot;32:256&quot;/&amp;gt;
         &amp;lt;match value=&quot;\nReceived: by &quot; type=&quot;string&quot; offset=&quot;32:256&quot;/&amp;gt;
         &amp;lt;match value=&quot;\nReceived: via &quot; type=&quot;string&quot; offset=&quot;32:256&quot;/&amp;gt;
         &amp;lt;match value=&quot;\nReceived: from &quot; type=&quot;string&quot; offset=&quot;32:256&quot;/&amp;gt;
         &amp;lt;match value=&quot;\nMime-Version: &quot; type=&quot;string&quot; offset=&quot;32:256&quot;/&amp;gt;
      &amp;lt;/match&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;mbox file:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;From &quot;naveen.andrews@enron.com&quot; Wed Jan 30 18:07:01 2002
X-EDO-Dataset: EnronData.org Abridged Email Dataset (AED)
X-EDO-AED-Version: 1.0
X-EDO-AED-License: Creative Commons Attribution 3.0 United States;
 http://creativecommons.org/licenses/by/3.0/us/;
 To provide attribution, please cite to &quot;EnronData.org.&quot;
X-EDO-AED-ID: 516172
X-EDO-AED-File: zipper-a/inbox/38.eml
Message-ID: &amp;lt;8269158.1075842014924.JavaMail.evans@thyme&amp;gt;
Date: Wed, 30 Jan 2002 15:07:01 -0800 (PST)
From: naveen.andrews@enron.com
To: andy.zipper@enron.com
Subject: RE: Var simulation
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;MBOX rule looks for additional headers only in the first 256 bytes, which is not enough when X- headers are present.&lt;/p&gt;

&lt;p&gt;Side-note: prior to 1.17 such mbox was detected as text/plain. As of 1.17 it is detected as message/rfc822 (due to &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-2594&quot; title=&quot;Mail detected as application/xhtml+xml&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-2594&quot;&gt;&lt;del&gt;TIKA-2594&lt;/del&gt;&lt;/a&gt; that added a rule for Message-ID being present in the first 1000 bytes). Neither is correct!&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 25 Jul 2018 18:29:15 +0000" id="448" opendate="Thu, 19 Jul 2018 13:05:54 +0000">
		<buginformation>
			<summary>Can't create a RPM </summary>
			<description>&lt;p&gt;When I'm trying to generate RPM using the latest released Tika I get this error:&lt;br/&gt;
 Unable to get dependency information: Unable to read the metadata file for artifact 'com.github.jai-imageio:jai-imageio-core:jar': Invalid JDK version in profile 'java8-and-higher': Unbounded range: [1.8, for project com.github.jai-imageio:jai-imageio-core&lt;br/&gt;
 This error is from this internal Tika's dependency that have an issue&lt;/p&gt;

&lt;p&gt;&amp;lt;!-- jai-imageio-core is allowed since &lt;a href=&quot;https://issues.apache.org/jira/browse/LEGAL-304&quot; title=&quot;BSD3 with nuclear clause&quot; class=&quot;issue-link&quot; data-issue-key=&quot;LEGAL-304&quot;&gt;&lt;del&gt;LEGAL-304&lt;/del&gt;&lt;/a&gt; --&amp;gt;&lt;br/&gt;
 &amp;lt;dependency&amp;gt;&lt;br/&gt;
 &amp;lt;groupId&amp;gt;com.github.jai-imageio&amp;lt;/groupId&amp;gt;&lt;br/&gt;
 &amp;lt;artifactId&amp;gt;jai-imageio-core&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
 &amp;lt;version&amp;gt;1.3.1&amp;lt;/version&amp;gt;&lt;br/&gt;
 &amp;lt;/dependency&amp;gt;&lt;/p&gt;

&lt;p&gt;and pom.xml of  jai-imageio-core version 1.3.1 have inside something like this:&lt;br/&gt;
 &amp;lt;profile&amp;gt;&lt;br/&gt;
 &amp;lt;id&amp;gt;java8-and-higher&amp;lt;/id&amp;gt;&lt;br/&gt;
 &amp;lt;activation&amp;gt;&lt;br/&gt;
 &amp;lt;jdk&amp;gt;[1.8,&amp;lt;/jdk&amp;gt;&lt;br/&gt;
 &amp;lt;/activation&amp;gt;&lt;br/&gt;
 &amp;lt;build&amp;gt;&lt;br/&gt;
 &amp;lt;plugins&amp;gt;&lt;br/&gt;
 &amp;lt;plugin&amp;gt;&lt;/p&gt;

&lt;p&gt;in line: &amp;lt;jdk&amp;gt;[1.8,&amp;lt;/jdk&amp;gt; is missing closing bracket and because of this I get an error when I trying to generate  RPM &lt;br/&gt;
 In the newest version of this library this issue was solved (version 1.4.0)&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 26 Jul 2018 20:34:49 +0000" id="449" opendate="Thu, 26 Jul 2018 12:13:02 +0000">
		<buginformation>
			<summary>Blanket upgrades in prep for 1.19</summary>
			<description>&lt;p&gt;On the dev list &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=solomax&quot; class=&quot;user-hover&quot; rel=&quot;solomax&quot;&gt;Maxim Solodovnik&lt;/a&gt; recommended using: &lt;a href=&quot;https://sonatype.github.io/ossindex-maven/maven-plugin/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://sonatype.github.io/ossindex-maven/maven-plugin/&lt;/a&gt; to identify vulnerable dependencies.&lt;/p&gt;

&lt;p&gt;Let's do that and make other general upgrades as well in prep for 1.19.&lt;/p&gt;

&lt;p&gt;This is a blanket ticket to cover these upgrades.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-bundle/pom.xml</file>
			<file>/tika-example/pom.xml</file>
			<file>/tika-batch/src/test/java/org/apache/tika/batch/fs/BatchDriverTest.java</file>
			<file>/tika-parent/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 9 Aug 2018 19:00:22 +0000" id="450" opendate="Thu, 26 Jul 2018 14:24:21 +0000">
		<buginformation>
			<summary>Upgrade Lucene in tika-eval and tika-example</summary>
			<description>&lt;p&gt;Now that we're on Java 8, we can update Lucene to 7.x.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-eval/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 11 Oct 2018 17:49:26 +0000" id="451" opendate="Wed, 1 Aug 2018 13:57:24 +0000">
		<buginformation>
			<summary>Error indexing a xlsx file</summary>
			<description>&lt;p&gt;Hallo.&lt;/p&gt;

&lt;p&gt;Indexing a xlsx file of 38 MB&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;I obtain the error:&lt;br/&gt;
Error from server at &lt;a href=&quot;http://localhost:8983/solr/core_share:&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://localhost:8983/solr/core_share:&lt;/a&gt; Expected mime type application/xml but got text/html. &amp;lt;html&amp;gt; &amp;lt;head&amp;gt; &amp;lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html;charset=utf-8&quot;/&amp;gt; &amp;lt;title&amp;gt;Error 500 Server Error&amp;lt;/title&amp;gt; &amp;lt;/head&amp;gt; &amp;lt;body&amp;gt;&amp;lt;h2&amp;gt;HTTP ERROR 500&amp;lt;/h2&amp;gt; &amp;lt;p&amp;gt;Problem accessing /solr/core_share/update/extract. Reason: &amp;lt;pre&amp;gt; Server Error&amp;lt;/pre&amp;gt;&amp;lt;/p&amp;gt;&amp;lt;h3&amp;gt;Caused by:&amp;lt;/h3&amp;gt;&amp;lt;pre&amp;gt;java.lang.OutOfMemoryError at java.base/java.lang.AbstractStringBuilder.hugeCapacity(AbstractStringBuilder.java:188) at java.base/java.lang.AbstractStringBuilder.newCapacity(AbstractStringBuilder.java:180) at java.base/java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:147) at java.base/java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:660) at java.base/java.lang.StringBuilder.append(StringBuilder.java:195) at org.apache.solr.handler.extraction.SolrContentHandler.characters(SolrContentHandler.java:302) at org.apache.tika.sax.ContentHandlerDecorator.characters(ContentHandlerDecorator.java:146) at org.apache.tika.sax.SecureContentHandler.characters(SecureContentHandler.java:270) at org.apache.tika.sax.ContentHandlerDecorator.characters(ContentHandlerDecorator.java:146) at org.apache.tika.sax.ContentHandlerDecorator.characters(ContentHandlerDecorator.java:146) at org.apache.tika.sax.ContentHandlerDecorator.characters(ContentHandlerDecorator.java:146) at org.apache.tika.sax.SafeContentHandler.access$001(SafeContentHandler.java:46) at org.apache.tika.sax.SafeContentHandler$1.write(SafeContentHandler.java:82) at org.apache.tika.sax.SafeContentHandler.filter(SafeContentHandler.java:140) at org.apache.tika.sax.SafeContentHandler.characters(SafeContentHandler.java:287) at org.apache.tika.sax.XHTMLContentHandler.characters(XHTMLContentHandler.java:279) at org.apache.tika.sax.XHTMLContentHandler.characters(XHTMLContentHandler.java:306) at org.apache.tika.parser.microsoft.ooxml.OOXMLTikaBodyPartHandler.run(OOXMLTikaBodyPartHandler.java:147) at org.apache.tika.parser.microsoft.ooxml.OOXMLWordAndPowerPointTextHandler.handleEndOfRun(OOXMLWordAndPowerPointTextHandler.java:468) at org.apache.tika.parser.microsoft.ooxml.OOXMLWordAndPowerPointTextHandler.endElement(OOXMLWordAndPowerPointTextHandler.java:450) at org.apache.tika.sax.ContentHandlerDecorator.endElement(ContentHandlerDecorator.java:136) at org.apache.tika.sax.ContentHandlerDecorator.endElement(ContentHandlerDecorator.java:136) at java.xml/com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.endElement(AbstractSAXParser.java:609) at java.xml/com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanEndElement(XMLDocumentFragmentScannerImpl.java:1714) at java.xml/com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:2879) at java.xml/com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:602) at java.xml/com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.next(XMLNSDocumentScannerImpl.java:112) at java.xml/com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:532) at java.xml/com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:888) at java.xml/com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:824) at java.xml/com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:141) at java.xml/com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.parse(AbstractSAXParser.java:1213) at java.xml/com.sun.org.apache.xerces.internal.jaxp.SAXParserImpl$JAXPSAXParser.parse(SAXParserImpl.java:635) at java.xml/com.sun.org.apache.xerces.internal.jaxp.SAXParserImpl.parse(SAXParserImpl.java:324) at java.xml/javax.xml.parsers.SAXParser.parse(SAXParser.java:197) at org.apache.tika.parser.microsoft.ooxml.AbstractOOXMLExtractor.handleGeneralTextContainingPart(AbstractOOXMLExtractor.java:506) at org.apache.tika.parser.microsoft.ooxml.XSSFExcelExtractorDecorator.processShapes(XSSFExcelExtractorDecorator.java:279) at org.apache.tika.parser.microsoft.ooxml.XSSFExcelExtractorDecorator.buildXHTML(XSSFExcelExtractorDecorator.java:185) at org.apache.tika.parser.microsoft.ooxml.AbstractOOXMLExtractor.getXHTML(AbstractOOXMLExtractor.java:135) at org.apache.tika.parser.microsoft.ooxml.XSSFExcelExtractorDecorator.getXHTML(XSSFExcelExtractorDecorator.java:120) at org.apache.tika.parser.microsoft.ooxml.OOXMLExtractorFactory.parse(OOXMLExtractorFactory.java:143) at org.apache.tika.parser.microsoft.ooxml.OOXMLParser.parse(OOXMLParser.java:106) at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:280) at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:280) at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:143) at org.apache.solr.handler.extraction.ExtractingDocumentLoader.load(ExtractingDocumentLoader.java:228) at org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:68) at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:195) at org.apache.solr.core.SolrCore.execute(SolrCore.java:2503) at org.apache.solr.servlet.HttpSolrCall.execute(HttpSolrCall.java:711) at org.apache.solr.servlet.HttpSolrCall.call(HttpSolrCall.java:517) at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:384) at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:330) at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1629) at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533) at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143) at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548) at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132) at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:190) at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595) at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:188) at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1253) at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:168) at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473) at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1564) at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:166) at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1155) at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141) at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:219) at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126) at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132) at org.eclipse.jetty.rewrite.handler.RewriteHandler.handle(RewriteHandler.java:335) at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132) at org.eclipse.jetty.server.Server.handle(Server.java:530) at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:347) at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:256) at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:279) at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:102) at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:124) at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:247) at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:140) at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131) at org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:382) at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:708) at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:626) at java.base/java.lang.Thread.run(Thread.java:844) &amp;lt;/pre&amp;gt; &amp;lt;/body&amp;gt; &amp;lt;/html&amp;gt;&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;How could I solve it?&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;Thanks a lot&lt;/p&gt;

&lt;p&gt;Mario&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/XSSFExcelExtractorDecorator.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/XSSFBExcelExtractorDecorator.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 3 Aug 2018 17:51:00 +0000" id="452" opendate="Fri, 3 Aug 2018 17:47:40 +0000">
		<buginformation>
			<summary>MPEGStream should throw an EOF if appropriate in skipFrame</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/mp3/MpegStream.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 7 Aug 2018 17:55:38 +0000" id="453" opendate="Tue, 7 Aug 2018 16:21:21 +0000">
		<buginformation>
			<summary>Allow configuration of TesseractOCRParser as we do for other parsers</summary>
			<description>&lt;p&gt;It would be handy to be able to configure tesseract via our regular tika-config set up.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/ocr/TesseractOCRConfig.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 8 Aug 2018 17:31:03 +0000" id="454" opendate="Wed, 8 Aug 2018 17:20:34 +0000">
		<buginformation>
			<summary>Store exceptions from VBAMacroReader as we do other embedded exceptions</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/OfficeParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 16 Aug 2018 15:32:54 +0000" id="455" opendate="Thu, 16 Aug 2018 15:16:25 +0000">
		<buginformation>
			<summary>Upgrade to commons-compress 1.18</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parent/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 18 Aug 2018 03:16:26 +0000" id="456" opendate="Fri, 17 Aug 2018 18:21:16 +0000">
		<buginformation>
			<summary>Set Tika to OSGi Execution Environment JavaSE-1.8</summary>
			<description>&lt;p&gt;In &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-2692&quot; title=&quot;Blanket upgrades in prep for 1.19&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-2692&quot;&gt;&lt;del&gt;TIKA-2692&lt;/del&gt;&lt;/a&gt; the OSGi execution environment (osgi.ee) was removed since it was coming out as 9.0 when 1.8 was expected.  This is due to the org.ow2.asm:asm:6.2 JAR file having a module-info.class within it.  Turns out this causes the maven-bundle-plugin to increase the execution environment to Java 9.  Rather than removing osgi.ee we should manually set it to 1.8 which is correct since Java 8 will simply ignore the module-info.class.  This can be reverted once Tika moves up to Java 9 or beyond.{{}}&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-bundle/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 31 Jan 2019 14:01:23 +0000" id="457" opendate="Wed, 22 Aug 2018 09:47:39 +0000">
		<buginformation>
			<summary>Sonatype Nexus auditor is reporting that Jackson databind version used by Apache Tika is vulnerable</summary>
			<description>&lt;p&gt;Sonatype Nexus auditor is reporting that Jackson databind version used by Apache Tika is vulnerable. Recommendation is not to use global default typing with Jackson,&lt;/p&gt;

&lt;p&gt;Refer following for details.&lt;/p&gt;

&lt;p&gt; &lt;br/&gt;
Source Sonatype Data Research&lt;br/&gt;
 &lt;br/&gt;
Severity Sonatype CVSS 3.0: 8.5&lt;br/&gt;
 &lt;br/&gt;
Weakness Sonatype CWE: &lt;a href=&quot;https://cwe.mitre.org/data/definitions/502.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;502&lt;/a&gt;&lt;br/&gt;
 &lt;br/&gt;
Explanation&lt;br/&gt;
&lt;tt&gt;jackson-databind&lt;/tt&gt; is vulnerable to Remote Code Execution (RCE). The &lt;tt&gt;createBeanDeserializer()&lt;/tt&gt; function in the &lt;tt&gt;BeanDeserializerFactory&lt;/tt&gt; class allows untrusted Java objects to be deserialized. A remote attacker can exploit this by uploading a malicious serialized object that will result in RCE if the application attempts to deserialize it.&lt;/p&gt;

&lt;p&gt;Note: This vulnerability exists due to the incomplete fix for CVE-2017-7525, CVE-2017-15095, CVE-2017-17485, CVE-2018-5968, and CVE-2018-7489. Evidence of this can be found at &lt;a href=&quot;https://pivotal.io/security/cve-2017-4995&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://pivotal.io/security/cve-2017-4995&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Jackson provides a blacklisting approach to protecting against this type of attack, but Spring Security should be proactive against blocking unknown “deserialization gadgets” when Spring Security enables default typing.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt; &lt;br/&gt;
Detection&lt;br/&gt;
The application is vulnerable by using this component, when default typing is enabled and passing in untrusted data to be deserialization.&lt;/p&gt;

&lt;p&gt;Note: Spring Security has provided their own fix for this vulnerability (&lt;a href=&quot;https://pivotal.io/security/cve-2017-4995&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;CVE-2017-4995&lt;/a&gt;). If this component is being used as part of Spring Security, then you are not vulnerable if you are running Spring Security 4.2.3.RELEASE or greater for 4.x or Spring Security 5.0.0.M2 or greater for 5.x.&lt;br/&gt;
 &lt;br/&gt;
Recommendation&lt;br/&gt;
There is no non vulnerable version of this component. We recommend investigating alternative components or a potential mitigating control.&lt;/p&gt;

&lt;p&gt;Workaround: Do not use the default typing. Instead you will need to implement your own.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;It is also possible to customize global defaulting, using ObjectMapper.setDefaultTyping(…) – you just have to implement your own TypeResolverBuilder (which is not very difficult); and by doing so, can actually configure all aspects of type information. Builder itself is just a short-cut for building actual handlers.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt; &lt;/p&gt;

&lt;p&gt;Reference: &lt;a href=&quot;https://github.com/FasterXML/jackson-docs/wiki/JacksonPolymorphicDeserialization&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/FasterXML/jackson-docs/wiki/JacksonPolymorphicDeserialization&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Examples of implementing your own typing can be found by looking at &lt;a href=&quot;https://github.com/spring-projects/spring-security/commit/947d11f433b78294942cb5ea56e8aa5c3a0ca439&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;Spring Security's fix&lt;/a&gt; or &lt;a href=&quot;https://stackoverflow.com/questions/12353774/how-to-customize-jackson-type-information-mechanism&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;this Stack Overflow article&lt;/a&gt;.&lt;br/&gt;
 &lt;br/&gt;
Categories&lt;br/&gt;
Data&lt;br/&gt;
Root Cause&lt;br/&gt;
tika-app-1.18.jar &lt;b&gt;&amp;lt;=&lt;/b&gt; SubTypeValidator.class : [2.9.5, )&lt;br/&gt;
Advisories&lt;br/&gt;
Attack: &lt;a href=&quot;https://adamcaudill.com/2017/10/04/exploiting-jackson-rce-cve-2017-7525/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://adamcaudill.com/2017/10/04/exploiting-jackson-rce-cv...&lt;/a&gt;&lt;br/&gt;
Evidence: &lt;a href=&quot;https://pivotal.io/security/cve-2017-4995&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://pivotal.io/security/cve-2017-4995&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parent/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 10 Sep 2018 09:41:35 +0000" id="458" opendate="Thu, 30 Aug 2018 13:44:39 +0000">
		<buginformation>
			<summary>Java 9: Requiring tika-parsers from module-info.java fails with &quot;module not found&quot;</summary>
			<description>&lt;p&gt;When requiring `tika.parsers` from a Java 9 `module-info.java`, Maven throws an error about not being able to find `tika.parsers`:&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.7.0:compile (default-compile) on project annot8-components-tika: Compilation failure&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; /home/bakerj/annot8/annot8-components/annot8-components-tika/src/main/java/module-info.java:&lt;span class=&quot;error&quot;&gt;&amp;#91;9,16&amp;#93;&lt;/span&gt; module not found: tika.parsers&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;It looks like this is likely to be a similar issue to: &lt;a href=&quot;https://github.com/elastic/elasticsearch/issues/28984&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/elastic/elasticsearch/issues/28984&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For an example of a failing project, see: &lt;a href=&quot;https://github.com/annot8/annot8-components/tree/tika&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/annot8/annot8-components/tree/tika&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 11 Sep 2018 20:47:50 +0000" id="459" opendate="Thu, 6 Sep 2018 12:26:07 +0000">
		<buginformation>
			<summary>Make tika-server robust against ooms/infinite loops/memory leaks</summary>
			<description>&lt;p&gt;Currently, tika-server is vulnerable to ooms, inifinite loops and memory leaks.  I see two ways of making it robust:&lt;/p&gt;

&lt;p&gt;1) use the ForkParser&lt;br/&gt;
2) have tika-server spawn a child process that actually runs the server, put a watcher thread in the child that will kill the child on oom/timeout/after x files.  The parent process can then restart the child if it dies. &lt;/p&gt;

&lt;p&gt;I somewhat prefer 2) so that we don't have to doubly pass the inputstream.  I propose 2), and I propose making it optional in Tika 1.x, but then the default in Tika 2.x.  We could also add a status ping from parent to child in case the child gets caught up in stop the world gc (h/t &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bleskes&quot; class=&quot;user-hover&quot; rel=&quot;bleskes&quot;&gt;Boaz Leskes&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Other options/recommendations?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-server/src/main/java/org/apache/tika/server/TikaServerWatchDog.java</file>
			<file>/tika-server/src/main/java/org/apache/tika/server/resource/TikaResource.java</file>
			<file>/tika-server/src/main/java/org/apache/tika/server/resource/DetectorResource.java</file>
			<file>/tika-server/src/main/java/org/apache/tika/server/FileCountExceededException.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 31 Oct 2018 17:52:21 +0000" id="460" opendate="Tue, 25 Sep 2018 02:26:36 +0000">
		<buginformation>
			<summary>notes and footer contents are duplicated in extracting text from power point slides</summary>
			<description>&lt;p&gt;notes and footer contents are duplicated at the end when extract text from ppt slides (like the one in the attachment). Both the input file and the text results are attached. &lt;/p&gt;

&lt;p&gt;Is there a configuration option that can be used to suppress this kind of duplication?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/HSLFExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 22 Oct 2018 16:57:43 +0000" id="461" opendate="Thu, 18 Oct 2018 11:09:59 +0000">
		<buginformation>
			<summary>ScriptsExtractor incorrectly reports Javascript to characters() in SAX ContentHandler</summary>
			<description>&lt;p&gt;We extract Javascript as text content while instead it is actually a script tag with base64 inline. This inline code is decoded and reported in the characters() method of our custom ContentHandler, and ends up as text being extracted, but it seems the Javascript start tag itself is never reported to startElement(). The Javascript is reported to characters() after we left the head and entered the body.&lt;/p&gt;

&lt;p&gt;HTML file is attached&lt;/p&gt;

&lt;p&gt;The following script tag:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &amp;lt;script src=&lt;span class=&quot;code-quote&quot;&gt;&quot;data:text/javascript;base64,Oyh3aW5kb3cuanExODN8fGpRdWVyeSkoZnVuY3Rpb24oJCl7bmV3IEltcHJvdmVkQUpBWExvZ2luKHsNCmlkOiAxNTcsDQppc0d1ZXN0OiAxLA0Kb2F1dGg6IHsiZmFjZWJvb2siOiJodHRwczpcL1wvd3d3LmZhY2Vib29rLmNvbVwvZGlhbG9nXC9vYXV0aD9zY29wZT1lbWFpbCZyZXNwb25zZV90eXBlPWNvZGUmZGlzcGxheT1wb3B1cCZjbGllbnRfaWQ9MTcyODk0MjQzMDY1MDQ4NiZyZWRpcmVjdF91cmk9aHR0cCUzQSUyRiUyRnBldHJvbGljaW91cy5jb20lMkZpbmRleC5waHAlM0ZvcHRpb24lM0Rjb21faW1wcm92ZWRfYWpheF9sb2dpbiUyNnRhc2slM0RmYWNlYm9vayIsImdvb2dsZSI6Imh0dHBzOlwvXC9hY2NvdW50cy5nb29nbGUuY29tXC9vXC9vYXV0aDJcL2F1dGg/c2NvcGU9aHR0cHMlM0ElMkYlMkZ3d3cuZ29vZ2xlYXBpcy5jb20lMkZhdXRoJTJGdXNlcmluZm8uZW1haWwraHR0cHMlM0ElMkYlMkZ3d3cuZ29vZ2xlYXBpcy5jb20lMkZhdXRoJTJGdXNlcmluZm8ucHJvZmlsZSZyZXNwb25zZV90eXBlPWNvZGUmZGlzcGxheT1wb3B1cCZjbGllbnRfaWQ9ODQ5NDk3NjQ3ODUzLW1mOThqNGdlOGkwYzlkaTFrbG9zc2YxbmdibWI2cG12LmFwcHMuZ29vZ2xldXNlcmNvbnRlbnQuY29tJnJlZGlyZWN0X3VyaT1odHRwJTNBJTJGJTJGcGV0cm9saWNpb3VzLmNvbSUyRmluZGV4LnBocCUzRm9wdGlvbiUzRGNvbV9pbXByb3ZlZF9hamF4X2xvZ2luJTI2dGFzayUzRGdvb2dsZSJ9LA0KYmdPcGFjaXR5OiAwLjQsDQpyZXR1cm5Vcmw6ICcvaXMtdGhpcy1kdXRjaC1jbGFzc2ljLWZpbmFsbHktYXMtY29vbC1hcy1hLWJtdycsDQpib3JkZXI6IHBhcnNlSW50KCdmNWY1ZjV8KnwzfCp8YzRjNGM0fCp8Nycuc3BsaXQoJ3wqfCcpWzFdKSwNCnBhZGRpbmc6IDQsDQp1c2VBSkFYOiAwLA0Kb3BlbkV2ZW50OiAnb25jbGljaycsDQp3bmRDZW50ZXI6IDAsDQpyZWdQb3B1cDogMSwNCmR1cjogMzAwLA0KdGltZW91dDogMCwNCmJhc2U6ICcvJywNCnRoZW1lOiAncGV0cm9saWNpb3VzJywNCnNvY2lhbFByb2ZpbGU6ICcnLA0Kc29jaWFsVHlwZTogJ2J0bkljbycsDQpjc3NQYXRoOiAnL21vZHVsZXMvbW9kX2ltcHJvdmVkX2FqYXhfbG9naW4vY2FjaGUvMTU3LzNkNDE4Mzk2NDk2N2Y2ZWVlYjI5MTdhOTI2OGM2MTIxLmNzcycsDQpyZWdQYWdlOiAnam9vbWxhJywNCmNhcHRjaGE6ICcnLA0Kc2hvd0hpbnQ6IDAsDQpnZW9sb2NhdGlvbjogZmFsc2UsDQp3aW5kb3dBbmltOiAnJw0KfSl9KTs=&quot;&lt;/span&gt; type=&lt;span class=&quot;code-quote&quot;&gt;&quot;text/javascript&quot;&lt;/span&gt;&amp;gt;&amp;lt;/script&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;gets reported outside the head (in html.p) as:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
;(window.jq183||jQuery)(function($){&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ImprovedAJAXLogin({
id: 157,
isGuest: 1,
oauth: {&lt;span class=&quot;code-quote&quot;&gt;&quot;facebook&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;https:\/\/www.facebook.com\/dialog\/oauth?scope=email&amp;amp;response_type=code&amp;amp;display=popup&amp;amp;client_id=1728942430650486&amp;amp;redirect_uri=http%3A%2F%2Fpetrolicious.com%2Findex.php%3Foption%3Dcom_improved_ajax_login%26task%3Dfacebook&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;google&quot;&lt;/span&gt;:&lt;span class=&quot;code-quote&quot;&gt;&quot;https:\/\/accounts.google.com\/o\/oauth2\/auth?scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.profile&amp;amp;response_type=code&amp;amp;display=popup&amp;amp;client_id=849497647853-mf98j4ge8i0c9di1klossf1ngbmb6pmv.apps.googleusercontent.com&amp;amp;redirect_uri=http%3A%2F%2Fpetrolicious.com%2Findex.php%3Foption%3Dcom_improved_ajax_login%26task%3Dgoogle&quot;&lt;/span&gt;},
bgOpacity: 0.4,
returnUrl: &lt;span class=&quot;code-quote&quot;&gt;'/is-&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;-dutch-classic-&lt;span class=&quot;code-keyword&quot;&gt;finally&lt;/span&gt;-as-cool-as-a-bmw'&lt;/span&gt;,
border: parseInt(&lt;span class=&quot;code-quote&quot;&gt;'f5f5f5|*|3|*|c4c4c4|*|7'&lt;/span&gt;.split(&lt;span class=&quot;code-quote&quot;&gt;'|*|'&lt;/span&gt;)[1]),
padding: 4,
useAJAX: 0,
openEvent: &lt;span class=&quot;code-quote&quot;&gt;'onclick'&lt;/span&gt;,
wndCenter: 0,
regPopup: 1,
dur: 300,
timeout: 0,
base: &lt;span class=&quot;code-quote&quot;&gt;'/'&lt;/span&gt;,
theme: &lt;span class=&quot;code-quote&quot;&gt;'petrolicious'&lt;/span&gt;,
socialProfile: '',
socialType: &lt;span class=&quot;code-quote&quot;&gt;'btnIco'&lt;/span&gt;,
cssPath: &lt;span class=&quot;code-quote&quot;&gt;'/modules/mod_improved_ajax_login/cache/157/3d4183964967f6eeeb2917a9268c6121.css'&lt;/span&gt;,
regPage: &lt;span class=&quot;code-quote&quot;&gt;'joomla'&lt;/span&gt;,
captcha: '',
showHint: 0,
geolocation: &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;,
windowAnim: ''
})});
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/html/HtmlHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 8 Nov 2018 18:51:18 +0000" id="462" opendate="Thu, 8 Nov 2018 22:19:10 +0000">
		<buginformation>
			<summary>Bulk upgrade plugins and dependencies</summary>
			<description>&lt;p&gt;Thanks to &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-2757&quot; title=&quot;Add versions-maven-plugin &quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-2757&quot;&gt;&lt;del&gt;TIKA-2757&lt;/del&gt;&lt;/a&gt;...there are some areas for upgrading. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/biggrin.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-bundle/pom.xml</file>
			<file>/tika-parent/pom.xml</file>
			<file>/pom.xml</file>
			<file>/tika-eval/pom.xml</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 24 Sep 2018 14:20:23 +0000" id="463" opendate="Tue, 1 May 2018 21:21:43 +0000">
		<buginformation>
			<summary>Tika server fails with status 500 if X-Tika-OCRLanguage set to multiple OCR dictionaries</summary>
			<description>&lt;p&gt;Tika 1.18 fails with returned status 500 if setting MULTIPLE (delimited by +) dictionaries for Tesseract OCR set by HTTP &lt;tt&gt;header like &quot;X-Tika-OCRLanguage: eng+fra&quot;&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;Setting a single OCR dictionary works.&lt;/p&gt;

&lt;p&gt;Relevant documentation part from &lt;a href=&quot;https://wiki.apache.org/tika/TikaOCR:&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://wiki.apache.org/tika/TikaOCR&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;&lt;a name=&quot;Overridingtheconfiguredlanguageaspartofyourrequest&quot;&gt;&lt;/a&gt;Overriding the configured language as part of your request&lt;/h2&gt;

&lt;p&gt;Different requests may need processing using different language models. These can be specified for specific requests using the &lt;em&gt;X-Tika-OCRLanguage&lt;/em&gt; custom header. An example of this is shown below:&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;curl &lt;del&gt;T /path/to/tiff/image.jpg &lt;a href=&quot;http://localhost:9998/tika&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://localhost:9998/tika&lt;/a&gt;&lt;/del&gt; -header &quot;X-Tika-OCRLanguage: eng&quot;&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;Or for multiple languages:&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;curl &lt;del&gt;T /path/to/tiff/image.jpg &lt;a href=&quot;http://localhost:9998/tika&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://localhost:9998/tika&lt;/a&gt;&lt;/del&gt; -header &quot;X-Tika-OCRLanguage: eng+fra&quot;&lt;/tt&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-server/src/main/java/org/apache/tika/server/resource/TikaResource.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 19 Sep 2018 16:55:54 +0000" id="464" opendate="Wed, 19 Sep 2018 10:12:30 +0000">
		<buginformation>
			<summary>parseToString fails for a simple mp3</summary>
			<description>&lt;p&gt;This is a regression from 1.18. I've attached the mp3 that fails. The exception I get is:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.tika.exception.TikaException: TIKA-198: Illegal IOException from org.apache.tika.parser.mp3.Mp3Parser@cefe6c6
    at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:286)
    at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:280)
    at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:143)
    at org.apache.tika.Tika.parseToString(Tika.java:527)
    at com.company.TextExtractor.getText(TextExtractor.java:39)

    Caused by:
    java.io.EOFException: EOF: tried to skip 361 but could only skip 247
        at org.apache.tika.parser.mp3.MpegStream.skipFrame(MpegStream.java:166)
        at org.apache.tika.parser.mp3.Mp3Parser.getAllTagHandlers(Mp3Parser.java:204)
        at org.apache.tika.parser.mp3.Mp3Parser.parse(Mp3Parser.java:71)
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:280)
        ... 5 more&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/mp3/Mp3Parser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 25 Sep 2018 19:09:38 +0000" id="465" opendate="Fri, 28 Sep 2018 07:57:55 +0000">
		<buginformation>
			<summary>Update Python dependency check for TesseractOCR Parser rotation.py script</summary>
			<description>&lt;p&gt;TesseractOCRParserTest.testRotatedOCR fails when TkInter module is not available but Terreract and other named Python dependencies are installed.&lt;/p&gt;

&lt;p&gt;To address this we should:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Update the &lt;em&gt;hasPython&lt;/em&gt; me to include this in the check&lt;/li&gt;
	&lt;li&gt;Update the Wiki to list explicitly the dependencies and how to install them on major platforms/variants.&lt;/li&gt;
&lt;/ul&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/ocr/TesseractOCRParser.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 2 Oct 2018 14:49:52 +0000" id="466" opendate="Mon, 1 Oct 2018 16:14:32 +0000">
		<buginformation>
			<summary>Tika 1.19 trigger a dependency on slf4j-log4j12</summary>
			<description>&lt;p&gt;It's actually jmatio's fault.&lt;/p&gt;

&lt;p&gt;It's not library's job to choose the slf4j's implementation to use.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 2 Aug 2017 11:42:02 +0000" id="467" opendate="Wed, 2 Aug 2017 10:50:03 +0000">
		<buginformation>
			<summary>Avoid NullPointerException in org.apache.tika.langdetect.OptimaizeLangDetector if models haven't been loaded</summary>
			<description>&lt;p&gt;In an intuitive usage of&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;LanguageDetector languageDetector = LanguageDetector.getDefaultLanguageDetector();
List&amp;lt;LanguageResult&amp;gt; languageResults = languageDetector.detectAll(someNonEmptyString);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;`org.apache.tika.langdetect.OptimaizeLangDetector` might/will &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/help_16.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; be chosen as default detector, but since there's no call to `LanguageDetector.loadModels()` `LanguageDetector.detector` is `null` in `detectAll` which causes an unhelpful error situation which one has to investigate in the code. A simple check whether `detector` is `null` and throwing an `IllegalStateException(&quot;models haven't been loaded yet (forgot to call loadModels?&quot;)` would be much more intuitive.&lt;/p&gt;

&lt;p&gt;If that corresponds to the expected behaviour (it's my first week with Tika), I can provide a patch or pull request.&lt;/p&gt;

&lt;p&gt;experienced with 1.16-75-g4455a6f08&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-langdetect/src/main/java/org/apache/tika/langdetect/OptimaizeLangDetector.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 18 Nov 2017 01:25:30 +0000" id="468" opendate="Sat, 18 Nov 2017 01:23:34 +0000">
		<buginformation>
			<summary>Nullpointer in tika-dl test on windows</summary>
			<description>&lt;p&gt;During a build on windows I get the following:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Running org.apache.tika.dl.imagerec.DL4JVGG16NetTest
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.024 sec &amp;lt;&amp;lt;&amp;lt; FAILURE! - in org.apache.tika.dl.imagerec.DL4JVGG16NetTest
recognise(org.apache.tika.dl.imagerec.DL4JVGG16NetTest)  Time elapsed: 0.024 sec  &amp;lt;&amp;lt;&amp;lt; ERROR!
java.lang.NullPointerException: &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;
	at org.apache.tika.Tika.&amp;lt;init&amp;gt;(Tika.java:109)
	at org.apache.tika.dl.imagerec.DL4JVGG16NetTest.recognise(DL4JVGG16NetTest.java:42)

HDF5-DIAG: Error detected in HDF5 (1.10.0-patch1) thread 0:
  #000: C:\autotest\HDF5110ReleaseRWDITAR\src\H5F.c line 579 in H5Fopen(): unable to open file
    major: File accessibilty
    minor: Unable to open file
  #001: C:\autotest\HDF5110ReleaseRWDITAR\src\H5Fint.c line 1208 in H5F_open(): unable to read superblock
    major: File accessibilty
    minor: Read failed
SUREFIRE-859:   #002: C:\autotest\HDF5110ReleaseRWDITAR\src\H5Fsuper.c line 443 in H5F__super_read(): truncated file: eof = 147097136, sblock-&amp;gt;base_addr = 0, stored_eof = 553466928
    major: File accessibilty
    minor: File has been truncated

Results :

Tests in error: 
  DL4JVGG16NetTest.recognise:42 » NullPointer
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It appears to be looking for some installed native code that it can't find.  I believe we should check for null config and if null we skip this test.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-dl/src/test/java/org/apache/tika/dl/imagerec/DL4JVGG16NetTest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 15 Jan 2018 11:46:39 +0000" id="469" opendate="Mon, 27 Nov 2017 15:41:31 +0000">
		<buginformation>
			<summary>TesseractOCRParser ignores configured ImageMagickPath in processImage method</summary>
			<description>&lt;p&gt;The TesseractOCRParser class uses the configured ImageMagickPath in method hasImageMagick to determine whether ImageMagick is present.  Ref:&lt;br/&gt;
String ImageMagick = config.getImageMagickPath() + getImageMagickProg();&lt;/p&gt;

&lt;p&gt;BUT then completely ignores the configured path in the processImage method meaning ImageMagick has to be present on system path (so what's the point of the ImageMagickPath config setting).&lt;/p&gt;

&lt;p&gt;The doOCR method on the other hand DOES use the configured tesseractPath.&lt;/p&gt;

&lt;p&gt;Incidentally I notice there is no equivalent PythonPath config setting even though Python is attempted to be found/used.&lt;/p&gt;

&lt;p&gt;Some consistency would be appreciated so that ImageMagick and Python don't have to be present on the system path.  i.e. follow the model already in place for finding/using Tesseract.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/ocr/TesseractOCRParser.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 15 Dec 2017 01:45:37 +0000" id="470" opendate="Sat, 16 Dec 2017 20:44:07 +0000">
		<buginformation>
			<summary>OutlookExtractor &quot;buffer underrun&quot; when parsing .msg with embedded .msg</summary>
			<description>&lt;p&gt;When parsing certain .msg files containing certain attachments (e.g. other .msg files), I get this error:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;...
Caused by: org.apache.poi.util.LittleEndian$BufferUnderrunException: buffer underrun
        at org.apache.poi.util.LittleEndian.readInt(LittleEndian.java:662)
        at org.apache.poi.hmef.CompressedRTF.decompress(CompressedRTF.java:73)
        at org.apache.poi.util.LZWDecompresser.decompress(LZWDecompresser.java:81)
        at org.apache.poi.hmef.attribute.MAPIRtfAttribute.&amp;lt;init&amp;gt;(MAPIRtfAttribute.java:42)
        at org.apache.tika.parser.microsoft.OutlookExtractor.parse(OutlookExtractor.java:270)
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I think the issue is with &lt;tt&gt;MAPIRtfAttribute&lt;/tt&gt; not liking it when receiving an empty byte array from &lt;tt&gt;OutlookExtractor&lt;/tt&gt;.  I was able to eliminate the error at around line 269 of &lt;tt&gt;OutlookExtractor&lt;/tt&gt; with Tika 1.16 code (or around line 322 with Tika 1.17) with the following:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
            &lt;span class=&quot;code-comment&quot;&gt;//--- START FIX ---
&lt;/span&gt;            ByteChunk chunk = (ByteChunk) rtfChunk;
            &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (chunk != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; &amp;amp;&amp;amp; chunk.getValue() != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; 
                    &amp;amp;&amp;amp; chunk.getValue().length &amp;gt; 0 &amp;amp;&amp;amp; !doneBody) {
                &lt;span class=&quot;code-comment&quot;&gt;//ByteChunk chunk = (ByteChunk) rtfChunk;
&lt;/span&gt;            &lt;span class=&quot;code-comment&quot;&gt;//--- END FIX ---&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I am not sure if that is a real fix or more should be done than just getting rid of the error to make sure all is extracted properly from all files.&lt;/p&gt;

&lt;p&gt;I cannot share the sample file I have to test since it was given to me as sensitive content and I could not recreate a faulty msg file.&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/OutlookExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 21 Dec 2017 03:24:21 +0000" id="471" opendate="Mon, 18 Dec 2017 07:31:35 +0000">
		<buginformation>
			<summary>RarParser throws RarException instead of EncryptedDocumentException when parsing encrypted file</summary>
			<description>&lt;p&gt;While the code tries to check and throw EncryptedDocumentException at line 70~72, junrar itself will throw RarException with Encrypted type during Archive construction (line 68).&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/pkg/RarParserTest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 23 Jan 2018 16:21:50 +0000" id="472" opendate="Wed, 3 Jan 2018 15:02:58 +0000">
		<buginformation>
			<summary>Use latest org.opengis:geoapi to avoid rejected/EOL'd jsr-275 dependency</summary>
			<description>&lt;p&gt;Tika currently depends on the very old (June 2011) 3.0.0 version of org.opengis:geoapi&lt;br/&gt;
which in turn unfortunately pulls in the rejected jsr-275 as a dependency (which is EOL).&lt;/p&gt;

&lt;p&gt;However the latest 3.0.1 version of org.opengis:geoapi released recently (Sept 2017) switches to depending on the active 1.0 version of javax.measure.unit-api which is an implementation of jsr-363 which was accepted and effectively replaced the rejected jsr-275.&lt;/p&gt;

&lt;p&gt;Therefore requesting that Tika move to the 3.0.1 version of org.opengis:geoapi&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-java7/src/test/java/org/apache/tika/filetypedetector/TikaFileTypeDetectorTest.java</file>
			<file>/tika-bundle/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 26 Jan 2018 18:54:50 +0000" id="473" opendate="Tue, 23 Jan 2018 16:12:47 +0000">
		<buginformation>
			<summary>TIka Server uses HtmlParser for XML no matter what config is given, even if XML is disabled in Config</summary>
			<description>&lt;p&gt;For some reason, the Tika Server has this line in TikaResource.java&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
parsers.put(MediaType.APPLICATION_XML, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; HtmlParser());
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The upshot of which is that the Tika Server (only) will always use the HtmlParser for XML files, no matter what is configured in the Tika Config. If you disable XML in the Tika Config, or assign it to a different parser, this will be silently ignored&lt;/p&gt;

&lt;p&gt;To test, run the Tika Server with the &lt;tt&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-866&quot; title=&quot;Invalid configuration file causes OutOfMemoryException&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-866&quot;&gt;&lt;del&gt;TIKA-866&lt;/del&gt;&lt;/a&gt;-valid.xml&lt;/tt&gt; test file from &lt;tt&gt;tika-core/src/test/resources/org/apache/tika/config&lt;/tt&gt; which uses the EmptyParser for everything. If you ask the server what parsers it has, it correctly reports none at &lt;a href=&quot;http://localhost:9998/parsers&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://localhost:9998/parsers&lt;/a&gt; . If you give it an XML file, you'd expect it to fall through to the fallback parser (or possibly empty parser). Instead, it gets processed as html, which is completely unexpected!&lt;/p&gt;

&lt;p&gt;Originally discovered via &lt;a href=&quot;https://stackoverflow.com/questions/48391615/tell-tika-not-to-parse-xml&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://stackoverflow.com/questions/48391615/tell-tika-not-to-parse-xml&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 6 Sep 2018 15:34:26 +0000" id="474" opendate="Thu, 25 Jan 2018 14:03:07 +0000">
		<buginformation>
			<summary>Subtypes for common text formats currently included in text/plain</summary>
			<description>&lt;p&gt;Currently, we have a very large number of file extension globs all feeding into the &lt;tt&gt;text/plain&lt;/tt&gt; mimetype. This includes not only variations on actual plain text, but also lots of other text-based formats (eg config or makefile/autoconf files). This list dates back quite a while (&lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-85&quot; title=&quot;Add glob patterns from the ASF svn:eol-style documentation&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-85&quot;&gt;&lt;del&gt;TIKA-85&lt;/del&gt;&lt;/a&gt; seems to have added most of them)&lt;/p&gt;

&lt;p&gt;While this simplifies things in Tika, it has the downside of making it very tricky for people to add custom parsers for these text-based formats (eg &lt;a href=&quot;https://stackoverflow.com/questions/48411421/define-a-mime-type-for-txt-files-for-tika&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://stackoverflow.com/questions/48411421/define-a-mime-type-for-txt-files-for-tika&lt;/a&gt; where they want to handle .cfg differently to other .txt)&lt;/p&gt;

&lt;p&gt;Because of how &lt;tt&gt;AutoDetectParser&lt;/tt&gt; works, as long as there's no more specific parser defined, if we create some new &lt;tt&gt;text/&lt;/tt&gt; subtypes which extend &lt;tt&gt;text/plain&lt;/tt&gt; then there won't be any change in parsing behaviour. The only change would be for detection, where a more specific type would be returned&lt;/p&gt;

&lt;p&gt;I therefore propose that we pull some of these (file-magic-less) globs out into other &lt;tt&gt;text/&lt;/tt&gt; mimetypes with a parent of &lt;tt&gt;text/plan&lt;/tt&gt; , grouped roughly by type&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 31 Jan 2018 14:53:07 +0000" id="475" opendate="Fri, 26 Jan 2018 10:25:49 +0000">
		<buginformation>
			<summary>org.json package clash</summary>
			<description>&lt;p&gt;Hello dear tika contributors.&lt;br/&gt;
In version 1.16, the dependency of org.json:json (&lt;a href=&quot;https://mvnrepository.com/artifact/org.json/json/20140107&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://mvnrepository.com/artifact/org.json/json/20140107&lt;/a&gt;)&lt;br/&gt;
was replaced with com.tdunning:json (&lt;a href=&quot;https://mvnrepository.com/artifact/com.tdunning/json/1.8&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://mvnrepository.com/artifact/com.tdunning/json/1.8&lt;/a&gt;)&lt;br/&gt;
as part of &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1804&quot; title=&quot;Tika use no free json.org&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1804&quot;&gt;&lt;del&gt;TIKA-1804&lt;/del&gt;&lt;/a&gt;.&lt;br/&gt;
I read and understand the reasons. &lt;br/&gt;
The problem I see is that the package org.json is reused in the replacement lib (com.tdunning) therefore creating really ugly runtime problems for any application that might use the old org.json lib ( Douglas Crockford ) either directly or indirectly (And please note that there are still a lot of libraries that use that lib: &lt;a href=&quot;https://mvnrepository.com/artifact/org.json/json/usages&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://mvnrepository.com/artifact/org.json/json/usages&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;b&gt;What is tika's recommendation for these kind of projects that have to(for various reasons) use directly or indirectly(brought in by another lib) that old org.json:json lib?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;This topic has been discussed here also :  &lt;a href=&quot;https://github.com/tdunning/open-json/issues/11&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/tdunning/open-json/issues/11&lt;/a&gt;&lt;br/&gt;
As you can see other people have this problem.&lt;/p&gt;

&lt;p&gt;They forked and released an alternative version, that is identical in functionality, but uses a different package name : &lt;a href=&quot;https://github.com/openjson/openjson&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/openjson/openjson&lt;/a&gt; -&amp;gt; &lt;a href=&quot;https://mvnrepository.com/artifact/com.github.openjson/openjson&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://mvnrepository.com/artifact/com.github.openjson/openjson&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Is there any plan to switch to the cleaner (&quot;jar hell&quot; free) implementation of json?&lt;/b&gt;&lt;/p&gt;


&lt;p&gt;Thank you!&lt;/p&gt;

&lt;p&gt;I raised this as an improvement, as I think that is what needs to happen: move to the forked lib version to avoid package name collisions.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 2 Feb 2018 13:21:10 +0000" id="476" opendate="Wed, 31 Jan 2018 22:28:30 +0000">
		<buginformation>
			<summary>Tika Parser includes oudated/vulnerable version of JSoup</summary>
			<description>&lt;p&gt;org.apache.tika:tika-parsers:1.17 pulls in dependency JSoup 1.7.2.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;JSoup versions older than 1.8.3 have a vulnerability in parsing.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://nvd.nist.gov/vuln/detail/CVE-2015-6748&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://nvd.nist.gov/vuln/detail/CVE-2015-6748&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 21 Feb 2018 21:37:52 +0000" id="477" opendate="Fri, 2 Feb 2018 15:50:42 +0000">
		<buginformation>
			<summary>Extract embedded objects in HTML and javascript</summary>
			<description>&lt;p&gt;Files (esp images) and other objects can be embedded in html/css/javascript with the &lt;a href=&quot;https://en.wikipedia.org/wiki/Data_URI_scheme&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;data: uri scheme&lt;/a&gt;.  We should extract those like any other embedded file.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 5 Feb 2018 14:55:47 +0000" id="478" opendate="Fri, 2 Feb 2018 22:48:40 +0000">
		<buginformation>
			<summary>Tika client cannot extract files from embedded archive formats</summary>
			<description>&lt;p&gt; &lt;/p&gt;

&lt;p&gt;This may be related to &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-2395&quot; title=&quot;The parser does not support AutoCloseInputStream anymore&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-2395&quot;&gt;&lt;del&gt;TIKA-2395&lt;/del&gt;&lt;/a&gt;. When trying to extract the files from &lt;/p&gt;

&lt;p&gt;tika/tika-parsers/src/test/resources/test-documents/test-documents.tgz &lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;% coursier launch org.apache.tika:tika-app:1.17 --main org.apache.tika.cli.TikaCLI &amp;#8211; --extract test-documents.tgz&lt;/p&gt;

&lt;p&gt;I see the exception:&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;Exception in thread &quot;main&quot; org.apache.tika.exception.TikaException: &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-198&quot; title=&quot;Better distinction between IOException and TikaException&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-198&quot;&gt;&lt;del&gt;TIKA-198&lt;/del&gt;&lt;/a&gt;: Illegal IOException from org.apache.tika.parser.pkg.CompressorParser@62628e78&lt;/p&gt;

&lt;p&gt;at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:286)&lt;/p&gt;

&lt;p&gt;at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:280)&lt;/p&gt;

&lt;p&gt;at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:143)&lt;/p&gt;

&lt;p&gt;at org.apache.tika.cli.TikaCLI$OutputType.process(TikaCLI.java:205)&lt;/p&gt;

&lt;p&gt;at org.apache.tika.cli.TikaCLI.process(TikaCLI.java:486)&lt;/p&gt;

&lt;p&gt;at org.apache.tika.cli.TikaCLI.main(TikaCLI.java:145)&lt;/p&gt;

&lt;p&gt;at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;/p&gt;

&lt;p&gt;at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&lt;/p&gt;

&lt;p&gt;at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;/p&gt;

&lt;p&gt;at java.base/java.lang.reflect.Method.invoke(Method.java:564)&lt;/p&gt;

&lt;p&gt;at coursier.cli.qR.a(Unknown Source)&lt;/p&gt;

&lt;p&gt;at coursier.cli.qQ.j(Unknown Source)&lt;/p&gt;

&lt;p&gt;at coursier.cli.qW.a(Unknown Source)&lt;/p&gt;

&lt;p&gt;at d.h.a.c(Unknown Source)&lt;/p&gt;

&lt;p&gt;at b.b.c_(Unknown Source)&lt;/p&gt;

&lt;p&gt;at d.b.d.E.g(Unknown Source)&lt;/p&gt;

&lt;p&gt;at d.b.e.aW.g(Unknown Source)&lt;/p&gt;

&lt;p&gt;at d.b.f.b.aa.a(Unknown Source)&lt;/p&gt;

&lt;p&gt;at coursier.cli.qQ.b(Unknown Source)&lt;/p&gt;

&lt;p&gt;at coursier.cli.Q.b(Unknown Source)&lt;/p&gt;

&lt;p&gt;at b.J.c_(Unknown Source)&lt;/p&gt;

&lt;p&gt;at d.F.h(Unknown Source)&lt;/p&gt;

&lt;p&gt;at b.F.a(Unknown Source)&lt;/p&gt;

&lt;p&gt;at coursier.cli.Coursier.main(Unknown Source)&lt;/p&gt;

&lt;p&gt;at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;/p&gt;

&lt;p&gt;at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&lt;/p&gt;

&lt;p&gt;at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;/p&gt;

&lt;p&gt;at java.base/java.lang.reflect.Method.invoke(Method.java:564)&lt;/p&gt;

&lt;p&gt;at coursier.Bootstrap.main(Bootstrap.java:428)&lt;/p&gt;

&lt;p&gt;Caused by: java.io.IOException: mark/reset not supported&lt;/p&gt;

&lt;p&gt;at java.base/java.io.InputStream.reset(InputStream.java:474)&lt;/p&gt;

&lt;p&gt;at org.apache.tika.parser.microsoft.POIFSContainerDetector.detect(POIFSContainerDetector.java:444)&lt;/p&gt;

&lt;p&gt;at org.apache.tika.detect.CompositeDetector.detect(CompositeDetector.java:84)&lt;/p&gt;

&lt;p&gt;at org.apache.tika.cli.TikaCLI$FileEmbeddedDocumentExtractor.parseEmbedded(TikaCLI.java:1045)&lt;/p&gt;

&lt;p&gt;at org.apache.tika.parser.pkg.CompressorParser.parse(CompressorParser.java:222)&lt;/p&gt;

&lt;p&gt;at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:280)&lt;/p&gt;

&lt;p&gt;... 28 more&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;However, I can browse the document fine using:&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;% coursier launch org.apache.tika:tika-app:1.17 --main org.apache.tika.cli.TikaCLI &amp;#8211; test-documents.tgz&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;This issue affects: test-documents.rar, test-documents.tar.Z, test-documents.tbz2, and test-documents.tgz&lt;/p&gt;

&lt;p&gt;But it does not affect test-documents.7z, test-documents.cab, test-documents.ddf, test-documents.dmg, test-documents.tar, or test-documents.zip&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt; This makes me suspect that it has something to do with extracting files from packages that are embedded in other archive parsers.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 7 Feb 2018 12:53:27 +0000" id="479" opendate="Wed, 7 Feb 2018 00:57:39 +0000">
		<buginformation>
			<summary>Tika mistakenly determines mimetype of .min.js file as matlab</summary>
			<description>&lt;p&gt;Attached file is misinterpreted as being a matlab file when it's really just a minimised javascript file.&lt;/p&gt;

&lt;p&gt;Using:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; DefaultDetector mimeTypeDetector = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; DefaultDetector();

&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; determineMimeType(&lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt;[] data, &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; fileName) {
    &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; TikaInputStream inputStream = TikaInputStream.get(data);
    &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Metadata metadata = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Metadata();
    metadata.set(Metadata.RESOURCE_NAME_KEY, fileName);
    &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; mimeTypeDetector.detect(inputStream, metadata).toString();
    } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (&lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; IOException e) {
        &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ApiException(e);
    }
}&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 8 Mar 2018 11:44:02 +0000" id="480" opendate="Thu, 8 Feb 2018 16:00:59 +0000">
		<buginformation>
			<summary>Full encrypted 7Z file not detected as such</summary>
			<description>&lt;p&gt;&lt;em&gt;Full encrypted 7zip containers that hide its subitem names are not detected as encrypted. Fix is to catch PasswordRequiredException when creating SevenZFile into PackageParser and rethrow it as EncryptedDocumentException&lt;/em&gt; &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 9 Feb 2018 19:44:30 +0000" id="481" opendate="Fri, 9 Feb 2018 13:40:40 +0000">
		<buginformation>
			<summary>Swallows security exception and returns null</summary>
			<description>&lt;p&gt;&lt;a href=&quot;https://github.com/elastic/elasticsearch/pull/28570#issuecomment-364330623&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;We&lt;/a&gt; had what looked like a Tika error when we ran our tests on Java 10. It turns out that this was caused by a &lt;a href=&quot;https://github.com/dmlloyd/openjdk/commit/0052c70c389aa5bed6e9b47e09ca887ac0ef4b2f#diff-704561497d8c3df4c77bc8f35b15ad73R825&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;change&lt;/a&gt; in Java 10 to make ZipFile required accessDeclaredMembers, a permission that we don't grant ourselves. Tika made this &lt;b&gt;very&lt;/b&gt; difficult to debug by catching `RuntimeException` and returning `null`. Since `SecurityException` extends `RuntimeException` we lost all debugging information that might have made this problem simple to track down. Tika should probably let `SecurityException` bubble up.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pkg/ZipContainerDetector.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 7 Mar 2018 00:55:03 +0000" id="482" opendate="Thu, 15 Feb 2018 07:58:23 +0000">
		<buginformation>
			<summary>Add application/zstd detection and parser</summary>
			<description>&lt;p&gt;The IETF is currently checking the specification of Zstandard compression and the application/zstd Media Type: &lt;a href=&quot;https://tools.ietf.org/id/draft-kucherawy-dispatch-zstd-01.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://tools.ietf.org/id/draft-kucherawy-dispatch-zstd-01.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As soon as the MediaType application/zstd is set as standard the Media Type shall be implemented.&lt;/p&gt;

&lt;p&gt;Possible mime-detection for tika-mimetypes.xml (second comment has to be changed when the standard is final):&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-xml&quot;&gt;
  &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;mime-type type=&lt;span class=&quot;code-quote&quot;&gt;&quot;application/zstd&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;_comment&amp;gt;&lt;/span&gt;https://en.wikipedia.org/wiki/Zstandard&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/_comment&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;_comment&amp;gt;&lt;/span&gt;https://tools.ietf.org/id/draft-kucherawy-dispatch-zstd-01.html&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/_comment&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;magic priority=&lt;span class=&quot;code-quote&quot;&gt;&quot;50&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;match value=&lt;span class=&quot;code-quote&quot;&gt;&quot;0xFD2FB528&quot;&lt;/span&gt; type=&lt;span class=&quot;code-quote&quot;&gt;&quot;little32&quot;&lt;/span&gt; offset=&lt;span class=&quot;code-quote&quot;&gt;&quot;0&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/magic&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;glob pattern=&lt;span class=&quot;code-quote&quot;&gt;&quot;*.zstd&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/mime-type&amp;gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;commons-compress version 1.16 and later provide a compressor and decompressor for the algorithm, based on com.github.luben zstd-jni &lt;a href=&quot;https://github.com/luben/zstd-jni&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/luben/zstd-jni&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Attached sampe zstd file (huffman-compressed-larger) and the result after decompressing it.&lt;/p&gt;

&lt;p&gt;Decompression was done with commons-compress 1.16.1 and zstd-jni 1.3.3-3&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-xml&quot;&gt;

&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;dependency&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;groupId&amp;gt;&lt;/span&gt;org.apache.commons&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/groupId&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;artifactId&amp;gt;&lt;/span&gt;commons-compress&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/artifactId&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;version&amp;gt;&lt;/span&gt;1.16.1&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/version&amp;gt;&lt;/span&gt;
&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/dependency&amp;gt;&lt;/span&gt;
&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;dependency&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;groupId&amp;gt;&lt;/span&gt;com.github.luben&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/groupId&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;artifactId&amp;gt;&lt;/span&gt;zstd-jni&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/artifactId&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;version&amp;gt;&lt;/span&gt;1.3.3-3&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/version&amp;gt;&lt;/span&gt;
&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/dependency&amp;gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;Regards&lt;/p&gt;

&lt;p&gt;Andreas&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 28 Mar 2018 16:50:13 +0000" id="483" opendate="Wed, 21 Feb 2018 14:23:04 +0000">
		<buginformation>
			<summary>Update to PDFBox 2.0.9 when available</summary>
			<description>&lt;p&gt;Hey team&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;We got this report in elasticsearch ingest attachment project: &lt;a href=&quot;https://github.com/elastic/elasticsearch/issues/27198&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/elastic/elasticsearch/issues/27198&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Basically when a font is not available PDFBox is throwing an exception like&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;2017/10/31 00:01:13.348 &lt;span class=&quot;error&quot;&gt;&amp;#91;WARN &amp;#93;&lt;/span&gt; [elasticsearch&lt;span class=&quot;error&quot;&gt;&amp;#91;test&amp;#93;&lt;/span&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;bulk&amp;#93;&lt;/span&gt;&lt;a href=&quot;#3&quot;&gt;T#3&lt;/a&gt;] &lt;span class=&quot;error&quot;&gt;&amp;#91;FontManager&amp;#93;&lt;/span&gt; Font not found: TimesNewRomanPS-BoldMT 2017/10/31 00:01:13.413 &lt;span class=&quot;error&quot;&gt;&amp;#91;ERROR&amp;#93;&lt;/span&gt; [elasticsearch&lt;span class=&quot;error&quot;&gt;&amp;#91;test&amp;#93;&lt;/span&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;bulk&amp;#93;&lt;/span&gt;&lt;a href=&quot;#3&quot;&gt;T#3&lt;/a&gt;] &lt;span class=&quot;error&quot;&gt;&amp;#91;TrueTypeFont&amp;#93;&lt;/span&gt; An error occured when reading table cmap java.io.IOException: CMap subtype 14 not yet implemented at org.apache.fontbox.ttf.CMAPEncodingEntry.processSubtype14(CMAPEncodingEntry.java:304) at org.apache.fontbox.ttf.CMAPEncodingEntry.initSubtable(CMAPEncodingEntry.java:114) at org.apache.fontbox.ttf.CMAPTable.initData(CMAPTable.java:100) at org.apache.fontbox.ttf.TrueTypeFont.initializeTable(TrueTypeFont.java:280) at org.apache.fontbox.ttf.AbstractTTFParser.parseTables(AbstractTTFParser.java:128) at org.apache.fontbox.ttf.TTFParser.parseTables(TTFParser.java:80) at org.apache.fontbox.ttf.AbstractTTFParser.parseTTF(AbstractTTFParser.java:109) at org.apache.fontbox.ttf.TTFParser.parseTTF(TTFParser.java:25) at org.apache.fontbox.ttf.AbstractTTFParser.parseTTF(AbstractTTFParser.java:84) at org.apache.fontbox.ttf.TTFParser.parseTTF(TTFParser.java:25) at org.apache.pdfbox.pdmodel.font.PDTrueTypeFont.getTTFFont(PDTrueTypeFont.java:632) at org.apache.pdfbox.pdmodel.font.PDTrueTypeFont.getFontWidth(PDTrueTypeFont.java:673) at org.apache.pdfbox.pdmodel.font.PDSimpleFont.getFontWidth(PDSimpleFont.java:231) at org.apache.pdfbox.pdmodel.font.PDSimpleFont.getSpaceWidth(PDSimpleFont.java:533) at org.apache.pdfbox.util.PDFStreamEngine.processEncodedText(PDFStreamEngine.java:355) at org.apache.pdfbox.util.operator.ShowTextGlyph.process(ShowTextGlyph.java:62) at org.apache.pdfbox.util.PDFStreamEngine.processOperator(PDFStreamEngine.java:557) at org.apache.pdfbox.util.PDFStreamEngine.processSubStream(PDFStreamEngine.java:268) at org.apache.pdfbox.util.PDFStreamEngine.processSubStream(PDFStreamEngine.java:235) at org.apache.pdfbox.util.PDFStreamEngine.processStream(PDFStreamEngine.java:215) at org.apache.pdfbox.util.PDFTextStripper.processPage(PDFTextStripper.java:458) at org.apache.pdfbox.util.PDFTextStripper.processPages(PDFTextStripper.java:383) at org.apache.pdfbox.util.PDFTextStripper.writeText(PDFTextStripper.java:342) at org.apache.tika.parser.pdf.PDF2XHTML.process(PDF2XHTML.java:148) at org.apache.tika.parser.pdf.PDFParser.parse(PDFParser.java:148) at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:280) at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120) at org.apache.tika.Tika.parseToString(Tika.java:537)&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;This might have been solved by PDFParser with &lt;a href=&quot;https://issues.apache.org/jira/browse/PDFBOX-3997&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/PDFBOX-3997&lt;/a&gt; which is available in PDFBox 2.0.9 but Tika 1.17 is still using 2.0.8. See related issue &lt;a href=&quot;https://issues.apache.org/jira/browse/PDFBOX-3985&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/PDFBOX-3985&lt;/a&gt;. Unclear if that will actually fix the problem reported but FWIW upgrading to 2.0.9 of PDFBox could be useful.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 22 Feb 2018 14:33:02 +0000" id="484" opendate="Wed, 21 Feb 2018 20:21:28 +0000">
		<buginformation>
			<summary>SafeContentHandler documentation is incorrect about replacement character</summary>
			<description>&lt;p&gt;SafeContentHandler's doc comment states &quot;All invalid characters are replaced with spaces.&quot;  This has been untrue since &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-698&quot; title=&quot;&amp;quot;Invalid UTF-16 surrogate detected:&amp;quot; parsing PowerPoint 97-2003&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-698&quot;&gt;&lt;del&gt;TIKA-698&lt;/del&gt;&lt;/a&gt; (Sep 2011).&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/sax/SafeContentHandler.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 21 Mar 2018 08:41:39 +0000" id="485" opendate="Wed, 21 Feb 2018 21:41:58 +0000">
		<buginformation>
			<summary>TikaInputStream support for resetting via a factory of InputStreams</summary>
			<description>&lt;p&gt;As raised in the 2.0 breaking changes thread, currently the only way that Tika has of handling the need to fully read an InputStream multiple times is to use &lt;tt&gt;TikaInputStream.getFile()&lt;/tt&gt; which will spool to a temp file if not already file-based. (Reading a few kb is handled via buffering and mark/reset, but that doesn't scale for huge full files)&lt;/p&gt;

&lt;p&gt;In some cases, grabbing a fresh &lt;tt&gt;InputStream&lt;/tt&gt; is actually cheaper than Tika spooling to a temp file, but we've no way of a caller expressing that&lt;/p&gt;

&lt;p&gt;So, before we make too much extra use of re-processing the whole input several times (eg for the augmenting-parsers and fallback-parsers), we should provide a way for callers to instead supply new &lt;tt&gt;InputStream&lt;/tt&gt; instances on demand&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/io/InputStreamFactory.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 26 Feb 2018 20:26:11 +0000" id="486" opendate="Sat, 24 Feb 2018 16:00:07 +0000">
		<buginformation>
			<summary>Tika detecting/parsing pptx with embedded Excel worksheet(s)...</summary>
			<description>&lt;p&gt;Hello tika-developers,&lt;/p&gt;

&lt;p&gt;First, a big 'thank-you' for creating and maintaining Apache-Tika!  A really useful capability/service that can be used in so many different ways.  You folks are the true Debabelizer (h2g2.com).&lt;/p&gt;

&lt;p&gt;On to issue-encountered: using Tika 1.17 to extract an embedded Excel object out of a pptx is causing issues.  Simple example attached to this Jira-issue (&lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12911913/12911913_tikaSample.pptx&quot; title=&quot;tikaSample.pptx attached to TIKA-2588&quot;&gt;tikaSample.pptx&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt;) which if run against Tika 1.17 (with verbose/list-parsers/list-detectors) provides the output in (&lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12911912/12911912_foo.out&quot; title=&quot;foo.out attached to TIKA-2588&quot;&gt;foo.out&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt;).  The deck contains a title slide, and a single-slide with embedded Excel object on it.&lt;/p&gt;

&lt;p&gt;As noted to &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gagravarr&quot; class=&quot;user-hover&quot; rel=&quot;gagravarr&quot;&gt;Nick Burch&lt;/a&gt; on S-Overflow, I grabbed the unit-test data which you use in your parser/office JUnit suite (test_ppt_embedded_two_slides.pptx) and tried opening in Office/PPT 2016.  I selected (with mouse) the embedded sheet (had Alfresco logo in it) and pasted it into an empty Office/Excel 2016 workbook.  When I tried to interact with it, I had to double-click to make it active.  As a result, I ended up with two Excel instances on my Windows 10 desktop (the original object in 1, the Excel worksheet in another).  I have included a picture of the embedded Excel object pasted into the workbook...  &lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/12911911/12911911_pptEmbedExcelInEmptyWorkbook.PNG&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt; ).&lt;/p&gt;

&lt;p&gt;followed by the worksheet opened inside the workbook (required double-click within the black-bordered area in the first pic above):&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/12911910/12911910_pptEmbedExcelDoubleClickFromWorkbook.PNG&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;I managed to extract the embedded object using apache POI.  The logic sequence was something like the following:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Create an XMLSlideShow object, and pull the list of underlying slide entities.&lt;/li&gt;
	&lt;li&gt;Walk the list of XSLFSlide(s), searching for a matching slide (by name) - e.g. 'MFL'.&lt;/li&gt;
	&lt;li&gt;Examine PackagePart of XSLFSlide (matching name) and for content-type.&lt;/li&gt;
	&lt;li&gt;If pPart.content-type is 'application/vnd.openxmlformats-officedocument.oleObject' then - 'candidate FOUND'.&lt;/li&gt;
	&lt;li&gt;Build POIFS around the candidate FOUND, extract root of FileSystem.&lt;/li&gt;
	&lt;li&gt;Verify that root has entries for { 'Package', '\u0001Ole', and '\u0001CompObj' }.&lt;/li&gt;
	&lt;li&gt;Extract entry '\u0001CompObj', verify entry is a DocumentEntry and underlying bytes for DocumentNode match an 'Excel' signature.&lt;/li&gt;
	&lt;li&gt;If (step 7 is true) -&amp;gt; extract entry 'Package'.&lt;/li&gt;
	&lt;li&gt;The resulting entry represents the byte-stream of the embedded Excel entity.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;I was able to instantiate this into a new workbook (as an example) using POI, and when I opened it, the worksheet was correctly embedded in that 'example.xlsx'.&lt;/p&gt;

&lt;p&gt;I am not as familiar with Tika, so was a little less comfortable trying to walk it through.  I thought however, recreating this path would provide further insight for you.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 7 Mar 2018 20:20:32 +0000" id="487" opendate="Tue, 27 Feb 2018 11:58:35 +0000">
		<buginformation>
			<summary>ExcelExtractor: cannot choose listening to the selected records only</summary>
			<description>&lt;p&gt;The listenForAllRecords argument is being always reset to 'true', so the 'else' branch is never reached. It may cause incorrect text extraction when records with certain unsupported types (e.g. SharedFormula) are present in a file.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
        &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void processFile(DirectoryNode root, &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; listenForAllRecords)
                &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException, SAXException, TikaException {

            &lt;span class=&quot;code-comment&quot;&gt;// Set up listener and register the records we want to process
&lt;/span&gt;            HSSFRequest hssfRequest = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; HSSFRequest();
            listenForAllRecords = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;;
            &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (listenForAllRecords) {
                hssfRequest.addListenerForAllRecords(formatListener);
            } &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
                hssfRequest.addListener(formatListener, BOFRecord.sid);
                hssfRequest.addListener(formatListener, EOFRecord.sid);
                hssfRequest.addListener(formatListener, DateWindow1904Record.sid);
                hssfRequest.addListener(formatListener, CountryRecord.sid);
                hssfRequest.addListener(formatListener, BoundSheetRecord.sid);
                hssfRequest.addListener(formatListener, SSTRecord.sid);
                hssfRequest.addListener(formatListener, FormulaRecord.sid);
                hssfRequest.addListener(formatListener, LabelRecord.sid);
                hssfRequest.addListener(formatListener, LabelSSTRecord.sid);
                hssfRequest.addListener(formatListener, NumberRecord.sid);
                hssfRequest.addListener(formatListener, RKRecord.sid);
                hssfRequest.addListener(formatListener, StringRecord.sid);
                hssfRequest.addListener(formatListener, HyperlinkRecord.sid);
                hssfRequest.addListener(formatListener, TextObjectRecord.sid);
                hssfRequest.addListener(formatListener, SeriesTextRecord.sid);
                hssfRequest.addListener(formatListener, FormatRecord.sid);
                hssfRequest.addListener(formatListener, ExtendedFormatRecord.sid);
                hssfRequest.addListener(formatListener, DrawingGroupRecord.sid);
                &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (extractor.officeParserConfig.getIncludeHeadersAndFooters()) {
                    hssfRequest.addListener(formatListener, HeaderRecord.sid);
                    hssfRequest.addListener(formatListener, FooterRecord.sid);
                }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ExcelExtractor.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 7 Mar 2018 20:14:13 +0000" id="488" opendate="Wed, 28 Feb 2018 08:14:02 +0000">
		<buginformation>
			<summary>HTML with charset unicode handled as utf-16 instead utf-8</summary>
			<description>&lt;p&gt;HTML files are detected as utf-16 when meta content is set to &quot;unicode&quot;.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-xml&quot;&gt;
&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;meta http-equiv=&lt;span class=&quot;code-quote&quot;&gt;&quot;Content-Type&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;text/html; charset=&quot;&lt;/span&gt;unicode&quot;&amp;gt;&lt;/span&gt;
 &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt; &lt;br/&gt;
Shouldn't the default be utf-8?&lt;/p&gt;

&lt;p&gt;The attached sample file is shown correctly in:&lt;br/&gt;
Chromium Version 55.0.2883.75&lt;br/&gt;
Firefox 50.1.0&lt;br/&gt;
IE 11&lt;/p&gt;


&lt;p&gt;I am aware that there is no charset &quot;unicode&quot; (available character encodings: &lt;a href=&quot;http://www.iana.org/assignments/character-sets/character-sets.xhtml&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.iana.org/assignments/character-sets/character-sets.xhtml&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Unfortunately there are many wrong encodings used out there.&lt;/p&gt;

&lt;p&gt;All unknown encodings should be validated or at least be set to default utf-8.&lt;/p&gt;


&lt;p&gt;Regards &lt;/p&gt;

&lt;p&gt;Andreas&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 7 Mar 2018 20:13:10 +0000" id="489" opendate="Thu, 1 Mar 2018 07:55:17 +0000">
		<buginformation>
			<summary>Mail detected as application/xhtml+xml</summary>
			<description>&lt;p&gt;The attached mail (message/rfc822) with inline xhtml is recognized as application/xhtml+xml&lt;/p&gt;


&lt;p&gt;Regards&lt;/p&gt;

&lt;p&gt;Andreas&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 6 Mar 2018 20:21:44 +0000" id="490" opendate="Mon, 5 Mar 2018 10:23:59 +0000">
		<buginformation>
			<summary>Fix dependency convergence</summary>
			<description>&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;We tried to upgrade Tika to 1.17 in Hibernate Search and we had some dependency convergence issues:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Dependency convergence error &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; com.healthmarketscience.jackcess:jackcess:2.1.8 paths to dependency are:
+-org.hibernate:hibernate-search-engine:5.10.0-SNAPSHOT
    +-org.apache.tika:tika-parsers:1.17
         +-com.healthmarketscience.jackcess:jackcess:2.1.8
and
+-org.hibernate:hibernate-search-engine:5.10.0-SNAPSHOT
     +-org.apache.tika:tika-parsers:1.17
         +-com.healthmarketscience.jackcess:jackcess-encrypt:2.1.2
             +-com.healthmarketscience.jackcess:jackcess:2.1.0
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We could fix them downstream in Hibernate Search but I thought it would be better if Tika could ensure the convergence of its dependencies using the Maven enforcer plugin so that all the downstream projects can benefit from it.&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-dl/pom.xml</file>
			<file>/tika-batch/pom.xml</file>
			<file>/tika-langdetect/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 29 Oct 2018 22:08:26 +0000" id="491" opendate="Wed, 7 Mar 2018 03:27:34 +0000">
		<buginformation>
			<summary>Hyperlink surrounded by Italics not closed Properly</summary>
			<description>&lt;p&gt;If a Word document contains a hyperlink surrounded by italicized text, the resulting xhtml is:&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;&amp;lt;p&amp;gt;&amp;lt;i&amp;gt;Italic Test before link &amp;lt;a href=&quot;http://www.google.com&quot;/&amp;gt;&amp;lt;b&amp;gt;&amp;lt;i&amp;gt;&amp;lt;u&amp;gt;hyperlink italics&amp;lt;/u&amp;gt;&amp;lt;/i&amp;gt;&amp;lt;/b&amp;gt;&amp;lt;/a&amp;gt;&amp;lt;i&amp;gt; Italic text after hyperlink&amp;lt;/i&amp;gt;&amp;lt;/p&amp;gt;&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;The opening italics tag is not closed which is not valid XHTML.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/WordExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 7 Mar 2018 14:51:08 +0000" id="492" opendate="Wed, 7 Mar 2018 13:33:08 +0000">
		<buginformation>
			<summary>Don't use md5 checksum due to changes to the release distribuition policy</summary>
			<description>&lt;p&gt;To plagiarize from &lt;a href=&quot;https://issues.apache.org/jira/browse/PDFBOX-4142&quot; title=&quot;Don&amp;#39;t use md5 checksum due to changes to the release distribuition policy&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PDFBOX-4142&quot;&gt;&lt;del&gt;PDFBOX-4142&lt;/del&gt;&lt;/a&gt;:&lt;br/&gt;
The release distribution policy was changes with regard to the checksums to be used:&lt;/p&gt;

&lt;p&gt;Old policy :&lt;/p&gt;

&lt;p&gt;MUST provide a MD5-file&lt;br/&gt;
SHOULD provide a SHA-file &lt;span class=&quot;error&quot;&gt;&amp;#91;SHA-512 recommended&amp;#93;&lt;/span&gt;&lt;br/&gt;
New policy :&lt;/p&gt;

&lt;p&gt;MUST provide a SHA- or MD5-file&lt;br/&gt;
SHOULD provide a SHA-file&lt;br/&gt;
SHOULD NOT provide a MD5-file&lt;br/&gt;
see &lt;a href=&quot;http://www.apache.org/dev/release-distribution&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.apache.org/dev/release-distribution&lt;/a&gt; for further details&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 9 Mar 2018 18:11:16 +0000" id="493" opendate="Fri, 9 Mar 2018 08:33:15 +0000">
		<buginformation>
			<summary>Error with certain jar paths on OS X</summary>
			<description>&lt;p&gt;I've been developing an R interface to the Tika batch processor for the past month ( see: &lt;a href=&quot;https://github.com/predict-r/rtika&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/predict-r/rtika&lt;/a&gt; ), and this software is awesome. I use the command line to call the batch processor, and my code has worked on Ubuntu, Windows 10 and OS X. Several people have been testing my code as well. Its been working.&lt;/p&gt;

&lt;p&gt;A few days ago I found an issue with the batch processor on OS X. &lt;/p&gt;

&lt;p&gt;When calling the batch processor with the tika-app-1.17.jar on a path with spaces in it, Tika starts to continually restart.&lt;/p&gt;

&lt;p&gt;Here is an example of calling the jar &lt;b&gt;when the path has spaces.&lt;/b&gt; It &lt;b&gt;produces this error, and the unexpected restarts&lt;/b&gt;: &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
java -Djava.awt.headless=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; -jar &lt;span class=&quot;code-quote&quot;&gt;'/Users/sasha/Downloads/space folder/tika-app.jar'&lt;/span&gt; -maxRestarts 1 -t -i &lt;span class=&quot;code-quote&quot;&gt;'/'&lt;/span&gt; -o &lt;span class=&quot;code-quote&quot;&gt;'/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/folders/nr/74rgb64s3n98yccxwbv6vsxw0000gn/T/Rtmp9VEJvX/rtika_dircf81200b313e'&lt;/span&gt; -fileList &lt;span class=&quot;code-quote&quot;&gt;'/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/folders/nr/74rgb64s3n98yccxwbv6vsxw0000gn/T/Rtmp9VEJvX/rtika_filecf81530d27ee'&lt;/span&gt;

INFO about to start driver
INFO BatchProcess: Error: Could not find or load main &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.tika.batch.fs.FSBatchProcessCLI
INFO BatchProcess: Caused by: java.lang.ClassNotFoundException: org.apache.tika.batch.fs.FSBatchProcessCLI
INFO The child process has finished with an exit value of: 1
WARN Restarting on unexpected restart code: 1
WARN Must restart process (exitValue=1 numRestarts=0 receivedRestartMessage=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;)
INFO BatchProcess: Error: Could not find or load main &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.tika.batch.fs.FSBatchProcessCLI
INFO BatchProcess: Caused by: java.lang.ClassNotFoundException: org.apache.tika.batch.fs.FSBatchProcessCLI
INFO The child process has finished with an exit value of: 1
WARN Restarting on unexpected restart code: 1
WARN Hit the maximum number of process restarts. Driver is shutting down now.
INFO &lt;span class=&quot;code-object&quot;&gt;Process&lt;/span&gt; driver has completed&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The error ALSO occurs with double quotes also around the jar.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Now, in contrast,&lt;/b&gt; calling the jar when the &lt;b&gt;path does not have spaces produces absolutely NO error&lt;/b&gt;:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
java -Djava.awt.headless=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; -jar &lt;span class=&quot;code-quote&quot;&gt;'/Users/sasha/Downloads/tika-app.jar'&lt;/span&gt; -maxRestarts 1 -t -i &lt;span class=&quot;code-quote&quot;&gt;'/'&lt;/span&gt; -o &lt;span class=&quot;code-quote&quot;&gt;'/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/folders/nr/74rgb64s3n98yccxwbv6vsxw0000gn/T/Rtmp9VEJvX/rtika_dircf81200b313e'&lt;/span&gt; -fileList &lt;span class=&quot;code-quote&quot;&gt;'/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/folders/nr/74rgb64s3n98yccxwbv6vsxw0000gn/T/Rtmp9VEJvX/rtika_filecf81530d27ee'&lt;/span&gt;
INFO about to start driver
INFO BatchProcess: log4j:WARN No appenders could be found &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; logger (org.apache.tika.batch.fs.FSBatchProcessCLI).
INFO BatchProcess: log4j:WARN Please initialize the log4j system properly.
INFO BatchProcess: log4j:WARN See http:&lt;span class=&quot;code-comment&quot;&gt;//logging.apache.org/log4j/1.2/faq.html#noconfig &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; more info.
&lt;/span&gt;INFO BatchProcess: Mar 09, 2018 12:19:17 AM org.apache.tika.config.InitializableProblemHandler$3 handleInitializableProblem
INFO BatchProcess: WARNING: JBIG2ImageReader not loaded. jbig2 files will be ignored
INFO BatchProcess: See https:&lt;span class=&quot;code-comment&quot;&gt;//pdfbox.apache.org/2.0/dependencies.html#jai-image-io
&lt;/span&gt;INFO BatchProcess: &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; optional dependencies.
INFO BatchProcess: TIFFImageWriter not loaded. tiff files will not be processed
INFO BatchProcess: See https:&lt;span class=&quot;code-comment&quot;&gt;//pdfbox.apache.org/2.0/dependencies.html#jai-image-io
&lt;/span&gt;INFO BatchProcess: &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; optional dependencies.
INFO BatchProcess: J2KImageReader not loaded. JPEG2000 files will not be processed.
INFO BatchProcess: See https:&lt;span class=&quot;code-comment&quot;&gt;//pdfbox.apache.org/2.0/dependencies.html#jai-image-io
&lt;/span&gt;INFO BatchProcess: &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; optional dependencies.
INFO BatchProcess:
INFO BatchProcess: Mar 09, 2018 12:19:17 AM org.apache.tika.config.InitializableProblemHandler$3 handleInitializableProblem
INFO BatchProcess: WARNING: org.xerial's sqlite-jdbc is not loaded.
INFO BatchProcess: Please provide the jar on your classpath to parse sqlite files.
INFO BatchProcess: See tika-parsers/pom.xml &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; the correct version.
INFO BatchProcess: randomCrawl attribute is ignored by FSListCrawler
BatchProcess:Main thread in TikaFSBatchCLI has finished processing. BatchProcess: BatchProcess: BatchProcess:ParallelFileProcessingResult{considered=1, added=1, consumed=1, numberHandledExceptions=0, secondsElapsed=0.853, exitStatus=0, causeForTermination=&lt;span class=&quot;code-quote&quot;&gt;'COMPLETED_NORMALLY'&lt;/span&gt;}
INFO The child process has finished with an exit value of: 0
INFO &lt;span class=&quot;code-object&quot;&gt;Process&lt;/span&gt; driver has completed&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Further, and what makes this a batch processor issue, is that that path with the space in it produces absolutely &lt;b&gt;NO error in the normal Tika CLI mode either&lt;/b&gt;:  &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
java -jar &lt;span class=&quot;code-quote&quot;&gt;'/Users/sasha/Downloads/space folder/tika-app.jar'&lt;/span&gt; -t /Library/Frameworks/R.framework/Versions/3.4/Resources/library/rtika/extdata/jsonlite.pdf

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The last two examples work, but the first does not. &lt;/p&gt;

&lt;p&gt;The only difference is the first is calling the batch processor, and that is causing restarts with whatever file.&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/main/java/org/apache/tika/cli/BatchCommandLineBuilder.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 28 Mar 2018 16:49:57 +0000" id="494" opendate="Wed, 28 Mar 2018 14:34:31 +0000">
		<buginformation>
			<summary>RFC822 treats non-multipart as attachment</summary>
			<description>&lt;p&gt;Found during regression testing in prep for 1.18, now that we're identifying a lot more rfc822...for those that have no multipart, we need to treat the body &quot;inline&quot; and not as an attachment.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/mail/MailContentHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 28 Mar 2018 16:49:41 +0000" id="495" opendate="Wed, 28 Mar 2018 15:15:24 +0000">
		<buginformation>
			<summary>message/news now incorrectly identified as rfc822</summary>
			<description>&lt;p&gt;Thanks to &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gagravarr&quot; class=&quot;user-hover&quot; rel=&quot;gagravarr&quot;&gt;Nick Burch&lt;/a&gt; on the dev list for confirming, this is a regression.  Let's move the priority for message-id in rfc822 lower to preserve &lt;tt&gt;message/news&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;e.g.: &lt;a href=&quot;http://162.242.228.174/docs/commoncrawl2/VG/VGXYD2ISNSDJAVMK6CK7DHB3KI6ZHB6L&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://162.242.228.174/docs/commoncrawl2/VG/VGXYD2ISNSDJAVMK6CK7DHB3KI6ZHB6L&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 28 Mar 2018 16:49:22 +0000" id="496" opendate="Wed, 28 Mar 2018 16:22:38 +0000">
		<buginformation>
			<summary>Ignore NPOIFS IOOBE in PPT attachments</summary>
			<description>&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-2588&quot; title=&quot;Tika detecting/parsing pptx with embedded Excel worksheet(s)...&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-2588&quot;&gt;&lt;del&gt;TIKA-2588&lt;/del&gt;&lt;/a&gt; has us trying to parse more embedded streams as npoifs.  Some of these are throwing IOOBE in our regression set.  Rather than throw a runtime exception while trying to parse an embedded stream, let's treat this like any other embedded stream IOException.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/HSLFExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 28 Mar 2018 19:27:44 +0000" id="497" opendate="Wed, 28 Mar 2018 18:52:42 +0000">
		<buginformation>
			<summary>LabelRecord and LabelSSTRecord text can be overwritten in xls</summary>
			<description>&lt;p&gt;In our regression tests, we've lost small amounts of text from quite a few xls (standalone, but especially embedded).  This is somewhat caused by removing the &lt;tt&gt;listenForAllRecords=true&lt;/tt&gt; that I accidentally left in as part of debugging something a while ago. When that is true, we don't cache the records in currentSheet, so they are added to the &lt;tt&gt;extraTextCells&lt;/tt&gt; list.  When that is false, which is now the default, the &lt;tt&gt;LabelRecord&lt;/tt&gt; and &lt;tt&gt;LabelSSTRecord&lt;/tt&gt; are sometimes being overwritten because multiple cells can have the same x/y coordinates in the &lt;tt&gt;currentSheet&lt;/tt&gt; map.&lt;/p&gt;

&lt;p&gt;When &lt;tt&gt;listenForAllRecords=false&lt;/tt&gt;, we're trying to listen for labels, but we're often overwriting them because of the map.&lt;/p&gt;

&lt;p&gt;Let's add labels to &lt;tt&gt;extraTextCells&lt;/tt&gt; so that at least the text is processed.&lt;/p&gt;

&lt;p&gt;As one example: &quot;africa&quot; in govdocs1/199/199294.ppt&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ExcelExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 30 Mar 2018 10:50:15 +0000" id="498" opendate="Thu, 29 Mar 2018 13:00:17 +0000">
		<buginformation>
			<summary>Set sys property to get better rendering speed by default</summary>
			<description>&lt;p&gt;After upgrading to PDFBox 2.0.9, we now get a logged warning:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;INFO  To get higher rendering speed on JDK8 or later,
INFO    use the option -Dsun.java2d.cmm=sun.java2d.cmm.kcms.KcmsServiceProvider
INFO    or call System.setProperty(&quot;sun.java2d.cmm&quot;, &quot;sun.java2d.cmm.kcms.KcmsServiceProvider&quot;)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Unless there are objections, I'll add a static call to the PDFParser to &lt;tt&gt;System.setProperty...&lt;/tt&gt;.  &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDFParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 30 Mar 2018 10:50:15 +0000" id="499" opendate="Thu, 29 Mar 2018 13:12:56 +0000">
		<buginformation>
			<summary>Brotli support</summary>
			<description>&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I've got some documents &lt;a href=&quot;https://en.wikipedia.org/wiki/Brotli&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;Brotli&lt;/a&gt; encoded. Tika works perfectly with documents gzip encoded. I've &lt;a href=&quot;https://github.com/apache/tika/commit/77900ab626a2a05700cadf46f090966295c29149&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;found a commit&lt;/a&gt; looks for me like a brotli support, but it doesn't work for me with Tika 1.17. Do you plan to add brotli support the same way the gzip works?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 6 Apr 2018 18:54:18 +0000" id="500" opendate="Fri, 6 Apr 2018 15:16:40 +0000">
		<buginformation>
			<summary>EmbeddedDocUtil not correctly handling doubly decorated parsers in tryToFindExistingLeafParser</summary>
			<description>&lt;p&gt;In reviewing diffs with rfc822 in Tika 1.17 and Tika 1.18-SNAPSHOT, there is a subtle bug that prevents extraction of text when the AutoDetectParser is wrapped in a DigestingParser in a RecursiveParserWrapper&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/test/java/org/apache/tika/extractor/TestEmbeddedDocumentUtil.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 6 Apr 2018 18:54:40 +0000" id="501" opendate="Fri, 6 Apr 2018 15:19:17 +0000">
		<buginformation>
			<summary>RFC822Parser crazily slower because of creation of new Detector on each file</summary>
			<description>&lt;p&gt;RFC822 parser is crazily slower in 1.18-SNAPSHOT. Better to use regexes to determine html vs text than to create a new Detector for every file.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/mail/MailContentHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 19 Apr 2018 14:58:18 +0000" id="502" opendate="Thu, 19 Apr 2018 12:38:12 +0000">
		<buginformation>
			<summary>Upgrade Jackson to 2.9.5</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 19 Apr 2018 15:21:53 +0000" id="503" opendate="Thu, 19 Apr 2018 14:38:49 +0000">
		<buginformation>
			<summary>Require imageMagick path be specified on Windows OS</summary>
			<description>&lt;p&gt;Our optional image preprocessing with imagemagick can run into problems on Windows machines where the executable `convert` is a system command, not the imagemagick executable.&lt;/p&gt;

&lt;p&gt;I propose that on Windows, we require users to specify a path for imagemagick.&lt;/p&gt;

&lt;p&gt;If there are other system 'convert' commands on other operating systems, should we require that imagemagick be in the path?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/ocr/TesseractOCRParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 9 Oct 2018 19:07:20 +0000" id="504" opendate="Sun, 16 Sep 2018 12:49:09 +0000">
		<buginformation>
			<summary>Parsing and detect mime type of XML file stuck in infinite loop</summary>
			<description>&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I'm trying to parse (even mime type detect) some XML file that it's not large, but kinda tricky and my process hangs on :&lt;/p&gt;

&lt;p&gt;XMLStringBuffer.append(char[], int, int) line: not available &lt;br/&gt;
XMLStringBuffer.append(XMLString) line: not available &lt;br/&gt;
XMLNSDocumentScannerImpl(XMLScanner).scanAttributeValue(XMLString, XMLString, String, boolean, String) line: not available &lt;br/&gt;
XMLNSDocumentScannerImpl.scanAttribute(XMLAttributesImpl) line: not available &lt;br/&gt;
XMLNSDocumentScannerImpl.scanStartElement() line: not available &lt;br/&gt;
XMLNSDocumentScannerImpl$NSContentDispatcher.scanRootElementHook() line: not available &lt;br/&gt;
XMLNSDocumentScannerImpl$NSContentDispatcher(XMLDocumentFragmentScannerImpl$FragmentContentDispatcher).dispatch(boolean) line: not available &lt;br/&gt;
XMLNSDocumentScannerImpl(XMLDocumentFragmentScannerImpl).scanDocument(boolean) line: not available &lt;br/&gt;
XIncludeAwareParserConfiguration(XML11Configuration).parse(boolean) line: not available &lt;br/&gt;
XIncludeAwareParserConfiguration(XML11Configuration).parse(XMLInputSource) line: not available &lt;br/&gt;
SAXParserImpl$JAXPSAXParser(XMLParser).parse(XMLInputSource) line: not available &lt;br/&gt;
SAXParserImpl$JAXPSAXParser(AbstractSAXParser).parse(InputSource) line: not available &lt;br/&gt;
SAXParserImpl$JAXPSAXParser.parse(InputSource) line: not available &lt;br/&gt;
SAXParserImpl.parse(InputSource, DefaultHandler) line: not available &lt;br/&gt;
SAXParserImpl(SAXParser).parse(InputStream, DefaultHandler) line: 195 &lt;br/&gt;
XmlRootExtractor.extractRootElement(InputStream) line: 62 &lt;br/&gt;
XmlRootExtractor.extractRootElement(byte[]) line: 42 &lt;br/&gt;
MimeTypes.getMimeType(byte[]) line: 212 &lt;br/&gt;
MimeTypes.detect(InputStream, Metadata) line: 494 &lt;br/&gt;
DefaultDetector(CompositeDetector).detect(InputStream, Metadata) line: 84&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;Please see attached XML file.&lt;/p&gt;

&lt;p&gt;Please advise.&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/utils/XMLReaderUtils.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 14 Nov 2017 15:58:08 +0000" id="505" opendate="Fri, 15 Jul 2016 13:59:36 +0000">
		<buginformation>
			<summary>Upgrade XMPCore to 5.1.3</summary>
			<description>&lt;p&gt;To avoid potential &lt;a href=&quot;https://helpx.adobe.com/security/products/xmpcore/apsb16-24.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;XXE&lt;/a&gt;.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 5 Jul 2017 11:56:29 +0000" id="506" opendate="Wed, 5 Jul 2017 22:46:23 +0000">
		<buginformation>
			<summary>Jackcess toSQLString throws UnsupportedOperationException for unknown query type</summary>
			<description>&lt;p&gt;In JackcessExtractor when calling toSQLString on a query with an unknown type Jackcess throws an UnsupportedOperationException (see com.healthmarketscience.jackcess.impl.query.QueryImpl.UnknownQueryImpl.toSQLString) An easy fix is to just check the query type before calling toSQLString.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/JackcessExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 18 Sep 2017 13:02:43 +0000" id="507" opendate="Thu, 13 Jul 2017 15:20:26 +0000">
		<buginformation>
			<summary>Upgrade to POI 3.17-final when available</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
			<file>/tika-eval/src/main/java/org/apache/tika/eval/reports/ResultsReporter.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 14 Jul 2017 12:43:02 +0000" id="508" opendate="Thu, 13 Jul 2017 21:27:08 +0000">
		<buginformation>
			<summary>Add at least dev test capability to run Tika against fuzzed files</summary>
			<description>&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lfcnassif&quot; class=&quot;user-hover&quot; rel=&quot;lfcnassif&quot;&gt;Luis Filipe Nassif&lt;/a&gt; observed on &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-2428&quot; title=&quot;EMFParser loops forever with corrupted files&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-2428&quot;&gt;&lt;del&gt;TIKA-2428&lt;/del&gt;&lt;/a&gt; that a corrupt file caused a permanent hang for the EMFParser.  Files can be corrupted for various reasons.  We can add some optional code to let people experiment with running Tika against randomly corrupted versions of the files in our test suite.  I suspect that this will unearth too many errors to start to be run on a regular basis.&lt;/p&gt;

&lt;p&gt;Let's at least add some code in tika-parsers to let devs run the tests.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/TestCorruptedFiles.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 24 Jul 2017 13:32:18 +0000" id="509" opendate="Mon, 24 Jul 2017 13:30:24 +0000">
		<buginformation>
			<summary>Upgrade to PDFBox 2.0.7</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 25 Jul 2017 14:23:14 +0000" id="510" opendate="Tue, 25 Jul 2017 13:10:48 +0000">
		<buginformation>
			<summary>Tika 1.16 - Nullpointer Exception after update - Asking for help</summary>
			<description>&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;i would like to kindly ask for help. We had to update to the latest Tika 1.16. I have no experience in Tika so far, i am just maintaining the configuration and application from another developer.&lt;/p&gt;

&lt;p&gt;Version 1.15 worked very fine for us. But right now i see following error (office is the name of our docker container, hence this output):&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/tika/blob/1.16/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java#L202&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/tika/blob/1.16/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java#L202&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;office     | java.lang.NullPointerException
office     | 	at org.apache.tika.cli.TikaCLI$OutputType.process(TikaCLI.java:202)
office     | 	at org.apache.tika.cli.TikaCLI$TikaServer$1.run(TikaCLI.java:1153)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I have checked the source on github and have seen, that this code part was changed with one of the latest commits before the 1.16 release (see link above).&lt;/p&gt;

&lt;p&gt;I checked the Change.txt at &lt;a href=&quot;https://tika.apache.org/1.16/index.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://tika.apache.org/1.16/index.html&lt;/a&gt;. As i haven't used Tika so far, and i cannot see that the CLI requirements changed from the release notes, i would like to ask, whether this is the case anyway. Do you have some hints on where to start, is this maybe due to improper cli usage? Or do you think there is a missing java package or dependency?&lt;/p&gt;

&lt;p&gt;It's hard for me to say, as the cli commands are automated and distributed over several layers and configuration files in the application stack, hence i am asking for a hint.&lt;/p&gt;

&lt;p&gt;Thx for any advice, best Karl.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 25 Jul 2017 14:23:14 +0000" id="511" opendate="Thu, 27 Jul 2017 16:14:32 +0000">
		<buginformation>
			<summary>Language detection slow, cpu intensive, CLI interrupts work</summary>
			<description>&lt;p&gt;Since version 1.16, when using tika -l FILE, it takes a lot longer than e.g. 1.15.&lt;/p&gt;

&lt;p&gt;Also, when batch processing a bunch of files in the background, the Java runtime icon pops up when processing the next file, stealing the input focus from whatever other application I'm currently working on, thus constantly interrupting my work.&lt;/p&gt;

&lt;p&gt;Also, the Java runtime uses from 100% to 400% CPU when executing Tika.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/main/java/org/apache/tika/cli/BatchCommandLineBuilder.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 2 Aug 2017 11:35:58 +0000" id="512" opendate="Wed, 2 Aug 2017 08:55:55 +0000">
		<buginformation>
			<summary>Test failure at OOXMLParserTest.testBigIntegersWGeneralFormat:1350-&gt;TikaTest.assertContains:102</summary>
			<description>&lt;p&gt;`mvn clean install` fails due to&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Failed tests: 
  OOXMLParserTest.testBigIntegersWGeneralFormat:1350-&amp;gt;TikaTest.assertContains:102 1.23456789012345E+15&amp;lt;/td&amp;gt;	&amp;lt;td&amp;gt;1.23456789012345E+15 not found in:
&amp;lt;html xmlns=&lt;span class=&quot;code-quote&quot;&gt;&quot;http:&lt;span class=&quot;code-comment&quot;&gt;//www.w3.org/1999/xhtml&quot;&lt;/span&gt;&amp;gt;
&lt;/span&gt;&amp;lt;head&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;date&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;2016-07-22T11:32:25Z&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;extended-properties:AppVersion&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;16.0300&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;dc:creator&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;Allison, Timothy B.&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;extended-properties:Company&quot;&lt;/span&gt; content=&quot;&quot; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;dcterms:created&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;2016-06-29T16:29:27Z&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;Last-Modified&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;2016-07-22T11:32:25Z&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;dcterms:modified&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;2016-07-22T11:32:25Z&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;Last-Save-Date&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;2016-07-22T11:32:25Z&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-keyword&quot;&gt;protected&lt;/span&gt;&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;meta:save-date&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;2016-07-22T11:32:25Z&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;Application-Name&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;Microsoft Excel&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;modified&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;2016-07-22T11:32:25Z&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;Content-Type&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;application/vnd.openxmlformats-officedocument.spreadsheetml.sheet&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;X-Parsed-By&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.tika.parser.DefaultParser&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;X-Parsed-By&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.tika.parser.microsoft.ooxml.OOXMLParser&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;creator&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;Allison, Timothy B.&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;meta:author&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;Allison, Timothy B.&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;meta:creation-date&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;2016-06-29T16:29:27Z&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;extended-properties:Application&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;Microsoft Excel&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;meta:last-author&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;Allison, Timothy B.&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;Creation-Date&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;2016-06-29T16:29:27Z&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;Last-Author&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;Allison, Timothy B.&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;Application-Version&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;16.0300&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;Author&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;Allison, Timothy B.&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;publisher&quot;&lt;/span&gt; content=&quot;&quot; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;dc:publisher&quot;&lt;/span&gt; content=&quot;&quot; /&amp;gt;
&amp;lt;title&amp;gt;&amp;lt;/title&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;&amp;lt;div&amp;gt;&amp;lt;h1&amp;gt;Sheet1&amp;lt;/h1&amp;gt;
&amp;lt;table&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;	&amp;lt;td&amp;gt;123456789012345&amp;lt;/td&amp;gt;	&amp;lt;td&amp;gt;123456789012346&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;
&amp;lt;tr&amp;gt;	&amp;lt;td&amp;gt;1,23456789012345E+15&amp;lt;/td&amp;gt;	&amp;lt;td&amp;gt;1,23456789012345E+15&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;
&amp;lt;tr /&amp;gt;
&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;
&amp;lt;/div&amp;gt;
&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;
  OOXMLParserTest.testXLSBVarious:1552-&amp;gt;TikaTest.assertContains:102 &amp;lt;td&amp;gt;13.1211231321&amp;lt;/td&amp;gt; not found in:
&amp;lt;html xmlns=&lt;span class=&quot;code-quote&quot;&gt;&quot;http:&lt;span class=&quot;code-comment&quot;&gt;//www.w3.org/1999/xhtml&quot;&lt;/span&gt;&amp;gt;
&lt;/span&gt;&amp;lt;head&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;date&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;2017-03-10T14:58:49Z&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;extended-properties:AppVersion&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;16.0300&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;dc:creator&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;Allison, Timothy B.&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;extended-properties:Company&quot;&lt;/span&gt; content=&quot;&quot; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;dcterms:created&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;2017-03-09T12:24:26Z&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;Last-Modified&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;2017-03-10T14:58:49Z&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;dcterms:modified&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;2017-03-10T14:58:49Z&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;Last-Save-Date&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;2017-03-10T14:58:49Z&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-keyword&quot;&gt;protected&lt;/span&gt;&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;meta:save-date&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;2017-03-10T14:58:49Z&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;Application-Name&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;Microsoft Excel&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;modified&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;2017-03-10T14:58:49Z&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;Content-Type&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;application/vnd.ms-excel.sheet.binary.macroenabled.12&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;X-Parsed-By&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.tika.parser.DefaultParser&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;X-Parsed-By&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.tika.parser.microsoft.ooxml.OOXMLParser&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;creator&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;Allison, Timothy B.&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;meta:author&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;Allison, Timothy B.&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;meta:creation-date&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;2017-03-09T12:24:26Z&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;extended-properties:Application&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;Microsoft Excel&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;meta:last-author&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;Allison, Timothy B.&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;Creation-Date&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;2017-03-09T12:24:26Z&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;Last-Author&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;Allison, Timothy B.&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;X-TIKA:origResourceName&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;C:\Users\tallison\Desktop\working\xlsb\&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;Application-Version&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;16.0300&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;Author&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;Allison, Timothy B.&quot;&lt;/span&gt; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;publisher&quot;&lt;/span&gt; content=&quot;&quot; /&amp;gt;
&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;dc:publisher&quot;&lt;/span&gt; content=&quot;&quot; /&amp;gt;
&amp;lt;title&amp;gt;&amp;lt;/title&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;&amp;lt;div&amp;gt;&amp;lt;h1&amp;gt;mySheet1&amp;lt;/h1&amp;gt;
&amp;lt;table&amp;gt;&amp;lt;tbody&amp;gt;&amp;lt;tr&amp;gt;	&amp;lt;td&amp;gt;&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;&amp;lt;/td&amp;gt;	&amp;lt;td&amp;gt;This is a string&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;
&amp;lt;tr&amp;gt;	&amp;lt;td&amp;gt;integer&amp;lt;/td&amp;gt;	&amp;lt;td&amp;gt;13&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;
&amp;lt;tr&amp;gt;	&amp;lt;td&amp;gt;&lt;span class=&quot;code-object&quot;&gt;float&lt;/span&gt;&amp;lt;/td&amp;gt;	&amp;lt;td&amp;gt;13,1211231321&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;
&amp;lt;tr&amp;gt;	&amp;lt;td&amp;gt;currency&amp;lt;/td&amp;gt;	&amp;lt;td&amp;gt;$   3.03&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;
&amp;lt;tr&amp;gt;	&amp;lt;td&amp;gt;percent&amp;lt;/td&amp;gt;	&amp;lt;td&amp;gt;20%&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;
&amp;lt;tr&amp;gt;	&amp;lt;td&amp;gt;&lt;span class=&quot;code-object&quot;&gt;float&lt;/span&gt; 2&amp;lt;/td&amp;gt;	&amp;lt;td&amp;gt;13,12&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;
&amp;lt;tr&amp;gt;	&amp;lt;td&amp;gt;&lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;&amp;lt;/td&amp;gt;	&amp;lt;td&amp;gt;123456789012345&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;
&amp;lt;tr&amp;gt;	&amp;lt;td&amp;gt;longer &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;&amp;lt;/td&amp;gt;	&amp;lt;td&amp;gt;1,23456789012345E+15&amp;lt;/td&amp;gt;	&amp;lt;td&amp;gt;&amp;lt;br /&amp;gt;
Allison, Timothy B.: Allison, Timothy B.:
test comment2
&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;
&amp;lt;tr&amp;gt;	&amp;lt;td&amp;gt;fraction&amp;lt;/td&amp;gt;	&amp;lt;td&amp;gt;1/4&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;
&amp;lt;tr&amp;gt;	&amp;lt;td&amp;gt;date&amp;lt;/td&amp;gt;	&amp;lt;td&amp;gt;3/9/17&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;
&amp;lt;tr&amp;gt;	&amp;lt;td&amp;gt;comment&amp;lt;/td&amp;gt;	&amp;lt;td&amp;gt;contents&amp;lt;br /&amp;gt;
Allison, Timothy B.: Allison, Timothy B.:
test comment
&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;
&amp;lt;tr&amp;gt;	&amp;lt;td&amp;gt;hyperlink&amp;lt;/td&amp;gt;	&amp;lt;td&amp;gt;tika_link&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;
&amp;lt;tr&amp;gt;	&amp;lt;td&amp;gt;formula&amp;lt;/td&amp;gt;	&amp;lt;td&amp;gt;4&amp;lt;/td&amp;gt;	&amp;lt;td&amp;gt;2&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;
&amp;lt;tr&amp;gt;	&amp;lt;td&amp;gt;formulaErr&amp;lt;/td&amp;gt;	&amp;lt;td&amp;gt;ERROR&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;
&amp;lt;tr&amp;gt;	&amp;lt;td&amp;gt;formulaFloat&amp;lt;/td&amp;gt;	&amp;lt;td&amp;gt;0,5&amp;lt;/td&amp;gt;	&amp;lt;td&amp;gt;March&amp;lt;/td&amp;gt;	&amp;lt;td&amp;gt;April&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;
&amp;lt;tr&amp;gt;	&amp;lt;td&amp;gt;customFormat1&amp;lt;/td&amp;gt;	&amp;lt;td&amp;gt;   46/1963&amp;lt;/td&amp;gt;	&amp;lt;td&amp;gt;merchant1&amp;lt;/td&amp;gt;	&amp;lt;td&amp;gt;1&amp;lt;/td&amp;gt;	&amp;lt;td&amp;gt;3&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;
&amp;lt;tr&amp;gt;	&amp;lt;td&amp;gt;customFormat2&amp;lt;/td&amp;gt;	&amp;lt;td&amp;gt;  3/128&amp;lt;/td&amp;gt;	&amp;lt;td&amp;gt;merchant2&amp;lt;/td&amp;gt;	&amp;lt;td&amp;gt;2&amp;lt;/td&amp;gt;	&amp;lt;td&amp;gt;4&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;
&amp;lt;tr&amp;gt;	&amp;lt;td&amp;gt;text test&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;
&amp;lt;tr&amp;gt;	&amp;lt;td&amp;gt;&amp;lt;br /&amp;gt;
Allison, Timothy B.: Allison, Timothy B.:
comment1
&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;
&amp;lt;tr&amp;gt;	&amp;lt;td&amp;gt;&amp;lt;br /&amp;gt;
Allison, Timothy B.: Allison, Timothy B.:
comment2
&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;
&amp;lt;tr&amp;gt;	&amp;lt;td&amp;gt;&amp;lt;br /&amp;gt;
Allison, Timothy B.: Allison, Timothy B.:
comment3
&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;
&amp;lt;tr&amp;gt;	&amp;lt;td&amp;gt;the&amp;lt;/td&amp;gt;	&amp;lt;td&amp;gt;&amp;lt;br /&amp;gt;
Allison, Timothy B.: Allison, Timothy B.:
comment4 (end of row)
&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;
&amp;lt;tr&amp;gt;	&amp;lt;td&amp;gt;the&amp;lt;/td&amp;gt;	&amp;lt;td&amp;gt;&amp;lt;br /&amp;gt;
Allison, Timothy B.: Allison, Timothy B.:
comment5 between cells
&amp;lt;/td&amp;gt;	&amp;lt;td&amp;gt;quick&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;
&amp;lt;tr&amp;gt;	&amp;lt;td&amp;gt;comment6&amp;lt;br /&amp;gt;
Allison, Timothy B.: Allison, Timothy B.:
comment6 actually in cell
&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;
&amp;lt;tr&amp;gt;	&amp;lt;td&amp;gt;&amp;lt;br /&amp;gt;
Allison, Timothy B.: Allison, Timothy B.:
comment7 end of file
&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;
&amp;lt;tr&amp;gt;	&amp;lt;td&amp;gt;&amp;lt;br /&amp;gt;
Allison, Timothy B.: Allison, Timothy B.:
comment8 end of file&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;
&amp;lt;/tbody&amp;gt;&amp;lt;/table&amp;gt;
&amp;lt;p&amp;gt;OddLeftHeader OddCenterHeader OddRightHeader&amp;lt;/p&amp;gt;
&amp;lt;p&amp;gt;EvenLeftHeader EvenCenterHeader EvenRightHeader
&amp;lt;/p&amp;gt;
&amp;lt;p&amp;gt;FirstPageLeftHeader FirstPageCenterHeader FirstPageRightHeader&amp;lt;/p&amp;gt;
&amp;lt;p&amp;gt;OddLeftFooter OddCenterFooter OddRightFooter&amp;lt;/p&amp;gt;
&amp;lt;p&amp;gt;EvenLeftFooter EvenCenterFooter EvenRightFooter&amp;lt;/p&amp;gt;
&amp;lt;p&amp;gt;FirstPageLeftFooter FirstPageCenterFooter FirstPageRightFooter&amp;lt;/p&amp;gt;
&amp;lt;p&amp;gt;test textbox
&amp;lt;/p&amp;gt;
&amp;lt;a href=&lt;span class=&quot;code-quote&quot;&gt;&quot;http:&lt;span class=&quot;code-comment&quot;&gt;//lucene.apache.org/&quot;&lt;/span&gt;&amp;gt;http://lucene.apache.org/&amp;lt;/a&amp;gt;&amp;lt;p&amp;gt;myChartTitle&amp;lt;/p&amp;gt;
&lt;/span&gt;&amp;lt;p /&amp;gt;
merchant1	March	April	1	3	merchant2	March	April	2	4	&amp;lt;p /&amp;gt;
&amp;lt;p /&amp;gt;
&amp;lt;p /&amp;gt;
&amp;lt;p /&amp;gt;
&amp;lt;p&amp;gt;test WordArt&amp;lt;/p&amp;gt;
&amp;lt;p&amp;gt;myChartTitle&amp;lt;/p&amp;gt;
&amp;lt;p /&amp;gt;
merchant1	March	April	1	3	merchant2	March	April	2	4	&amp;lt;p /&amp;gt;
&amp;lt;p /&amp;gt;
&amp;lt;p /&amp;gt;
&amp;lt;p /&amp;gt;
&amp;lt;p&amp;gt;myChartTitle&amp;lt;/p&amp;gt;
&amp;lt;p /&amp;gt;
merchant1	March	April	1	3	merchant2	March	April	2	4	&amp;lt;p /&amp;gt;
&amp;lt;p /&amp;gt;
&amp;lt;p /&amp;gt;
&amp;lt;p /&amp;gt;
&amp;lt;a href=&lt;span class=&quot;code-quote&quot;&gt;&quot;http:&lt;span class=&quot;code-comment&quot;&gt;//tika.apache.org/&quot;&lt;/span&gt;&amp;gt;http://tika.apache.org/&amp;lt;/a&amp;gt;&amp;lt;/div&amp;gt;
&lt;/span&gt;&amp;lt;div class=&lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt;-entry&quot;&lt;/span&gt; /&amp;gt;&amp;lt;div class=&lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt;-entry&quot;&lt;/span&gt; /&amp;gt;&amp;lt;div class=&lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt;-entry&quot;&lt;/span&gt; /&amp;gt;&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;experienced with 1.16-75-g4455a6f08&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 29 Aug 2017 16:26:44 +0000" id="513" opendate="Tue, 8 Aug 2017 06:27:24 +0000">
		<buginformation>
			<summary>Phonetic strings handling for multilingual environments.</summary>
			<description>&lt;p&gt;Hi there,&lt;/p&gt;

&lt;p&gt;I would like to propose an idea to improve phonetic strings handling for multilingual environments. I believe Tika should not concatenate phonetic strings because text with phonetic strings is recognized as noisy text in most situations of natural language processing.&lt;/p&gt;

&lt;p&gt;Excel files include phonetic strings in some languages such as Japanese, Chinese and so on. Apache POI concatenates phonetic strings onto the shared strings when Tika extract text from Excel files.&lt;/p&gt;

&lt;p&gt;Recent Apache POI has an switch flag for phonetic strings concatination as follows:&lt;br/&gt;
&lt;a href=&quot;https://poi.apache.org/apidocs/org/apache/poi/xssf/eventusermodel/ReadOnlySharedStringsTable.html#ReadOnlySharedStringsTable(org.apache.poi.openxml4j.opc.OPCPackage,%20boolean&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://poi.apache.org/apidocs/org/apache/poi/xssf/eventusermodel/ReadOnlySharedStringsTable.html#ReadOnlySharedStringsTable(org.apache.poi.openxml4j.opc.OPCPackage,%20boolean&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Tika should set the 2nd argument &quot;includePhoneticRuns&quot; as false. Here is the simple patch for my idea.&lt;/p&gt;


&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;$ diff -ru XSSFExcelExtractorDecorator.java ./tika/tika-1.15/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/XSSFExcelExtractorDecorator.java
--- XSSFExcelExtractorDecorator.java    2017-06-10 19:13:33.355412625 +0900
+++ ./tika/tika-1.15/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/XSSFExcelExtractorDecorator.java 2017-06-10 19:14:30.452411830 +0900
@@ -130,7 +130,7 @@
             styles = xssfReader.getStylesTable();

             iter = (XSSFReader.SheetIterator) xssfReader.getSheetsData();
-            strings = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ReadOnlySharedStringsTable(container);
+            strings = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ReadOnlySharedStringsTable(container,&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;);
         } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (InvalidFormatException e) {
             &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; XmlException(e);
         } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (OpenXML4JException oe) {

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 23 Aug 2017 22:41:20 +0000" id="514" opendate="Wed, 23 Aug 2017 22:27:14 +0000">
		<buginformation>
			<summary>Windows BAT / CMD detection</summary>
			<description>&lt;p&gt;As reported on the mailing list - &lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/tika-user/201708.mbox/%3Cpony-9e932a35faed48a79f1e59e683b146c163eeef9a-854a10648fc2ca400c2ddb83e46514cfe44f163b%40user.tika.apache.org%3E&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://mail-archives.apache.org/mod_mbox/tika-user/201708.mbox/%3Cpony-9e932a35faed48a79f1e59e683b146c163eeef9a-854a10648fc2ca400c2ddb83e46514cfe44f163b%40user.tika.apache.org%3E&lt;/a&gt; - detection of Windows Batch files (.cmd and .bat) isn't working properly. These need pulling out to a common type, and some vague magic adding&lt;/p&gt;

&lt;p&gt;See also &lt;a href=&quot;https://stackoverflow.com/questions/148968/windows-batch-files-bat-vs-cmd&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://stackoverflow.com/questions/148968/windows-batch-files-bat-vs-cmd&lt;/a&gt; for the very minimal differences between .BAT and .CMD&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 6 Jun 2018 19:36:27 +0000" id="515" opendate="Thu, 24 Aug 2017 07:35:22 +0000">
		<buginformation>
			<summary>Tainted Zip file can provoke OOM errors</summary>
			<description>&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;using Tika 1.16 with embedded POI 3.17-beta1 we experienced an OutOfMemory error on a Zip file. The suspicious code is in the constructor of FakeZipEntry in line 125. Here a ByteArrayOutputStream of up to 2 GiB in size is opened which will most probably lead to an OutOfMemory. The entry size in the zip file can be easily faked by an attacker.&lt;/p&gt;

&lt;p&gt;The code path to FakeZipEntry will be used only if the native java.util.zip.ZipFile implementation already failed to open the (possibly corrupted) Zip. Possibly a more fine grained error analysis could be done in ZipPackage.&lt;/p&gt;

&lt;p&gt;I have attached a tweaked zip file that will provoke this error.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; FakeZipEntry(ZipEntry entry, InputStream inp) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
			&lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;(entry.getName());
			
			&lt;span class=&quot;code-comment&quot;&gt;// Grab the de-compressed contents &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; later
&lt;/span&gt;            ByteArrayOutputStream baos;

            &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; entrySize = entry.getSize();

            &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (entrySize !=-1) {
                &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (entrySize&amp;gt;=&lt;span class=&quot;code-object&quot;&gt;Integer&lt;/span&gt;.MAX_VALUE) {
                    &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; IOException(&lt;span class=&quot;code-quote&quot;&gt;&quot;ZIP entry size is too large&quot;&lt;/span&gt;);
                }

                baos = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ByteArrayOutputStream((&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;) entrySize);
            } &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
    			baos = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ByteArrayOutputStream();
            }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Kinds,&lt;/p&gt;

&lt;p&gt;Thorsten&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pkg/ZipContainerDetector.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 24 Aug 2017 18:03:56 +0000" id="516" opendate="Thu, 24 Aug 2017 14:31:59 +0000">
		<buginformation>
			<summary>PSDParser creates unnecessary large byte array and discards it</summary>
			<description>&lt;p&gt;PSD files (Adobe Photoshop) are split into ResourceBlock's which contain different data, but only Caption Blocks are currently extracted into the description.&lt;br/&gt;
Parsing a file with very big blocks, i.e. for image data, a byte array of the size of the block is allocated:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/tika/blob/master/tika-parsers/src/main/java/org/apache/tika/parser/image/PSDParser.java#L191&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/tika/blob/master/tika-parsers/src/main/java/org/apache/tika/parser/image/PSDParser.java#L191&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;even if it is discarded after that:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/tika/blob/master/tika-parsers/src/main/java/org/apache/tika/parser/image/PSDParser.java#L116&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/tika/blob/master/tika-parsers/src/main/java/org/apache/tika/parser/image/PSDParser.java#L116&lt;/a&gt; and following lines&lt;/p&gt;

&lt;p&gt;This causes huge memory consumption and finally killed the App with an OutOfMemoryError.&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;java.lang.OutOfMemoryError: Java heap space
        at org.apache.tika.parser.image.PSDParser$ResourceBlock.&amp;lt;init&amp;gt;(PSDParser.java:191) ~[tika-parsers-1.15.jar!/:1.15]
        at org.apache.tika.parser.image.PSDParser$ResourceBlock.&amp;lt;init&amp;gt;(PSDParser.java:141) ~[tika-parsers-1.15.jar!/:1.15]
        at org.apache.tika.parser.image.PSDParser.parse(PSDParser.java:116) ~[tika-parsers-1.15.jar!/:1.15]
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:280) ~[tika-core-1.15.jar!/:1.15]
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:280) ~[tika-core-1.15.jar!/:1.15]
        at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:135) ~[tika-core-1.15.jar!/:1.15]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I am not able to deliver a file to reproduce that, since the file which caused that issue is owned by one of our customers.&lt;br/&gt;
I will prepare a pull request to fix that.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/image/PSDParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 30 Aug 2017 16:38:35 +0000" id="517" opendate="Tue, 29 Aug 2017 17:22:03 +0000">
		<buginformation>
			<summary>Handle phonetic strings in the SAX docx parser</summary>
			<description>&lt;p&gt;On &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-2440&quot; title=&quot;Phonetic strings handling for multilingual environments.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-2440&quot;&gt;&lt;del&gt;TIKA-2440&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Takahiro&quot; class=&quot;user-hover&quot; rel=&quot;Takahiro&quot;&gt;Takahiro Ochi&lt;/a&gt; requested the ability to turn off extraction of phonetic runs.  We should enable this for docx, too.  We'll have to make fixes in POI for our DOM docx parser, but it should be fairly straighforward in our SAX docx parser.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 14 Sep 2017 00:37:03 +0000" id="518" opendate="Tue, 29 Aug 2017 19:22:07 +0000">
		<buginformation>
			<summary>Enabling extraction of standard references from text</summary>
			<description>&lt;p&gt;Apache Tika currently provides many &lt;em&gt;ContentHandler&lt;/em&gt; which help to de-obfuscate some information from text. For instance, the &lt;tt&gt;PhoneExtractingContentHandler&lt;/tt&gt; is used to extract phone numbers while parsing.&lt;/p&gt;

&lt;p&gt;This improvement adds the &lt;b&gt;&lt;tt&gt;StandardsExtractingContentHandler&lt;/tt&gt;&lt;/b&gt; to Tika, a new ContentHandler that relies on regular expressions in order to identify and extract standard references from text. &lt;br/&gt;
Basically, a standard reference is just a reference to a norm/convention/requirement (i.e., a standard) released by a standard organization. This work is maily focused on identifying and extracting the references to the standards already cited within a given document (e.g., SOW/PWS) so the references can be stored and provided to the user as additional metadata in case the StandardExtractingContentHandler is used.&lt;/p&gt;

&lt;p&gt;In addition to the patch, the first version of the &lt;tt&gt;StandardsExtractingContentHandler&lt;/tt&gt; along with an example class to easily execute the handler is available on &lt;a href=&quot;https://github.com/giuseppetotaro/StandardsExtractingContentHandler&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;GitHub&lt;/a&gt;. The following sections provide more in detail how the &lt;tt&gt;StandardsExtractingHandler&lt;/tt&gt; has been developed.&lt;/p&gt;

&lt;h1&gt;&lt;a name=&quot;Background&quot;&gt;&lt;/a&gt;Background&lt;/h1&gt;

&lt;p&gt;From a technical perspective, a standard reference is a string that is usually composed of two parts: &lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;the name of the standard organization;&lt;/li&gt;
	&lt;li&gt;the alphanumeric identifier of the standard within the organization.&lt;br/&gt;
Specifically, the first part can include the acronym or the full name of the standard organization or even both, and the second part can include an alphanumeric string, possibly containing one or more separation symbols (e.g., &quot;-&quot;, &quot;_&quot;, &quot;.&quot;) depending on the format adopted by the organization, representing the identifier of the standard within the organization.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Furthermore, the standard references are usually reported within the &quot;Applicable Documents&quot; or &quot;References&quot; section of a SOW, and they can be cited also within sections that include in the header the word &quot;standard&quot;, &quot;requirement&quot;, &quot;guideline&quot;, or &quot;compliance&quot;.&lt;/p&gt;

&lt;p&gt;Consequently, the citation of standard references within a SOW/PWS document can be summarized by the following rules:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;&lt;b&gt;RULE #1&lt;/b&gt;: standard references are usually reported within the section named &quot;Applicable Documents&quot; or &quot;References&quot;.&lt;/li&gt;
	&lt;li&gt;&lt;b&gt;RULE #2&lt;/b&gt;: standard references can be cited also within sections including the word &quot;compliance&quot; or another semantically-equivalent word in their name.&lt;/li&gt;
	&lt;li&gt;&lt;b&gt;RULE #3&lt;/b&gt;: standard references is composed of two parts:
	&lt;ul&gt;
		&lt;li&gt;Name of the standard organization (acronym, full name, or both).&lt;/li&gt;
		&lt;li&gt;Alphanumeric identifier of the standard within the organization.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;&lt;b&gt;RULE #4&lt;/b&gt;: The name of the standard organization includes the acronym or the full name or both. The name must belong to the set of standard organizations &lt;tt&gt;S = O U V&lt;/tt&gt;, where &lt;tt&gt;O&lt;/tt&gt; represents the set of open standard organizations (e.g., ANSI) and &lt;tt&gt;V&lt;/tt&gt; represents the set of vendor-specific standard organizations (e.g., Motorola).&lt;/li&gt;
	&lt;li&gt;&lt;b&gt;RULE #5&lt;/b&gt;: A separation symbol (e.g., &quot;-&quot;, &quot;_&quot;, &quot;.&quot; or whitespace) can be used between the name of the standard organization and the alphanumeric identifier.&lt;/li&gt;
	&lt;li&gt;&lt;b&gt;RULE #6&lt;/b&gt;: The alphanumeric identifier of the standard is composed of alphabetic and numeric characters, possibly split in two or more parts by a separation symbol (e.g., &quot;-&quot;, &quot;_&quot;, &quot;.&quot;).&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;On the basis of the above rules, here are some examples of formats used for reporting standard references within a SOW/PWS:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;&lt;tt&gt;&amp;lt;ORGANIZATION_ACRONYM&amp;gt;&amp;lt;SEPARATION_SYMBOL&amp;gt;&amp;lt;ALPHANUMERIC_IDENTIFIER&amp;gt;&lt;/tt&gt;&lt;/li&gt;
	&lt;li&gt;&lt;tt&gt;&amp;lt;ORGANIZATION_ACRONYM&amp;gt;&amp;lt;SEPARATION_SYMBOL&amp;gt;(&amp;lt;ORGANIZATION_FULL_NAME&amp;gt;)&amp;lt;SEPARATION_SYMBOL&amp;gt;&amp;lt;ALPHANUMERIC_IDENTIFIER&amp;gt;&lt;/tt&gt;&lt;/li&gt;
	&lt;li&gt;&lt;tt&gt;&amp;lt;ORGANIZATION_FULL_NAME&amp;gt;&amp;lt;SEPARATION_SYMBOL&amp;gt;(&amp;lt;ORGANIZATION_FULL_NAME&amp;gt;)&amp;lt;SEPARATION_SYMBOL&amp;gt;&amp;lt;ALPHANUMERIC_IDENTIFIER&amp;gt;&lt;/tt&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Moreover, some standards are sometimes released by two standard organizations. In this case, the standard reference can be reported as follows:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;&lt;tt&gt;&amp;lt;MAIN_ORGANIZATION_ACRONYM&amp;gt;/&amp;lt;SECOND_ORGANIZATION_ACRONYM&amp;gt;&amp;lt;SEPARATION_SYMBOL&amp;gt;&amp;lt;ALPHANUMERIC_IDENTIFIER&amp;gt;&lt;/tt&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;&lt;a name=&quot;RegularExpressions&quot;&gt;&lt;/a&gt;Regular Expressions&lt;/h1&gt;

&lt;p&gt;The &lt;tt&gt;StandardsExtractingContentHandler&lt;/tt&gt; uses a helper class named &lt;tt&gt;StandardsText&lt;/tt&gt; that relies on Java regular expressions and provides some methods to identify headers and standard references, and determine the score of the references found within the given text.&lt;/p&gt;

&lt;p&gt;Here are the main regular expressions used within the &lt;tt&gt;StandardsText&lt;/tt&gt; class:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;&lt;b&gt;REGEX_HEADER&lt;/b&gt;: regular expression to match only uppercase headers.
  &lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;  (\d+\.(\d+\.?)*)\p{Blank}+([A-Z]+(\s[A-Z]+)*){5,}
  &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
	&lt;li&gt;&lt;b&gt;REGEX_APPLICABLE_DOCUMENTS&lt;/b&gt;: regular expression to match the the header of &quot;APPLICABLE DOCUMENTS&quot; and equivalent sections.
  &lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;  (?i:.*APPLICABLE\sDOCUMENTS|REFERENCE|STANDARD|REQUIREMENT|GUIDELINE|COMPLIANCE.*)
  &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
	&lt;li&gt;&lt;b&gt;REGEX_FALLBACK&lt;/b&gt;: regular expression to match a string that is supposed to be a standard reference.
  &lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;  \(?(?&amp;lt;mainOrganization&amp;gt;[A-Z]\w+)\)?((\s?(?&amp;lt;separator&amp;gt;\/)\s?)(\w+\s)*\(?(?&amp;lt;secondOrganization&amp;gt;[A-Z]\w+)\)?)?(\s(Publication|Standard))?(-|\s)?(?&amp;lt;identifier&amp;gt;([0-9]{3,}|([A-Z]+(-|_|\.)?[0-9]{2,}))((-|_|\.)?[A-Z0-9]+)*)
  &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
	&lt;li&gt;&lt;b&gt;REGEX_STANDARD&lt;/b&gt;: regular expression to match the standard organization within a string potentially representing a standard reference.&lt;br/&gt;
  This regular expression is obtained by using a helper class named &lt;tt&gt;StandardOrganizations&lt;/tt&gt; that provides a list of the most important standard organizations reported on &lt;a href=&quot;https://en.wikipedia.org/wiki/List_of_technical_standard_organisations&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;Wikipedia&lt;/a&gt;. Basically, the list is composed of International standard organizations, Regional standard organizations, and American and British among Nationally-based standard organizations. Other lists of standard organizations are reported on &lt;a href=&quot;http://www.openstandards.net/viewOSnet2C.jsp?showModuleName=Organizations&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;OpenStandards&lt;/a&gt; and &lt;a href=&quot;https://ibr.ansi.org/Standards/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;IBR Standards Portal&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;


&lt;h1&gt;&lt;a name=&quot;HowToUseTheStandardsExtractionCapability&quot;&gt;&lt;/a&gt;How To Use The Standards Extraction Capability&lt;/h1&gt;

&lt;p&gt;The standard references identification performed by using the &lt;tt&gt;StandardsExtractingContentHandler&lt;/tt&gt; is based on the following steps (see also the &lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12884324/12884324_flowchart_standards_extraction.png&quot; title=&quot;flowchart_standards_extraction.png attached to TIKA-2449&quot;&gt;flow chart&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt; in attachment):&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;searches for headers;&lt;/li&gt;
	&lt;li&gt;searches for patterns that are supposed to be standard references (basically, every string mostly composed of uppercase letters followed by an alphanumeric characters);&lt;/li&gt;
	&lt;li&gt;each potential standard reference starts with score equal to 0.25;&lt;/li&gt;
	&lt;li&gt;increases by 0.50 the score of references which include the name of a known standard organization;&lt;/li&gt;
	&lt;li&gt;increases by 0.25 the score of references which have been found within &quot;Applicable Documents&quot; and equivalent sections;&lt;/li&gt;
	&lt;li&gt;returns the standard references along with scores;&lt;/li&gt;
	&lt;li&gt;adds the standard references as additional metadata.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;The unit test is implemented within the &lt;b&gt;&lt;tt&gt;StandardsExtractingContentHandlerTest&lt;/tt&gt;&lt;/b&gt; class and extracts the standard references from a SoW downloaded from the &lt;a href=&quot;https://foiarr.cbp.gov/streamingWord.asp?i=607&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;FOIA Library&lt;/a&gt;. This &lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12884323/12884323_SOW-TacCOM.pdf&quot; title=&quot;SOW-TacCOM.pdf attached to TIKA-2449&quot;&gt;SoW&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt; is also provided as PDF in attachment.&lt;/p&gt;

&lt;p&gt;The &lt;b&gt;&lt;tt&gt;StandardsExtractionExample&lt;/tt&gt;&lt;/b&gt; is a class to demonstrate how to use the &lt;tt&gt;StandardsExtractingContentHandler&lt;/tt&gt; to get a list of the standard references from every file in a directory.&lt;/p&gt;

&lt;p&gt;The &lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12884325/12884325_standards_extraction.patch&quot; title=&quot;standards_extraction.patch attached to TIKA-2449&quot;&gt;patch&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt; in attachment includes all the changes to add the support for standards extraction. &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 30 Aug 2017 17:17:19 +0000" id="519" opendate="Wed, 30 Aug 2017 12:02:49 +0000">
		<buginformation>
			<summary>OfficeParser.parse called for zero-byte file with .doc extension</summary>
			<description>&lt;p&gt;A zero-byte (empty) file with a .doc extension is detected as a Word Document and the &lt;tt&gt;OfficeParser.parse&lt;/tt&gt; method is called for this file.&lt;/p&gt;

&lt;p&gt;We then get a &lt;tt&gt;TikaException&lt;/tt&gt;, with the cause given as an &lt;tt&gt;org.apache.poi.EmptyFileException&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;I think it would be more useful if the file were NOT detected as a Word Document, meaning that the &lt;tt&gt;AutoDetectParser&lt;/tt&gt; would then fall back to whatever is set as the fallback parser in the parse context.&lt;/p&gt;

&lt;p&gt;This is more useful because the user can then trigger some special logic for handling empty files.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 1 Sep 2017 14:52:36 +0000" id="520" opendate="Wed, 30 Aug 2017 16:23:17 +0000">
		<buginformation>
			<summary>Detect image frame counts for tiff files</summary>
			<description>&lt;p&gt;It would be useful to know the number of frames in a multi-page tiff image. My apologies if this already exists but I could not locate it in any of the existing metadata output. &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 30 Aug 2017 20:15:17 +0000" id="521" opendate="Wed, 30 Aug 2017 17:33:51 +0000">
		<buginformation>
			<summary>Emails extracted from PSTs detected as unexpected file types</summary>
			<description>&lt;p&gt;This issue is severe. The Outlook PST parser extracts a string for the body of every email and passes that string to the &lt;tt&gt;EmbeddedDocumentExtractor&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;However, no content type is set on the &lt;tt&gt;Metadata&lt;/tt&gt; object passed to the extractor. Therefore, if for example, the body of the email starts with the string &quot;From John Smith.&quot; (for example, when an email was forwarded), then body of the email is detected as &lt;tt&gt;application/mbox&lt;/tt&gt; and parsed as though it were an mbox file.&lt;/p&gt;

&lt;p&gt;I think the immediate fix for this issue is to force the type of the email to &lt;tt&gt;text/plain&lt;/tt&gt; and for it to be parsed as such.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/mbox/OutlookPSTParser.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 24 Oct 2017 00:39:22 +0000" id="522" opendate="Thu, 31 Aug 2017 16:03:03 +0000">
		<buginformation>
			<summary>Flag in metadata for alternative email bodies</summary>
			<description>&lt;p&gt;When multipart RFC822 emails are being parsed, there's no way to distinguish between alternative versions of the body and attachments.&lt;/p&gt;

&lt;p&gt;It would be ideal if some kind of flag were set in the metadata passed to the &lt;tt&gt;EmbeddedDocumentExtractor&lt;/tt&gt; that indicates that the stream is an alternative.&lt;/p&gt;

&lt;p&gt;In GUIs that present the data extracted from the email, alternative bodies can be distinguished from attachments and presented separately.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/mail/MailContentHandler.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/metadata/Message.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 31 Aug 2017 16:35:34 +0000" id="523" opendate="Thu, 31 Aug 2017 16:19:07 +0000">
		<buginformation>
			<summary>Emails extracted from MBOX not detected as rfc822</summary>
			<description>&lt;p&gt;Similar to &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-2454&quot; title=&quot;Emails extracted from PSTs detected as unexpected file types&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-2454&quot;&gt;&lt;del&gt;TIKA-2454&lt;/del&gt;&lt;/a&gt;, because of recurrent detection issues with message/rfc822 (&lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-2042&quot; title=&quot;MBOX file detected wrongly as text/html&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-2042&quot;&gt;&lt;del&gt;TIKA-2042&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1602&quot; title=&quot;Detecting standards-non-compliant emails as message/rfc822&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1602&quot;&gt;&lt;del&gt;TIKA-1602&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-879&quot; title=&quot;Detection problem: message/rfc822 file is detected as text/plain.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-879&quot;&gt;&lt;del&gt;TIKA-879&lt;/del&gt;&lt;/a&gt;), children of mbox files could not be detected as rfc822, but they will always be. Solution is to set Content-Type-Override inside MBOXPArser. Fix being prepared...&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 8 Sep 2017 16:48:35 +0000" id="524" opendate="Tue, 5 Sep 2017 22:51:24 +0000">
		<buginformation>
			<summary>Missing text in .doc file (but can be extracted by POI)</summary>
			<description>&lt;p&gt;I've got a document whose text can be extracted via org.apache.poi.hwpf.converter.WordToTextConverter, but does not fully get extracted by Tika. The 'paragraph one' paragraph is present in the POI extraction output, and is not present in Tika's output.&lt;/p&gt;

&lt;p&gt;Tika's output:&lt;/p&gt;


&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Something
One:
Else
Two:
Here
Three:
Four

Paragraph two
Paragraph three
Paragraph four
cc: Somebody
     Somebody else
Something here too
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;POI's output:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Something
One:    Else
Two:    Here
Three:  Four

Paragraph one

Paragraph two

Paragraph three

Paragraph four


cc: Somebody
     Somebody else


Something here too
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/WordExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 14 Sep 2017 01:42:51 +0000" id="525" opendate="Thu, 14 Sep 2017 01:41:11 +0000">
		<buginformation>
			<summary>Add explicit unit tests for xxe</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/TestXXEInXML.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 21 Sep 2017 20:51:23 +0000" id="526" opendate="Thu, 14 Sep 2017 19:25:02 +0000">
		<buginformation>
			<summary>Remove JAXB usage</summary>
			<description>&lt;p&gt;Starting with Java 9 the &lt;tt&gt;javax.xml.bind&lt;/tt&gt; classes are now part of the &lt;tt&gt;java.se.ee&lt;/tt&gt; module which is not enabled by default. To simplify the Java 9 integration ( no --add-modules CLI switch, no explicity Java 9 module ) I propose we simply replace JAXB with something else.&lt;/p&gt;

&lt;p&gt;See &lt;a href=&quot;https://lists.apache.org/thread.html/72342314e709417bcb777fd3511b700dee443a3a658b730e52f99e38@%3Cuser.tika.apache.org%3E&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://lists.apache.org/thread.html/72342314e709417bcb777fd3511b700dee443a3a658b730e52f99e38@%3Cuser.tika.apache.org%3E&lt;/a&gt; for more context&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 21 Sep 2017 20:51:23 +0000" id="527" opendate="Fri, 15 Sep 2017 15:08:38 +0000">
		<buginformation>
			<summary>Refactor XML reading components to a separate lclass</summary>
			<description>&lt;p&gt;As &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gagravarr&quot; class=&quot;user-hover&quot; rel=&quot;gagravarr&quot;&gt;Nick Burch&lt;/a&gt; mentioned on &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-2466&quot; title=&quot;Remove JAXB usage&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-2466&quot;&gt;&lt;del&gt;TIKA-2466&lt;/del&gt;&lt;/a&gt;, it is kind of strange to have the getSAXParser and friends placed in a non-static ParseContext.  Let's create a new class to handle and properly configure XML readers/processors via static methods.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-batch/src/main/java/org/apache/tika/batch/builders/BatchProcessBuilder.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 11 Oct 2017 14:59:58 +0000" id="528" opendate="Mon, 18 Sep 2017 20:06:08 +0000">
		<buginformation>
			<summary>False positives with x-ms-owner detection</summary>
			<description>&lt;p&gt;Attached windows system files are incorrectly detected as application/x-ms-owner. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tallison%40apache.org&quot; class=&quot;user-hover&quot; rel=&quot;tallison@apache.org&quot;&gt;Tim Allison&lt;/a&gt; did you add the magic for x-ms-owner? Is it possible to make the magic regex more strict?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 22 Sep 2017 20:48:29 +0000" id="529" opendate="Fri, 22 Sep 2017 20:41:06 +0000">
		<buginformation>
			<summary>Illegal reflective Access -- more cleanup for Java 9</summary>
			<description>&lt;p&gt;WARNING: Illegal reflective access by org.apache.tika.utils.XMLReaderUtils (&lt;a href=&quot;file:/C:/data/tika-eval-1.17-SNAPSHOT.jar&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;file:/C:/data/tika-eval-1.17-SNAPSHOT.jar&lt;/a&gt;) to method com.sun.org.apache.xerces.internal.util.SecurityManager.setEntityExpansionLimit(int)&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/utils/XMLReaderUtils.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 11 Oct 2017 10:44:37 +0000" id="530" opendate="Fri, 6 Oct 2017 10:31:08 +0000">
		<buginformation>
			<summary>PCX and DCX image support</summary>
			<description>&lt;p&gt;It's straightforward in theory to implement support for PCX and DCX. There's support for it in Commons Imaging as well as in ImageIO via TwelveMonkeys.&lt;/p&gt;

&lt;p&gt;In practise, however, I'm not really sure how implement support. We obviously want to OCR the images, but Tesseract has no support for the format. So where do we do the conversion to a BufferedImage? I tried to look for what is done to handle JBIG2 files but I can't find that anywhere.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 11 Oct 2017 13:21:38 +0000" id="531" opendate="Tue, 10 Oct 2017 20:31:28 +0000">
		<buginformation>
			<summary>discrepancy between CharsetDetector APIs</summary>
			<description>&lt;h3&gt;&lt;a name=&quot;Problem&quot;&gt;&lt;/a&gt;Problem&lt;/h3&gt;
&lt;p&gt;I ran into this trying to use CharsetDetector to detect charsets of attachments on emails when the mail client doesn't specify one. This used to work for us in tika 1.10, but in a recent upgrade to 1.14, behavior seems to have changed. I've attached a sample file, whose charset is ISO-8859-1, and was detected as such with Tika 1.10. When we updated our tika dependency, we noticed that this sample data (a mix of English, Portuguese, and Spanish language) was getting output as a lot of junk Chinese characters. Upon inspection, it was determined that this was because our usage of the newer tika dep was detecting the file as UTF-16LE, instead of ISO-8859-1.&lt;/p&gt;

&lt;p&gt;I've attached a sample file (multi-language.txt)&lt;/p&gt;

&lt;p&gt;Below is a Spock test that demonstrates the issue:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;    def &quot;test charset detection on multilingual file&quot;(){
        setup:
        def file = new File(&quot;src/test/resources/data/multi-language.txt&quot;)

        when: &quot;using the InputStream api&quot;
        def detector = new CharsetDetector()
        detector.setText(file.newInputStream())
        def fileCharSet = detector.detect()

        then: &quot;successfully detects the charset&quot;
        fileCharSet.name.startsWith(&quot;ISO&quot;)

        when: &quot;using the byte[] api, and munging the input&quot;
        detector = new CharsetDetector()
        detector.setText(file.newInputStream().bytes)
        detector.MungeInput()
        fileCharSet = detector.detect()

        then: &quot;sucessfully detects the charset&quot;
        fileCharSet.name.startsWith(&quot;ISO&quot;)

        when: &quot;using the byte[] api alone&quot;
        detector = new CharsetDetector()
        detector.setText(file.newInputStream().bytes)
        fileCharSet = detector.detect()

        then: &quot;this will fail - detects UTF-16LE instead&quot;
        fileCharSet.name.startsWith(&quot;ISO&quot;)
    }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As is shown in the above test, I believe the issue is that the CharsetDetector's various &lt;tt&gt;setText()&lt;/tt&gt; functions do not delegate to one another, and in one the &lt;tt&gt;MungeInput()&lt;/tt&gt; function is called, and in the other it is not.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 2 Nov 2017 12:37:51 +0000" id="532" opendate="Tue, 17 Oct 2017 00:30:59 +0000">
		<buginformation>
			<summary>RFC822 includes redundant copies of the text</summary>
			<description>&lt;p&gt;MBOX messages often get parsed into four documents:&lt;br/&gt;
a.	The mbox file - outer container &quot;/&quot;&lt;br/&gt;
b.	The actual email--  &quot;/embedded-1&quot;&lt;br/&gt;
c.	The utf-8 text content of the email &quot;/embedded-1/embedded-2&quot;&lt;br/&gt;
d.	The utf-8 html content of the email  &quot;/embedded-1/embedded-3&quot;&lt;/p&gt;

&lt;p&gt;entries C and D are redundant and distracting.  The MSG parser parses the first non-null: email body and then it skips the rest.  Please modify MBOX to not have separate &quot;attached&quot; documents for the html body and the text body.&lt;/p&gt;

&lt;p&gt;The attachment to &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-2471&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/TIKA-2471&lt;/a&gt; is an example of input sufficient to generate this behavior.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
			<file>/tika-server/src/main/java/org/apache/tika/server/TikaServerCli.java</file>
			<file>/tika-server/src/test/java/org/apache/tika/server/TikaServerIntegrationTest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 8 Dec 2017 16:39:02 +0000" id="533" opendate="Fri, 27 Oct 2017 03:28:00 +0000">
		<buginformation>
			<summary>Using PackageParser in ForkParser causes NPE</summary>
			<description>&lt;blockquote&gt;
&lt;p&gt;Caused by: java.lang.NullPointerException&lt;br/&gt;
        at org.apache.tika.mime.MimeTypesFactory.create(MimeTypesFactory.java:158)&lt;br/&gt;
        at org.apache.tika.mime.MimeTypes.getDefaultMimeTypes(MimeTypes.java:577)&lt;br/&gt;
        at org.apache.tika.config.TikaConfig.getDefaultMimeTypes(TikaConfig.java:78)&lt;br/&gt;
        at org.apache.tika.config.TikaConfig.&amp;lt;init&amp;gt;(TikaConfig.java:242)&lt;br/&gt;
        at org.apache.tika.config.TikaConfig.getDefaultConfig(TikaConfig.java:379)&lt;br/&gt;
        at org.apache.tika.parser.pkg.PackageParser.parse(PackageParser.java:165)&lt;br/&gt;
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:280)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The mediaTypeRegistry handling code in parse() of PackageParser seems cause the problem due to ForkParser cannot properly construct default TikaConfig. Also since TikaConfig is not serializable, there is no way to assign mediaTypeRegistry/bufferedMediaTypeRegistry before calling parse()&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pkg/PackageParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 2 Nov 2017 13:43:01 +0000" id="534" opendate="Fri, 27 Oct 2017 13:36:42 +0000">
		<buginformation>
			<summary>EncodingDetectors markLimits to be configurable</summary>
			<description>&lt;p&gt;Tim's response to my question:&lt;/p&gt;

&lt;p&gt;----&lt;del&gt;Original message&lt;/del&gt;----&lt;br/&gt;
&amp;gt; From:Allison, Timothy B. &amp;lt;tallison@mitre.org&amp;gt;&lt;br/&gt;
&amp;gt; Sent: Friday 27th October 2017 14:53&lt;br/&gt;
&amp;gt; To: user@tika.apache.org&lt;br/&gt;
&amp;gt; Subject: RE: Incorrect encoding detected&lt;br/&gt;
&amp;gt; &lt;br/&gt;
&amp;gt; Hi Markus,&lt;br/&gt;
&amp;gt;   &lt;br/&gt;
&amp;gt; My guess is that the ~32,000 characters of mostly ascii-ish &amp;lt;script/&amp;gt; are what is actually being used for encoding detection.  The HTMLEncodingDetector only looks in the first 8,192 characters, and the other encoding detectors have similar (but longer?) restrictions.&lt;br/&gt;
&amp;gt;  &lt;br/&gt;
&amp;gt; At some point, I had a dev version of a stripper that removed contents of &amp;lt;script/&amp;gt; and &amp;lt;style/&amp;gt; before trying to detect the encoding&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;...perhaps it is time to resurrect that code and integrate it?&lt;br/&gt;
&amp;gt; &lt;br/&gt;
&amp;gt; Or, given that HTML has been, um, blossoming, perhaps, more simply, we should expand how far we look into a stream for detection?&lt;br/&gt;
&amp;gt; &lt;br/&gt;
&amp;gt; Cheers,&lt;br/&gt;
&amp;gt; &lt;br/&gt;
&amp;gt;                Tim&lt;br/&gt;
&amp;gt; &lt;br/&gt;
&amp;gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-2038&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/TIKA-2038&lt;/a&gt;&lt;br/&gt;
&amp;gt;    &lt;br/&gt;
&amp;gt; &lt;br/&gt;
&amp;gt; ----&lt;del&gt;Original Message&lt;/del&gt;----&lt;br/&gt;
&amp;gt; From: Markus Jelsma &lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;mailto:markus.jelsma@openindex.io&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;markus.jelsma@openindex.io&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/mail_small.gif&quot; height=&quot;12&quot; width=&quot;13&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt; &lt;br/&gt;
&amp;gt; Sent: Friday, October 27, 2017 8:39 AM&lt;br/&gt;
&amp;gt; To: user@tika.apache.org&lt;br/&gt;
&amp;gt; Subject: Incorrect encoding detected&lt;br/&gt;
&amp;gt; &lt;br/&gt;
&amp;gt; Hello,&lt;br/&gt;
&amp;gt; &lt;br/&gt;
&amp;gt; We have a problem with Tika, encoding and pages on this website: &lt;a href=&quot;https://www.aarstiderne.com/frugt-groent-og-mere/mixkasser&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://www.aarstiderne.com/frugt-groent-og-mere/mixkasser&lt;/a&gt;&lt;br/&gt;
&amp;gt; &lt;br/&gt;
&amp;gt; Using Nutch and Tika 1.12, but also using Tika 1.16, we found out that the regular HTML parser does a fine job, but our TikaParser has a tough job dealing with this HTML. For some reason Tika thinks Content-Encoding=windows-1252 is what this webpage says it is, instead the page identifies itself properly as UTF-8.&lt;br/&gt;
&amp;gt; &lt;br/&gt;
&amp;gt; Of all websites we index, this is so far the only one giving trouble indexing accents, getting fÃ¥ instead of a regular få.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 14 Nov 2017 16:07:43 +0000" id="535" opendate="Fri, 27 Oct 2017 15:36:58 +0000">
		<buginformation>
			<summary>Upgrade metadata-extractor to 2.10.1</summary>
			<description>&lt;p&gt;...because earlier versions reference xmpcore 5.1.2 which is affected by &lt;a href=&quot;http://www.cvedetails.com/cve/CVE-2016-4216/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.cvedetails.com/cve/CVE-2016-4216/&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 14 Nov 2017 16:07:43 +0000" id="536" opendate="Mon, 30 Oct 2017 16:31:31 +0000">
		<buginformation>
			<summary>Outlook PST Parser fails from NullPointerException</summary>
			<description>&lt;p&gt;Getting this error when trying to parse PST file.&lt;br/&gt;
commands used:&lt;br/&gt;
java -jar tika-server-1.16.jar &lt;br/&gt;
curl -T test.pst &lt;a href=&quot;http://127.0.0.1:9998/tika&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://127.0.0.1:9998/tika&lt;/a&gt; --header &quot;Accept: text/plain&quot; &lt;/p&gt;

&lt;p&gt;WARN  tika: Text extraction failed&lt;br/&gt;
org.apache.tika.exception.TikaException: Unable to unpack document stream&lt;br/&gt;
        at org.apache.tika.parser.mbox.OutlookPSTParser.parse(OutlookPSTParser.java:95)&lt;br/&gt;
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:280)&lt;br/&gt;
        at org.apache.tika.parser.ParserDecorator.parse(ParserDecorator.java:188)&lt;br/&gt;
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:280)&lt;br/&gt;
        at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:135)&lt;br/&gt;
        at org.apache.tika.server.resource.TikaResource.parse(TikaResource.java:322)&lt;br/&gt;
        at org.apache.tika.server.resource.TikaResource$5.write(TikaResource.java:421)&lt;br/&gt;
        at org.apache.cxf.jaxrs.provider.BinaryDataProvider.writeTo(BinaryDataProvider.java:169)&lt;br/&gt;
        at org.apache.cxf.jaxrs.utils.JAXRSUtils.writeMessageBody(JAXRSUtils.java:1389)&lt;br/&gt;
        at org.apache.cxf.jaxrs.interceptor.JAXRSOutInterceptor.serializeMessage(JAXRSOutInterceptor.java:243)&lt;br/&gt;
        at org.apache.cxf.jaxrs.interceptor.JAXRSOutInterceptor.processResponse(JAXRSOutInterceptor.java:119)&lt;br/&gt;
        at org.apache.cxf.jaxrs.interceptor.JAXRSOutInterceptor.handleMessage(JAXRSOutInterceptor.java:82)&lt;br/&gt;
        at org.apache.cxf.phase.PhaseInterceptorChain.doIntercept(PhaseInterceptorChain.java:307)&lt;br/&gt;
        at org.apache.cxf.interceptor.OutgoingChainInterceptor.handleMessage(OutgoingChainInterceptor.java:83)&lt;br/&gt;
        at org.apache.cxf.phase.PhaseInterceptorChain.doIntercept(PhaseInterceptorChain.java:307)&lt;br/&gt;
        at org.apache.cxf.transport.ChainInitiationObserver.onMessage(ChainInitiationObserver.java:121)&lt;br/&gt;
        at org.apache.cxf.transport.http.AbstractHTTPDestination.invoke(AbstractHTTPDestination.java:252)&lt;br/&gt;
        at org.apache.cxf.transport.http_jetty.JettyHTTPDestination.doService(JettyHTTPDestination.java:261)&lt;br/&gt;
        at org.apache.cxf.transport.http_jetty.JettyHTTPHandler.handle(JettyHTTPHandler.java:76)&lt;br/&gt;
        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1088)&lt;br/&gt;
        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1024)&lt;br/&gt;
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)&lt;br/&gt;
        at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:255)&lt;br/&gt;
        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)&lt;br/&gt;
        at org.eclipse.jetty.server.Server.handle(Server.java:370)&lt;br/&gt;
        at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)&lt;br/&gt;
        at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:973)&lt;br/&gt;
        at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1035)&lt;br/&gt;
        at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:647)&lt;br/&gt;
        at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:231)&lt;br/&gt;
        at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)&lt;br/&gt;
        at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:696)&lt;br/&gt;
        at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:53)&lt;br/&gt;
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)&lt;br/&gt;
        at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:748)&lt;br/&gt;
Caused by: org.apache.tika.exception.TikaException: Unable to unpack document stream&lt;br/&gt;
        at org.apache.tika.parser.mbox.OutlookPSTParser.parseMailAttachments(OutlookPSTParser.java:241)&lt;br/&gt;
        at org.apache.tika.parser.mbox.OutlookPSTParser.parseFolder(OutlookPSTParser.java:121)&lt;br/&gt;
        at org.apache.tika.parser.mbox.OutlookPSTParser.parseFolder(OutlookPSTParser.java:133)&lt;br/&gt;
        at org.apache.tika.parser.mbox.OutlookPSTParser.parseFolder(OutlookPSTParser.java:133)&lt;br/&gt;
        at org.apache.tika.parser.mbox.OutlookPSTParser.parseFolder(OutlookPSTParser.java:133)&lt;br/&gt;
        at org.apache.tika.parser.mbox.OutlookPSTParser.parseFolder(OutlookPSTParser.java:133)&lt;br/&gt;
        at org.apache.tika.parser.mbox.OutlookPSTParser.parse(OutlookPSTParser.java:92)&lt;br/&gt;
        ... 35 more&lt;br/&gt;
Caused by: java.lang.NullPointerException&lt;br/&gt;
        at com.pff.PSTAttachment.getFileInputStream(PSTAttachment.java:119)&lt;br/&gt;
        at org.apache.tika.parser.mbox.OutlookPSTParser.parseMailAttachments(OutlookPSTParser.java:232)&lt;br/&gt;
        ... 41 more&lt;br/&gt;
ERROR Problem with writing the data, class org.apache.tika.server.resource.TikaResource$5, ContentType: text/plain&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 3 Nov 2017 13:37:47 +0000" id="537" opendate="Fri, 3 Nov 2017 13:03:20 +0000">
		<buginformation>
			<summary>Upgrade to PDFBox 2.0.8</summary>
			<description>&lt;p&gt;Now available.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 3 Nov 2017 20:56:04 +0000" id="538" opendate="Fri, 3 Nov 2017 14:25:10 +0000">
		<buginformation>
			<summary>Turn off stderr warnings in Tika-app</summary>
			<description>&lt;p&gt;Let's get rid of the stderr messages in tika-app and confirm that users can turn off warnings via tika-config.xml&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 5 Dec 2017 13:45:41 +0000" id="539" opendate="Mon, 13 Nov 2017 16:42:30 +0000">
		<buginformation>
			<summary>Upgrade OpenNLP to 1.8.3</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/tika-bundle/pom.xml</file>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 14 Nov 2017 16:27:39 +0000" id="540" opendate="Mon, 13 Nov 2017 16:46:15 +0000">
		<buginformation>
			<summary>Try to upgrade httpclient to &gt;=4.5.3</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 14 Nov 2017 16:26:54 +0000" id="541" opendate="Mon, 13 Nov 2017 17:45:33 +0000">
		<buginformation>
			<summary>Upgrade or remove plexus-utils</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 27 Nov 2017 19:55:39 +0000" id="542" opendate="Mon, 27 Nov 2017 17:10:57 +0000">
		<buginformation>
			<summary>Slowness parsing SQLite database file</summary>
			<description>&lt;p&gt;Parsing of the attached urlclassifier3.sqlite database is approximately 5 times slower in Tika 1.16 than it was in 1.14.&lt;/p&gt;

&lt;p&gt;I've performed some profiling and it appears as though the problem lies in the number of times a new TikaConfig instance gets created. See attached screenshot&lt;/p&gt;

&lt;p&gt;I notice that EmbeddedDocumentUtil has 2 methods to get a TikaConfig, one of which caches the config instance. Are both methods needed? Why does EmbeddedDocumentUtil.getExtension() use the version that doesn't cache the config?&lt;/p&gt;


			</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 28 Nov 2017 13:17:57 +0000" id="543" opendate="Tue, 28 Nov 2017 13:15:49 +0000">
		<buginformation>
			<summary>Add underline and strikethrough to SAX-based docx/pptx parsers</summary>
			<description>&lt;p&gt;See &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-2347&quot; title=&quot;Underlined text is not decorated as such when extracting from word documents&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-2347&quot;&gt;&lt;del&gt;TIKA-2347&lt;/del&gt;&lt;/a&gt;.  Add this for SAX-based pptx/docx extractors.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 4 Dec 2017 19:46:57 +0000" id="544" opendate="Mon, 4 Dec 2017 18:38:20 +0000">
		<buginformation>
			<summary>Upgrade CFX version to &gt; 3.0.13</summary>
			<description>&lt;p&gt;See &lt;a href=&quot;https://nvd.nist.gov/vuln/detail/CVE-2017-3156&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://nvd.nist.gov/vuln/detail/CVE-2017-3156&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-langdetect/pom.xml</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 8 Dec 2017 14:51:04 +0000" id="545" opendate="Wed, 6 Dec 2017 22:07:55 +0000">
		<buginformation>
			<summary>Issue parsing multiple CHM files concurrently</summary>
			<description>&lt;p&gt;Should I expect to be able to parse multiple CHM files concurrently in multiple threads?&lt;br/&gt;
What I'm noticing when attempting to parse 2 different CHM files in different threads is that:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;ChmExtractor.extractChmEntry() gets a ChmBlockInfo as follows:
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
                ChmBlockInfo bb = ChmBlockInfo.getChmBlockInfoInstance(
                        directoryListingEntry, (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;) getChmLzxcResetTable()
                                .getBlockLen(), getChmLzxcControlData());
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
	&lt;li&gt;ChmBlockInfo.getChmBlockInfoInstance() is a static method that appears to limit the number of ChmBlockInfo instances to 1.
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; ChmBlockInfo getChmBlockInfoInstance(
            DirectoryListingEntry dle, &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; bytesPerBlock,
            ChmLzxcControlData clcd) {
        setChmBlockInfo(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ChmBlockInfo());
        getChmBlockInfo().setStartBlock(dle.getOffset() / bytesPerBlock);
        getChmBlockInfo().setEndBlock(
                (dle.getOffset() + dle.getLength()) / bytesPerBlock);
        getChmBlockInfo().setStartOffset(dle.getOffset() % bytesPerBlock);
        getChmBlockInfo().setEndOffset(
                (dle.getOffset() + dle.getLength()) % bytesPerBlock);
        &lt;span class=&quot;code-comment&quot;&gt;// potential problem with casting &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; to &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;
&lt;/span&gt;        getChmBlockInfo().setIniBlock(
                getChmBlockInfo().startBlock - getChmBlockInfo().startBlock
                        % (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;) clcd.getResetInterval());
&lt;span class=&quot;code-comment&quot;&gt;//                (getChmBlockInfo().startBlock - getChmBlockInfo().startBlock)
&lt;/span&gt;&lt;span class=&quot;code-comment&quot;&gt;//                        % (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;) clcd.getResetInterval());
&lt;/span&gt;        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; getChmBlockInfo();
    }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Is there a good reason why there should only ever be one instance of ChmBlockInfo?&lt;/p&gt;

&lt;p&gt;Should we forget about attempting to process CHM files in parallel and instead queue them up to be processed sequentially?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/test/java/org/apache/tika/MultiThreadedTikaTest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 24 May 2018 20:55:28 +0000" id="546" opendate="Fri, 8 Dec 2017 15:08:46 +0000">
		<buginformation>
			<summary>OptimaizeLangDetector#loadModels() should not be called for every single langdetect HTTP request</summary>
			<description>&lt;p&gt;Tika REST server's `/language` resource invokes the relatively heavy `loadModels` operation for every language detect call:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeHeader panelHeader&quot; style=&quot;border-bottom-width: 1px;&quot;&gt;&lt;b&gt;LanguageResource.java&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; detect(&lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; string) &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException {
	LanguageResult language = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; OptimaizeLangDetector().loadModels().detect(string);
	&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; detectedLang = language.getLanguage();
	LOG.info(&lt;span class=&quot;code-quote&quot;&gt;&quot;Detecting language &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; incoming resource: [{}]&quot;&lt;/span&gt;, detectedLang);
	&lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; detectedLang;
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This could be optimized by (lazy?) loading the models only once and keep them in memory. I assume the `LanguageDetector` is not thread safe, so I expect this requires an ExecutorService with language detectors.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-langdetect/src/main/java/org/apache/tika/langdetect/OptimaizeLangDetector.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 8 Dec 2017 17:39:29 +0000" id="547" opendate="Fri, 8 Dec 2017 17:37:51 +0000">
		<buginformation>
			<summary>SAX-based docx/pptx should start a new line before second paragraph within a cell</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/OOXMLTikaBodyPartHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 13 Dec 2017 15:50:46 +0000" id="548" opendate="Mon, 11 Dec 2017 13:32:09 +0000">
		<buginformation>
			<summary>Create/integrate a parser for XPS</summary>
			<description>&lt;p&gt;When we parse XPS files using the AutoParser we always get an empty string.&lt;br/&gt;
If we use DefaultDetector.detect() it correctly detects the MediaType as &quot;application/vnd.ms-xpsdocument&quot;.&lt;/p&gt;

&lt;p&gt;This page&lt;br/&gt;
&lt;a href=&quot;https://tika.apache.org/1.16/formats.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://tika.apache.org/1.16/formats.html&lt;/a&gt;&lt;br/&gt;
suggests that XPS (application/vnd.ms-xpsdocument) is supported however. &lt;/p&gt;

&lt;p&gt;Our code:&lt;br/&gt;
		InputStream bis = this.getClass().getResourceAsStream(&quot;/&quot; + EXPECTED_LOCATION + &quot;doc_xps.xps&quot;);&lt;br/&gt;
		Metadata metadata = new Metadata();&lt;br/&gt;
		BodyContentHandler handler = new BodyContentHandler();&lt;br/&gt;
		AutoDetectParser parser = new AutoDetectParser();&lt;br/&gt;
		TikaInputStream tikaStream = TikaInputStream.get(bis);&lt;br/&gt;
		parser.parse(tikaStream, handler, metadata);&lt;br/&gt;
		String parsedText = handler.toString();&lt;/p&gt;

&lt;p&gt;I will attach doc_xps.xps if I can&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 12 Feb 2018 15:35:34 +0000" id="549" opendate="Thu, 8 Feb 2018 16:25:19 +0000">
		<buginformation>
			<summary>Grouped Text boxes in .ppt</summary>
			<description>&lt;p&gt;Grouped Text boxes are unable to be parsed and no content is returned when items have been grouped together. This issue does not seem to affect .pptx files, only .ppt. The attached documents are the same except the file format. It should give a very simple example of a .ppt document where no content will be returned.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 7 Mar 2018 20:15:57 +0000" id="550" opendate="Tue, 27 Feb 2018 14:07:08 +0000">
		<buginformation>
			<summary>Some tiffs (Big Endian with fax compression) are showing up as x-tarr</summary>
			<description>&lt;p&gt;I have found that a certain tiff that we manage is now reporting application/x-tar in Tika where it previously reported as a tiff (image/tiff). &lt;/p&gt;

&lt;p&gt;Observe this code in ArchiveStreamFactory, detect method.&lt;/p&gt;

&lt;p&gt;  // &lt;a href=&quot;https://issues.apache.org/jira/browse/COMPRESS-117&quot; title=&quot;Certain tar files not recognised by ArchiveStreamFactory&quot; class=&quot;issue-link&quot; data-issue-key=&quot;COMPRESS-117&quot;&gt;&lt;del&gt;COMPRESS-117&lt;/del&gt;&lt;/a&gt; - improve auto-recognition&lt;/p&gt;

&lt;p&gt;        if (signatureLength &amp;gt;= TAR_HEADER_SIZE) {&lt;/p&gt;

&lt;p&gt;            TarArchiveInputStream tais = null;&lt;/p&gt;

&lt;p&gt;            try {&lt;/p&gt;

&lt;p&gt;                tais = new TarArchiveInputStream(new ByteArrayInputStream(tarHeader));&lt;/p&gt;

&lt;p&gt;                // &lt;a href=&quot;https://issues.apache.org/jira/browse/COMPRESS-191&quot; title=&quot;Too relaxed tar detection in ArchiveStreamFactory&quot; class=&quot;issue-link&quot; data-issue-key=&quot;COMPRESS-191&quot;&gt;&lt;del&gt;COMPRESS-191&lt;/del&gt;&lt;/a&gt; - verify the header checksum&lt;/p&gt;

&lt;p&gt;                if (tais.getNextTarEntry().isCheckSumOK()) &lt;/p&gt;
{

                    return TAR;

                }

&lt;p&gt;            } catch (final Exception e) &lt;/p&gt;
{ // NOPMD // NOSONAR

                // can generate IllegalArgumentException as well

                // as IOException

                // autodetection, simply not a TAR

                // ignored

            }
&lt;p&gt; finally &lt;/p&gt;
{

                IOUtils.closeQuietly(tais);

            }

&lt;p&gt;What if find is that most TIFs, when they get to tais.getNextTarEntry() fail with an exception (i.e fall into the &quot;simply not a tar&quot; case). However this tiff actually does NOT fail here. This somewhat makes sense as the internal structure of a fax compressed tifs as a tar-like structure&lt;/p&gt;

&lt;p&gt;Note, the CompositeDetector class eventually does recognize it as a proper tiff as it loops through its detectors in its detect method. It is detected as tiff in the MimeTypes class, which is one of the implementations of the Detector interface&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt;    public MediaType detect(InputStream input, Metadata metadata)&lt;/p&gt;

&lt;p&gt;            throws IOException {&lt;/p&gt;

&lt;p&gt;        MediaType type = MediaType.OCTET_STREAM;&lt;/p&gt;

&lt;p&gt;        for (Detector detector : getDetectors()) {&lt;/p&gt;

&lt;p&gt;            //short circuit via OverrideDetector&lt;/p&gt;

&lt;p&gt;            //can't rely on ordering because subsequent detector may&lt;/p&gt;

&lt;p&gt;            //change Override's to a specialization of Override's&lt;/p&gt;

&lt;p&gt;            if (detector instanceof OverrideDetector &amp;amp;&amp;amp;        metadata.get(TikaCoreProperties.CONTENT_TYPE_OVERRIDE) != null) &lt;/p&gt;
{

                return detector.detect(input, metadata);

            }

&lt;p&gt;            MediaType detected = detector.detect(input, metadata);&lt;/p&gt;

&lt;p&gt;            if (registry.isSpecializationOf(detected, type)) &lt;/p&gt;
{

                type = detected;

            }

&lt;p&gt;        }&lt;/p&gt;

&lt;p&gt;        return type;&lt;/p&gt;

&lt;p&gt;However since Image/tiff isn't a specialization of application/x-tar it does not replace the type with tiff.&lt;/p&gt;

&lt;p&gt;My fix was to add a  &quot;&amp;lt;sub-class-of type=&quot;application/x-tar&quot;/&amp;gt;&quot; to the definition for image/tiff in the tika-mimetypes.xml file&lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt; &lt;/p&gt;

&lt;p&gt; &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pkg/ZipContainerDetector.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 26 Oct 2016 02:38:03 +0000" id="551" opendate="Wed, 18 Jun 2014 21:16:13 +0000">
		<buginformation>
			<summary>Create a Tika Translator implementation that uses JoshuaDecoder</summary>
			<description>&lt;p&gt;The Joshua Decoder toolkit is a BSD licensed Java-based statistical machine translation system hosted at Github:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://joshua-decoder.org/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://joshua-decoder.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Joshua takes in corpuses and trains models that can then be used to do language translation. Currently there is support for e.g., Spanisn-&amp;gt;English, Indian dialects-&amp;gt;English, Chinese-&amp;gt;English, and a few others. &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/joshua-decoder/joshua/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/joshua-decoder/joshua/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It would be nice to build a Tika Translator on top of Joshua. There are of course several issues with this:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;the models are huge - so we'll need a separate package or Maven module, maybe tika-translate-joshua or something to release the models and we'll need to build the models. I just went through the process of building the Spanish-&amp;gt;English one, and it still needs to be rebuilt b/c I did it wrong, but it took over a day&lt;/li&gt;
	&lt;li&gt;there is a configuration for Joshua, and so we need some way of passing that config into the Translator. Not sure of the best way to do this.&lt;/li&gt;
	&lt;li&gt;Joshua isn't in the Central repository. I've started a discussion on the Joshua lists about this: &lt;a href=&quot;https://groups.google.com/forum/#!topic/joshua_support/9Y04miboUj0&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://groups.google.com/forum/#!topic/joshua_support/9Y04miboUj0&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Anyhoo, I've got a working patch right now with hard code stuff, and a manual install into my Maven repo for brave souls out there that want to try it.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/language/detect/LanguageResult.java</file>
			<file>/tika-translate/src/main/java/org/apache/tika/language/translate/JoshuaNetworkTranslator.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 17 May 2016 19:53:17 +0000" id="552" opendate="Mon, 16 May 2016 08:46:10 +0000">
		<buginformation>
			<summary>Date not extracted from email saved as plain txt</summary>
			<description>&lt;p&gt;HI have two email testfiles:&lt;/p&gt;

&lt;p&gt;(1) A file that has been created by using &quot;save as&quot; in Mac Mail (this creates a .txt file)&lt;br/&gt;
(2) A file that has been created by dragging an email from Mac Mail to the Desktop (this creates an .eml file)&lt;/p&gt;

&lt;p&gt;If I feed the files with&lt;/p&gt;

&lt;p&gt;curl -T filename &lt;a href=&quot;http://localhost:9998/detect/stream&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://localhost:9998/detect/stream&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I get the response &quot;message/rfc822&quot; for both files.&lt;/p&gt;

&lt;p&gt;If I run&lt;/p&gt;

&lt;p&gt;curl -T filename &lt;a href=&quot;http://localhost:9998/meta&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://localhost:9998/meta&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I get the metadata, but in the case of (1) I do not get the DATE extracted, while in case (2) I do.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/mail/MailContentHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 17 May 2016 19:54:52 +0000" id="553" opendate="Mon, 16 May 2016 08:53:05 +0000">
		<buginformation>
			<summary>Email saved as .eml with no body not detected as rfc822, while same email saved as plain txt is.</summary>
			<description>&lt;p&gt;I save an email with no body text&lt;/p&gt;

&lt;p&gt;(1) by dragging it from Mac Mail so that an .eml file is created&lt;br/&gt;
(2) by using 'Save As' in Mac Mail so that a .txt file is created&lt;/p&gt;

&lt;p&gt;I then feed the files to Tika Server with the following command&lt;/p&gt;

&lt;p&gt;curl -T filename &lt;a href=&quot;http://localhost:9998/detect/stream&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://localhost:9998/detect/stream&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In case (1) the response is text/plain, while in case (2) the response is message/rfc822. This is strange, since (1) includes the full raw header, while (2) only includes a very abbreviated header.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 21 May 2017 15:40:36 +0000" id="554" opendate="Thu, 23 Jun 2016 19:12:05 +0000">
		<buginformation>
			<summary>A parser that combines Apache OpenNLP and Apache Tika and provides facilities for automatically deriving sentiment from text.</summary>
			<description>&lt;p&gt;A new project that implements a parser that uses Apache OpenNLP and Apache Tika to perform Sentiment Analysis.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/sentiment/analysis/SentimentParser.java</file>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/sentiment/analysis/SentimentParserTest.java</file>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 1 Nov 2016 12:16:40 +0000" id="555" opendate="Tue, 16 Aug 2016 19:35:35 +0000">
		<buginformation>
			<summary>Installing exiftool causes ForkParserIntegration test errors</summary>
			<description>&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rgauss&quot; class=&quot;user-hover&quot; rel=&quot;rgauss&quot;&gt;Ray Gauss II&lt;/a&gt; maybe you can help me with this. For some reason when I was trying your PR, I got all sorts of weird errors that I thought had to do with your PR, but in fact, had to do with Fork Parser Integration test. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=kkrugler&quot; class=&quot;user-hover&quot; rel=&quot;kkrugler&quot;&gt;Ken Krugler&lt;/a&gt; I've seen you've contributed to the Fork parser tests so tagging you on this too. Any reason you guys can think of that exiftool causes the Fork parser integration tests to fail?&lt;/p&gt;

&lt;p&gt;Here's the log msg (that I thought was due to the Sentiment parser, but is in fact not!):&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;[INFO] Changes detected - recompiling the module!
[INFO] Compiling 124 source files to /Users/mattmann/tmp/tika1.14/tika-parsers/target/test-classes
[INFO] /Users/mattmann/tmp/tika1.14/tika-parsers/src/test/java/org/apache/tika/parser/odf/ODFParserTest.java: Some input files use or override a deprecated API.
[INFO] /Users/mattmann/tmp/tika1.14/tika-parsers/src/test/java/org/apache/tika/parser/odf/ODFParserTest.java: Recompile with -Xlint:deprecation for details.
[INFO] 
[INFO] --- maven-surefire-plugin:2.18.1:test (default-test) @ tika-parsers ---
[INFO] Surefire report directory: /Users/mattmann/tmp/tika1.14/tika-parsers/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.tika.parser.fork.ForkParserIntegrationTest
Tests run: 5, Failures: 1, Errors: 3, Skipped: 0, Time elapsed: 2.46 sec &amp;lt;&amp;lt;&amp;lt; FAILURE! - in org.apache.tika.parser.fork.ForkParserIntegrationTest
testForkedTextParsing(org.apache.tika.parser.fork.ForkParserIntegrationTest)  Time elapsed: 0.185 sec  &amp;lt;&amp;lt;&amp;lt; ERROR!
org.apache.tika.exception.TikaException: Unable to serialize AutoDetectParser to pass to the Forked Parser
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1184)
    at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
    at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
    at java.util.ArrayList.writeObject(ArrayList.java:762)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1028)
    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
    at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
    at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
    at java.util.ArrayList.writeObject(ArrayList.java:762)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1028)
    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
    at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
    at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
    at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
    at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
    at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
    at org.apache.tika.fork.ForkObjectInputStream.sendObject(ForkObjectInputStream.java:84)
    at org.apache.tika.fork.ForkClient.sendObject(ForkClient.java:151)
    at org.apache.tika.fork.ForkClient.&amp;lt;init&amp;gt;(ForkClient.java:76)
    at org.apache.tika.fork.ForkParser.acquireClient(ForkParser.java:216)
    at org.apache.tika.fork.ForkParser.parse(ForkParser.java:168)
    at org.apache.tika.parser.fork.ForkParserIntegrationTest.testForkedTextParsing(ForkParserIntegrationTest.java:66)

testAttachingADebuggerOnTheForkedParserShouldWork(org.apache.tika.parser.fork.ForkParserIntegrationTest)  Time elapsed: 0.534 sec  &amp;lt;&amp;lt;&amp;lt; ERROR!
org.apache.tika.exception.TikaException: Unable to serialize AutoDetectParser to pass to the Forked Parser
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1184)
    at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
    at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
    at java.util.ArrayList.writeObject(ArrayList.java:762)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1028)
    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
    at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
    at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
    at java.util.ArrayList.writeObject(ArrayList.java:762)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1028)
    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
    at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
    at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
    at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
    at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
    at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
    at org.apache.tika.fork.ForkObjectInputStream.sendObject(ForkObjectInputStream.java:84)
    at org.apache.tika.fork.ForkClient.sendObject(ForkClient.java:151)
    at org.apache.tika.fork.ForkClient.&amp;lt;init&amp;gt;(ForkClient.java:76)
    at org.apache.tika.fork.ForkParser.acquireClient(ForkParser.java:216)
    at org.apache.tika.fork.ForkParser.parse(ForkParser.java:168)
    at org.apache.tika.parser.fork.ForkParserIntegrationTest.testAttachingADebuggerOnTheForkedParserShouldWork(ForkParserIntegrationTest.java:234)

testForkedPDFParsing(org.apache.tika.parser.fork.ForkParserIntegrationTest)  Time elapsed: 0.134 sec  &amp;lt;&amp;lt;&amp;lt; ERROR!
org.apache.tika.exception.TikaException: Unable to serialize AutoDetectParser to pass to the Forked Parser
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1184)
    at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
    at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
    at java.util.ArrayList.writeObject(ArrayList.java:762)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1028)
    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
    at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
    at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
    at java.util.ArrayList.writeObject(ArrayList.java:762)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1028)
    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
    at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
    at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
    at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
    at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
    at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
    at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
    at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
    at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
    at org.apache.tika.fork.ForkObjectInputStream.sendObject(ForkObjectInputStream.java:84)
    at org.apache.tika.fork.ForkClient.sendObject(ForkClient.java:151)
    at org.apache.tika.fork.ForkClient.&amp;lt;init&amp;gt;(ForkClient.java:76)
    at org.apache.tika.fork.ForkParser.acquireClient(ForkParser.java:216)
    at org.apache.tika.fork.ForkParser.parse(ForkParser.java:168)
    at org.apache.tika.parser.fork.ForkParserIntegrationTest.testForkedPDFParsing(ForkParserIntegrationTest.java:257)

testParserHandlingOfNonSerializable(org.apache.tika.parser.fork.ForkParserIntegrationTest)  Time elapsed: 0.134 sec  &amp;lt;&amp;lt;&amp;lt; FAILURE!
org.junit.ComparisonFailure: expected:&amp;lt;Unable to serialize [ParseContext] to pass to the Fork...&amp;gt; but was:&amp;lt;Unable to serialize [AutoDetectParser] to pass to the Fork...&amp;gt;
    at org.junit.Assert.assertEquals(Assert.java:115)
    at org.junit.Assert.assertEquals(Assert.java:144)
    at org.apache.tika.parser.fork.ForkParserIntegrationTest.testParserHandlingOfNonSerializable(ForkParserIntegrationTest.java:210)


Results :

Failed tests: 
  ForkParserIntegrationTest.testParserHandlingOfNonSerializable:210 expected:&amp;lt;Unable to serialize [ParseContext] to pass to the Fork...&amp;gt; but was:&amp;lt;Unable to serialize [AutoDetectParser] to pass to the Fork...&amp;gt;
Tests in error: 
  ForkParserIntegrationTest.testAttachingADebuggerOnTheForkedParserShouldWork:234 » Tika
  ForkParserIntegrationTest.testForkedPDFParsing:257 » Tika Unable to serialize ...
  ForkParserIntegrationTest.testForkedTextParsing:66 » Tika Unable to serialize ...

Tests run: 5, Failures: 1, Errors: 3, Skipped: 0

[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 11.841 s
[INFO] Finished at: 2016-08-15T14:36:46-07:00
[INFO] Final Memory: 53M/987M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test (default-test) on project tika-parsers: There are test failures.
[ERROR] 
[ERROR] Please refer to /Users/mattmann/tmp/tika1.14/tika-parsers/target/surefire-reports for the individual test results.
[ERROR] -&amp;gt; [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
LMC-053601:tika-parsers mattmann$ git status
On branch TIKA-2016
Changes not staged for commit:
  (use &quot;git add &amp;lt;file&amp;gt;...&quot; to update what will be committed)
  (use &quot;git checkout -- &amp;lt;file&amp;gt;...&quot; to discard changes in working directory)

    modified:   pom.xml
    modified:   src/main/java/org/apache/tika/parser/sentiment/analysis/SentimentParser.java

Untracked files:
  (use &quot;git add &amp;lt;file&amp;gt;...&quot; to include in what will be committed)

    ../chris.sent
    ../language-keys/
    ../tensorflow/
    pom.xml~
    src/main/java/org/apache/tika/parser/sentiment/analysis/SentimentParser.java~
    tensorflow/

no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)
LMC-053601:tika-parsers mattmann$ cd ..
LMC-053601:tika1.14 mattmann$ !more
more tika-parsers/target/surefire-reports/org.apache.tika.parser.fork.ForkParserIntegrationTest.txt 
-------------------------------------------------------------------------------
Test set: org.apache.tika.parser.fork.ForkParserIntegrationTest
-------------------------------------------------------------------------------
Tests run: 5, Failures: 1, Errors: 3, Skipped: 0, Time elapsed: 2.46 sec &amp;lt;&amp;lt;&amp;lt; FAILURE! - in org.apache.tika.parser.fork.ForkParserIntegrationTest
testForkedTextParsing(org.apache.tika.parser.fork.ForkParserIntegrationTest)  Time elapsed: 0.185 sec  &amp;lt;&amp;lt;&amp;lt; ERROR!
org.apache.tika.exception.TikaException: Unable to serialize AutoDetectParser to pass to the Forked Parser
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1184)
        at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
        at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
        at java.util.ArrayList.writeObject(ArrayList.java:762)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1028)
        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
        at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
        at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
        at java.util.ArrayList.writeObject(ArrayList.java:762)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1028)
        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
        at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
        at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
        at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
        at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
        at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
        at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
        at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
        at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
        at org.apache.tika.fork.ForkObjectInputStream.sendObject(ForkObjectInputStream.java:84)
        at org.apache.tika.fork.ForkClient.sendObject(ForkClient.java:151)
        at org.apache.tika.fork.ForkClient.&amp;lt;init&amp;gt;(ForkClient.java:76)
        at org.apache.tika.fork.ForkParser.acquireClient(ForkParser.java:216)
        at org.apache.tika.fork.ForkParser.parse(ForkParser.java:168)
        at org.apache.tika.parser.fork.ForkParserIntegrationTest.testForkedTextParsing(ForkParserIntegrationTest.java:66)

testAttachingADebuggerOnTheForkedParserShouldWork(org.apache.tika.parser.fork.ForkParserIntegrationTest)  Time elapsed: 0.534 sec  &amp;lt;&amp;lt;&amp;lt; ERROR!
org.apache.tika.exception.TikaException: Unable to serialize AutoDetectParser to pass to the Forked Parser
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1184)
        at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
        at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
        at java.util.ArrayList.writeObject(ArrayList.java:762)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
-------------------------------------------------------------------------------
Test set: org.apache.tika.parser.fork.ForkParserIntegrationTest
-------------------------------------------------------------------------------
Tests run: 5, Failures: 1, Errors: 3, Skipped: 0, Time elapsed: 2.46 sec &amp;lt;&amp;lt;&amp;lt; FAILURE! - in org.apache.tika.parser.fork.ForkParserIntegrationTest
testForkedTextParsing(org.apache.tika.parser.fork.ForkParserIntegrationTest)  Time elapsed: 0.185 sec  &amp;lt;&amp;lt;&amp;lt; ERROR!
org.apache.tika.exception.TikaException: Unable to serialize AutoDetectParser to pass to the Forked Parser
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1184)
        at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
        at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
        at java.util.ArrayList.writeObject(ArrayList.java:762)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1028)
        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
        at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
        at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1028)
        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
        at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
        at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
        at java.util.ArrayList.writeObject(ArrayList.java:762)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:1028)
        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
        at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
        at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
        at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
        at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
        at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
        at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
        at org.apache.tika.fork.ForkObjectInputStream.sendObject(ForkObjectInputStream.java:84)
        at org.apache.tika.fork.ForkClient.sendObject(ForkClient.java:151)
        at org.apache.tika.fork.ForkClient.&amp;lt;init&amp;gt;(ForkClient.java:76)
        at org.apache.tika.fork.ForkParser.acquireClient(ForkParser.java:216)
        at org.apache.tika.fork.ForkParser.parse(ForkParser.java:168)
        at org.apache.tika.parser.fork.ForkParserIntegrationTest.testForkedTextParsing(ForkParserIntegrationTest.java:66)

testAttachingADebuggerOnTheForkedParserShouldWork(org.apache.tika.parser.fork.ForkParserIntegrationTest)  Time elapsed: 0.534 sec  &amp;lt;&amp;lt;&amp;lt; ERROR!
org.apache.tika.exception.TikaException: Unable to serialize AutoDetectParser to pass to the Forked Parser
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1184)
LMC-053601:tika1.14 mattmann$ emacs tika-core/src/main/java/org/apache/tika/fork/ForkObjectInputStream.java 
LMC-053601:tika1.14 mattmann$ 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/parser/external/ExternalParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 8 Nov 2016 02:31:50 +0000" id="556" opendate="Tue, 8 Nov 2016 02:28:09 +0000">
		<buginformation>
			<summary>Upgrade SQLite to 3.15.1</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 9 Nov 2016 18:57:29 +0000" id="557" opendate="Wed, 9 Nov 2016 11:59:45 +0000">
		<buginformation>
			<summary>Too few formats in support declared by TesseractOCRParser</summary>
			<description>&lt;p&gt;A complete install of Leptonica with Tesseract will add support for formats that are not declared by TesseractOCRParser. These include JP2, JPX and PPM.&lt;/p&gt;

&lt;p&gt;Tesseract produces OCR output fine for JPX images as of this version:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;  $ tesseract -v
     tesseract 3.04.01
       leptonica-1.73
         libjpeg 8d : libpng 1.6.26 : libtiff 4.0.6 : zlib 1.2.5}}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;However, these types are not declared by getSupportTypes so no output is produced for PDFs which contained JPX images of scanned documents, for example.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/ocr/TesseractOCRParser.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 23 Nov 2016 20:29:58 +0000" id="558" opendate="Tue, 15 Nov 2016 19:12:28 +0000">
		<buginformation>
			<summary>WordMLParser fails to parse a word xml file</summary>
			<description>&lt;h3&gt;&lt;a name=&quot;Problem&quot;&gt;&lt;/a&gt;Problem&lt;/h3&gt;
&lt;p&gt;I have a sample word xml file (attached as File5.xml) that can be parsed by neither OOXMLParser (yields an exception that was &lt;tt&gt;Caused by: org.apache.poi.openxml4j.exceptions.NotOfficeXmlFileException: The supplied data appears to be a raw XML file. Formats such as Office 2003 XML are not supported&lt;/tt&gt;) nor by OfficeParser (yields an exception like: &lt;tt&gt;org.apache.poi.poifs.filesystem.NotOLE2FileException: The supplied data appears to be a raw XML file. Formats such as Office 2003 XML are not supported&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;I found &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1958&quot; title=&quot;Add mime detection and lightweight parsers for Office 2003 Word and Excel formats&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1958&quot;&gt;&lt;del&gt;TIKA-1958&lt;/del&gt;&lt;/a&gt; which mentioned the new WordMLParser, so downloaded the source, built, and updated my tika version to 1.14. However, when parsing with WordMLParser, the output text content I get is the empty string &lt;tt&gt;&quot;&quot;&lt;/tt&gt;, but I'm expecting something more like:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;It means that the guy that you are trading with was reported for a scam attempt. As the others mentioned, some of these BOFA could be false.
What's important is the current trade that you are doing.
If everything seems to be in order then there is nothing wrong with going through with the trade.
Auti, Sneha (QAPM)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;h3&gt;&lt;a name=&quot;Replication&quot;&gt;&lt;/a&gt;Replication&lt;/h3&gt;
&lt;p&gt;You can replicate with the below Spock test&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;    def &quot;display error with WordMLParser&quot;(){
        setup:
        File input = new File(&quot;/Users/sstory/Downloads/File5.xml&quot;) //modify for your path
        Parser parser = new WordMLParser()
        //Parser parser = new OOXMLParser()
        //Parser parser = new OfficeParser()
        org.xml.sax.ContentHandler textHandler = new BodyContentHandler(-1)
        Metadata metadata = new Metadata()
        ParseContext context = new ParseContext()
        
        when:
        parser.parse(input.newInputStream(), textHandler, metadata, context)
        String result = textHandler.toString()

        then:
        !result.isEmpty()
        result.contains(&quot;the guy that you are trading with&quot;)
        result.contains(&quot;BOFA&quot;)
    }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 13 Dec 2016 01:29:30 +0000" id="559" opendate="Tue, 13 Dec 2016 01:17:21 +0000">
		<buginformation>
			<summary>Consolidate MockParser's service loading file and custom-mimetype entry into tika-core's tests jar</summary>
			<description>&lt;p&gt;We duplicate the service loading file and the custom mimetype for MockParser in tika-parsers and tika-server.  Let's consolidate those into tika-core/test.  This will have the added benefit that integrators can drop the tika-core-X.Y-tests.jar into their classpath and easily unit test their handling of potentially badly behaving parsers.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 6 Feb 2017 15:30:26 +0000" id="560" opendate="Tue, 13 Dec 2016 17:51:40 +0000">
		<buginformation>
			<summary>NullPointerException on a valid Word file</summary>
			<description>&lt;p&gt;On the attached file, which opens fine in Word, the Tika parser throws the following error:&lt;/p&gt;

&lt;p&gt;java.lang.NullPointerException: &lt;br/&gt;
	at org.apache.poi.hwpf.model.ListTables.getLevel:141&lt;br/&gt;
	at org.apache.poi.hwpf.usermodel.Paragraph.newParagraph:125&lt;br/&gt;
	at org.apache.poi.hwpf.usermodel.Range.getParagraph:766&lt;br/&gt;
	at org.apache.tika.parser.microsoft.WordExtractor.parse:178&lt;br/&gt;
	at org.apache.tika.parser.microsoft.OfficeParser.parse:169&lt;br/&gt;
	at org.apache.tika.parser.microsoft.OfficeParser.parse:130&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ListManager.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 11 Jan 2017 15:38:52 +0000" id="561" opendate="Sat, 17 Dec 2016 00:42:06 +0000">
		<buginformation>
			<summary>Add experimental SAX/Streaming XSLF/pptx extractor</summary>
			<description>&lt;p&gt;On &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-2201&quot; title=&quot;OutOfMemoryError on a reasonably sized document&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-2201&quot;&gt;&lt;del&gt;TIKA-2201&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sevaa&quot; class=&quot;user-hover&quot; rel=&quot;sevaa&quot;&gt;Seva Alekseyev&lt;/a&gt; shared a reasonably sized pptx that caused an OOM.  While the SAX docx parser is still fresh in my mind, let's add one for pptx.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 21 Dec 2016 14:11:52 +0000" id="562" opendate="Sun, 18 Dec 2016 15:09:44 +0000">
		<buginformation>
			<summary>ePub formatting instructions appear in plain text output</summary>
			<description>&lt;p&gt;For some ePub files, format information appears in the plain text output produced by Apache Tika.  For example the Tika stand-alone application shows the following text for the file “Don Quijote de la Mancha - Miguel de Cervantes.epub” (dowloaded &lt;a href=&quot;http://www.literanda.com/don-quijote-de-la-mancha--miguel-de-cervantes--epub&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;):&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-comment&quot;&gt;/**/&lt;/span&gt;

  p.sgc-2 {font-style: italic; text-align: right}
  p.sgc-1 {text-align: justify;}

  h3.sgc-3 {text-align: center;}
  &lt;span class=&quot;code-comment&quot;&gt;/**/&lt;/span&gt;

Al duque de Béjar

Marqués de Gibraleón, conde de Benalcázar y Bañares, vizconde de La Puebla de Alcocer, señor de las villas de Capilla, Curiel y Burguillos

En fe del buen acogimiento y honra que hace Vuestra Excelencia a toda suerte de libros, como príncipe tan inclinado a favorecer las buenas artes, mayormente las que por su nobleza no se abaten al servicio y granjerías del vulgo, he determinado de sacar a luz El ingenioso hidalgo don Quijote de la Mancha, al abrigo del clarísimo nombre de Vuestra Excelencia, a quien, con el acatamiento que debo a tanta grandeza, suplico le reciba agradablemente en su protección, para que a su sombra, aunque desnudo de aquel precioso ornamento de elegancia y erudición de que suelen andar vestidas las obras que se componen en las casas de los hombres que saben, ose parecer seguramente en el juicio de algunos que, conteniéndose en los límites de su ignorancia, suelen condenar con más rigor y menos justicia los trabajos ajenos; que, poniendo los ojos la prudencia de Vuestra Excelencia en mi buen deseo, fío que no desdeñará la cortedad de tan humilde servicio.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To reproduce this problem run the stand-alone version of Tika and open an affected ePub file such as the one mentioned above.  Then go to View -&amp;gt; Plain Text.  You should see the problem there.&lt;/p&gt;

&lt;p&gt;By the way, thanks for making Apache Tika a really useful library.  Keep up the good work!&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/epub/EpubContentParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 20 Dec 2016 19:26:37 +0000" id="563" opendate="Mon, 19 Dec 2016 22:12:23 +0000">
		<buginformation>
			<summary>CharsetDetector no longer detects windows-1252 charset</summary>
			<description>&lt;p&gt;Starting with Tika 1.14, windows-1252 is no longer detected, as ISO-8859-1 is always detected instead.  While not tested, this likely affects other windows-125* encodings as well.&lt;/p&gt;

&lt;p&gt;I tracked it down to a change in the &lt;tt&gt;CharsetRecog_sbcs.CharsetRecog_8859_1#getName()&lt;/tt&gt; method.  Now it always returns &quot;ISO-8859-1&quot; whereas before it was: &lt;tt&gt;return haveC1Bytes ? &quot;windows-1252&quot; : &quot;ISO-8859-1&quot;;&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;Now that condition has been moved to the &lt;tt&gt;match(CharsetDetector det)&lt;/tt&gt; method so that the returned CharsetMatch has the proper name.  The problem with that is &lt;tt&gt;CharsetDetector#detectAll()&lt;/tt&gt; method overwrites the correct match with a new one that will return the value of &lt;tt&gt;#getName()&lt;/tt&gt;  from the &lt;tt&gt;CharsetRecognizer&lt;/tt&gt; instead (which is always &quot;ISO-8859-1&quot; in this case).&lt;/p&gt;

&lt;p&gt;There might be legitimate reasons why the &lt;tt&gt;CharsetMatch&lt;/tt&gt; instances in &lt;tt&gt;detectAll()&lt;/tt&gt; method are replaced with new ones, but changing this code in that method appears to work for me:&lt;/p&gt;

&lt;p&gt;// Remove this:&lt;br/&gt;
//                    CharsetMatch m = new CharsetMatch(this, csr, confidence);&lt;br/&gt;
//                    matches.add(m);&lt;/p&gt;

&lt;p&gt;// Add this instead:&lt;br/&gt;
                    matches.add(charsetMatch);&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/txt/CharsetDetector.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 20 Dec 2016 18:16:54 +0000" id="564" opendate="Tue, 20 Dec 2016 15:03:03 +0000">
		<buginformation>
			<summary>Refactor/merge new experimental docx/pptx components</summary>
			<description>&lt;p&gt;We can get rid of a fair amount of duplicate code by merging the docx and pptx SAX handlers.  If we find significant differences in future desired functionality, we can split them back out.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/AbstractDocumentXMLBodyHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 20 Dec 2016 18:30:18 +0000" id="565" opendate="Tue, 20 Dec 2016 17:04:50 +0000">
		<buginformation>
			<summary>poi.EncryptedDocumentException not wrapped in tika.exception.EncryptedDocumentException</summary>
			<description>&lt;p&gt;When parsing an encrypted Word document, a org.apache.poi.EncryptedDocumentException is thrown at WordExtractor.java#151. Tika catches this too far up the stack and incorrectly wraps it in a plain TikaException instead of a org.apache.tika.exception.EncryptedDocumentException.&lt;/p&gt;

&lt;p&gt;The fix would be to catch and wrap the exception correctly, for example:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;        try {
            document = new HWPFDocument(root);
        } catch (org.apache.poi.EncryptedDocumentException e) {
            throw new EncryptedDocumentException(e);
        } catch (OldWordFileFormatException e) {
            parseWord6(root, xhtml);
            return;
        }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/WordExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 4 Jan 2017 21:06:04 +0000" id="566" opendate="Thu, 22 Dec 2016 01:00:04 +0000">
		<buginformation>
			<summary>Mime magic for OneNote formats</summary>
			<description>&lt;p&gt;As raised at &lt;a href=&quot;http://stackoverflow.com/questions/41272195/onenote-support-for-apache-tika-parsers&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://stackoverflow.com/questions/41272195/onenote-support-for-apache-tika-parsers&lt;/a&gt;, we don't have any magic for the OneNote formats. Several years ago we dug out the file format specs (see &lt;a href=&quot;http://lucene.472066.n3.nabble.com/Tika-OneNote-Support-td4020393.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://lucene.472066.n3.nabble.com/Tika-OneNote-Support-td4020393.html&lt;/a&gt;), but didn't have volunteer energy to implement a parser. However, armed with those specs, we should be able to come up with some mime magic for detection&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
			<file>/tika-core/src/test/java/org/apache/tika/TikaDetectionTest.java</file>
			<file>/tika-parsers/src/test/java/org/apache/tika/mime/TestMimeTypes.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 18 Jan 2017 02:39:11 +0000" id="567" opendate="Thu, 5 Jan 2017 20:27:21 +0000">
		<buginformation>
			<summary>Invalid language code exception</summary>
			<description>&lt;p&gt;There is a regex in TesseractOCRConfig.setLanguage(String language) which attempts to validate the language being set.  Unfortunately it does not allow you to set some languages that are valid for tesseract.&lt;/p&gt;

&lt;p&gt;For example:&lt;/p&gt;

&lt;p&gt;TesseractOCRConfig config = new TesseractOCRConfig();&lt;br/&gt;
config.setLanguage(&quot;chi_tra&quot;);&lt;/p&gt;

&lt;p&gt;This throws an IllegalArgumentException because of the '_' in the language name.  &quot;chi_tra&quot; is a valid tesseract language code.&lt;/p&gt;

&lt;p&gt;Need to update the regex to allow '_' character.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/ocr/TesseractOCRConfig.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 13 Jan 2017 20:42:52 +0000" id="568" opendate="Fri, 6 Jan 2017 02:22:47 +0000">
		<buginformation>
			<summary>Add JBIG2 image parsing support</summary>
			<description>&lt;p&gt;If you are interested, I would like to add support for JBIG2 image files (.jb2, or .jbig2).  I have encountered them PDFs.&lt;/p&gt;

&lt;p&gt;I will make a pull-request shortly.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/image/ImageParserTest.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/image/ImageParser.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 11 Jan 2017 12:09:19 +0000" id="569" opendate="Wed, 11 Jan 2017 11:48:20 +0000">
		<buginformation>
			<summary>Use Tesseract's recommended DPI for PDF images</summary>
			<description>&lt;p&gt;From the &lt;a href=&quot;https://github.com/tesseract-ocr/tesseract/wiki/ImproveQuality&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;Tesseract wiki&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Tesseract works best on images which have a DPI of at least 300 dpi....&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;PDFParserConfig is currently initialised with a value of 200 for ocrDPI.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDFParserConfig.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 11 Jan 2017 15:20:25 +0000" id="570" opendate="Wed, 11 Jan 2017 14:41:03 +0000">
		<buginformation>
			<summary>UnsupportedOperationException due to SingletonList.set in ProbabilisticMimeDetectionSelector</summary>
			<description>
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;java.lang.UnsupportedOperationException
	at java.util.AbstractList.set(AbstractList.java:132)
	at org.apache.tika.mime.ProbabilisticMimeDetectionSelector.applyProbilities(ProbabilisticMimeDetectionSelector.java:241)
	at org.apache.tika.mime.ProbabilisticMimeDetectionSelector.detect(ProbabilisticMimeDetectionSelector.java:190)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/mime/ProbabilisticMimeDetectionSelector.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 18 Jan 2017 15:25:26 +0000" id="571" opendate="Tue, 17 Jan 2017 08:05:42 +0000">
		<buginformation>
			<summary> DumpTikaConfigExample generates strange tika-config.xml</summary>
			<description>&lt;div class=&quot;code panel&quot; style=&quot;border-style: solid;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-none&quot;&gt;mvn exec:java -Dexec.mainClass=&quot;org.apache.tika.example.DumpTikaConfigExample&quot; -Dexec.arguments=&quot;--dump-static&quot;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Tika 1.8 used to produce something like:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-style: solid;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-xml&quot;&gt;&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;?xml version=&lt;span class=&quot;code-quote&quot;&gt;&quot;1.0&quot;&lt;/span&gt; encoding=&lt;span class=&quot;code-quote&quot;&gt;&quot;UTF-8&quot;&lt;/span&gt; standalone=&lt;span class=&quot;code-quote&quot;&gt;&quot;no&quot;&lt;/span&gt;?&amp;gt;&lt;/span&gt;
&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;properties&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;code-tag&quot;&gt;&lt;span class=&quot;code-comment&quot;&gt;&amp;lt;!--for example: &amp;lt;mimeTypeRepository resource=&lt;span class=&quot;code-quote&quot;&gt;&quot;/org/apache/tika/mime/tika-mimetypes.xml&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;--&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;code-tag&quot;&gt;&lt;span class=&quot;code-comment&quot;&gt;&amp;lt;!--for example: &amp;lt;translator class=&lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.tika.language.translate.GoogleTranslator&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;--&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;detectors&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;detector class=&lt;span class=&quot;code-quote&quot;&gt;&quot;org.gagravarr.tika.OggDetector&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;detector class=&lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.tika.parser.microsoft.POIFSContainerDetector&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;detector class=&lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.tika.parser.pkg.ZipContainerDetector&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;detector class=&lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.tika.mime.MimeTypes&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/detectors&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;parsers&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;parser class=&lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.tika.parser.asm.ClassParser&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;mime&amp;gt;&lt;/span&gt;application/java-vm&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/mime&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/parser&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;parser class=&lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.tika.parser.audio.AudioParser&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;mime&amp;gt;&lt;/span&gt;audio/basic&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/mime&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;mime&amp;gt;&lt;/span&gt;audio/x-aiff&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/mime&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;mime&amp;gt;&lt;/span&gt;audio/x-wav&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/mime&amp;gt;&lt;/span&gt;
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With Tika 1.14 I get:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-style: solid;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-xml&quot;&gt;&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;?xml version=&lt;span class=&quot;code-quote&quot;&gt;&quot;1.0&quot;&lt;/span&gt; encoding=&lt;span class=&quot;code-quote&quot;&gt;&quot;UTF-8&quot;&lt;/span&gt; standalone=&lt;span class=&quot;code-quote&quot;&gt;&quot;no&quot;&lt;/span&gt;?&amp;gt;&lt;/span&gt;
&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;properties&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;code-tag&quot;&gt;&lt;span class=&quot;code-comment&quot;&gt;&amp;lt;!--for example: &amp;lt;mimeTypeRepository resource=&lt;span class=&quot;code-quote&quot;&gt;&quot;/org/apache/tika/mime/tika-mimetypes.xml&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;--&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;service-loader dynamic=&lt;span class=&quot;code-quote&quot;&gt;&quot;true&quot;&lt;/span&gt; loadErrorHandler=&lt;span class=&quot;code-quote&quot;&gt;&quot;IGNORE&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;code-tag&quot;&gt;&lt;span class=&quot;code-comment&quot;&gt;&amp;lt;!--No translators available--&amp;gt;&lt;/span&gt;&lt;/span&gt;
  &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;detectors&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;detector class=&lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.tika.parser.microsoft.POIFSContainerDetector&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;detector class=&lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.tika.parser.microsoft.POIFSContainerDetector&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;detector class=&lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.tika.parser.pkg.ZipContainerDetector&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;detector class=&lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.tika.parser.pkg.ZipContainerDetector&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;detector class=&lt;span class=&quot;code-quote&quot;&gt;&quot;org.gagravarr.tika.OggDetector&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;detector class=&lt;span class=&quot;code-quote&quot;&gt;&quot;org.gagravarr.tika.OggDetector&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;detector class=&lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.tika.mime.MimeTypes&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/detectors&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;parsers&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;parser class=&lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.tika.parser.apple.AppleSingleFileParser&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;parser class=&lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.tika.parser.apple.AppleSingleFileParser&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;parser class=&lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.tika.parser.asm.ClassParser&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;parser class=&lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.tika.parser.asm.ClassParser&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;parser class=&lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.tika.parser.audio.AudioParser&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;parser class=&lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.tika.parser.audio.AudioParser&quot;&lt;/span&gt;/&amp;gt;&lt;/span&gt;
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following problems IMHO:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Why are all classes in double?&lt;/li&gt;
	&lt;li&gt;As I understood the order of the parsers matters: earliler ones&lt;br/&gt;
  take precendence over later ones?&lt;/li&gt;
	&lt;li&gt;Matching MIME-types are missing completely?&lt;/li&gt;
&lt;/ul&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 10 Mar 2017 19:35:00 +0000" id="572" opendate="Tue, 17 Jan 2017 11:05:16 +0000">
		<buginformation>
			<summary>opendocument parsing produces malformed xml</summary>
			<description>&lt;p&gt;For some odt documents, a malformed xml is produced when parsing. &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/sax/ElementMappingContentHandler.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 23 Jan 2017 18:40:12 +0000" id="573" opendate="Mon, 23 Jan 2017 18:03:35 +0000">
		<buginformation>
			<summary>Remove the x- prefix for some Microsoft image format mimetypes, eg BMP</summary>
			<description>&lt;p&gt;Our main mime type for a number of Microsoft image formats, such as BMP or EMF, have an x- prefix because they weren't officially assigned&lt;/p&gt;

&lt;p&gt;In September of last year, Microsoft got round to doing the paperwork for getting several of these officially recognised, see &lt;a href=&quot;https://tools.ietf.org/html/rfc7903&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://tools.ietf.org/html/rfc7903&lt;/a&gt; , so the main / canonical type can now be the one without the x-&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 23 Jan 2017 18:40:12 +0000" id="574" opendate="Thu, 26 Jan 2017 13:26:22 +0000">
		<buginformation>
			<summary>TIKA-198 due to java.util.zip.ZipException: invalid literal/lengths set</summary>
			<description>&lt;p&gt;I got an exception to extract text from file. See stacktrace associated and file attached to reproduce:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;org.apache.tika.exception.TikaException: TIKA-198: Illegal IOException from org.apache.tika.parser.microsoft.ooxml.OOXMLParser@7f54cc49
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:286)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:280)
	... 16 more
Caused by: java.util.zip.ZipException: invalid literal/lengths set
	at java.util.zip.InflaterInputStream.read(InflaterInputStream.java:164)
	at java.util.zip.InflaterInputStream.read(InflaterInputStream.java:122)
	at org.apache.poi.openxml4j.util.ZipSecureFile$ThresholdInputStream.read(ZipSecureFile.java:207)
	at org.apache.xerces.impl.XMLEntityManager$RewindableInputStream.read(Unknown Source)
	at org.apache.xerces.impl.XMLEntityManager.setupCurrentEntity(Unknown Source)
	at org.apache.xerces.impl.XMLVersionDetector.determineDocVersion(Unknown Source)
	at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
	at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
	at org.apache.xerces.parsers.XMLParser.parse(Unknown Source)
	at org.apache.xerces.parsers.DOMParser.parse(Unknown Source)
	at org.apache.xerces.jaxp.DocumentBuilderImpl.parse(Unknown Source)
	at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:121)
	at org.apache.poi.util.DocumentHelper.readDocument(DocumentHelper.java:137)
	at org.apache.poi.POIXMLTypeLoader.parse(POIXMLTypeLoader.java:115)
	at org.openxmlformats.schemas.wordprocessingml.x2006.main.HdrDocument$Factory.parse(Unknown Source)
	at org.apache.poi.xwpf.usermodel.XWPFHeader.onDocumentRead(XWPFHeader.java:108)
	at org.apache.poi.xwpf.usermodel.XWPFDocument.onDocumentRead(XWPFDocument.java:212)
	at org.apache.poi.POIXMLDocument.load(POIXMLDocument.java:190)
	at org.apache.poi.xwpf.usermodel.XWPFDocument.&amp;lt;init&amp;gt;(XWPFDocument.java:124)
	at org.apache.poi.xwpf.extractor.XWPFWordExtractor.&amp;lt;init&amp;gt;(XWPFWordExtractor.java:58)
	at org.apache.poi.extractor.ExtractorFactory.createExtractor(ExtractorFactory.java:232)
	at org.apache.tika.parser.microsoft.ooxml.OOXMLExtractorFactory.parse(OOXMLExtractorFactory.java:86)
	at org.apache.tika.parser.microsoft.ooxml.OOXMLParser.parse(OOXMLParser.java:87)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:280)
	... 23 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/SXSLFPowerPointExtractorDecorator.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 27 Mar 2017 15:46:36 +0000" id="575" opendate="Thu, 26 Jan 2017 14:49:34 +0000">
		<buginformation>
			<summary>Obtain new Miredot license key and upgrade plugin version in tika-server</summary>
			<description>&lt;p&gt;As per our recent mailing list conversation &lt;a href=&quot;http://www.mail-archive.com/dev%40tika.apache.org/msg20558.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.mail-archive.com/dev%40tika.apache.org/msg20558.html&lt;/a&gt; our Miredot license has expired.&lt;br/&gt;
The kind folks over at Miredot have provided us with a new key it is valid until January 31st, 2020 after which we are free to request a new key.&lt;br/&gt;
Thanks Miredot!&lt;br/&gt;
PR coming.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-server/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 9 Jun 2017 01:46:41 +0000" id="576" opendate="Fri, 27 Jan 2017 15:07:50 +0000">
		<buginformation>
			<summary>Provide chart support for MS Office documents</summary>
			<description>&lt;p&gt;I would like to be able to extract text from the charts in XLSX documents.  Tim Allison has recommended that I raise this ticket which should cover it:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;On 23 Jan 2017, at 12:18, Allison, Timothy B. &amp;lt;tallison@mitre.org&amp;gt; wrote:

Hi Chris,
Good to hear from you. I don't know if it would help at all, but I'm planning to add chart support to Tika soon(ish). I haven't yet opened the ticket on Tika's JIRA, so please open one there too if that would be of any use to you.

Best,

Tim
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 30 Jan 2017 08:52:55 +0000" id="577" opendate="Mon, 30 Jan 2017 08:32:53 +0000">
		<buginformation>
			<summary>Test files for SAS mimetypes</summary>
			<description>&lt;p&gt;We have a number of mime types for files related to the SAS programming language/framework, but no test files for unit testing the detection. We should create some test files, so there are no license issues, then include them + test with them&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/resources/test-documents/testSAS.sas</file>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
			<file>/tika-parsers/src/test/java/org/apache/tika/mime/TestMimeTypes.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 21 Feb 2017 14:11:17 +0000" id="578" opendate="Mon, 20 Feb 2017 13:33:45 +0000">
		<buginformation>
			<summary>NPE with FeedParser</summary>
			<description>&lt;p&gt;Getting the NPE below when parsing &lt;a href=&quot;https://chm.tbe.taleo.net/chm02/ats/servlet/Rss?org=TSA&amp;amp;cws=43&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://chm.tbe.taleo.net/chm02/ats/servlet/Rss?org=TSA&amp;amp;cws=43&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Caused by: java.lang.NullPointerException&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;	at org.apache.tika.parser.feed.FeedParser.stripTags(FeedParser.java:119)&lt;br/&gt;
	at org.apache.tika.parser.feed.FeedParser.parse(FeedParser.java:74)&lt;br/&gt;
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:280)&lt;br/&gt;
	... 43 more&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 22 Feb 2017 13:42:35 +0000" id="579" opendate="Thu, 23 Feb 2017 02:09:02 +0000">
		<buginformation>
			<summary>Enable configuration of EncodingDetectors via TikaConfig</summary>
			<description>&lt;p&gt;It would be nice to allow easier configuration of encoding detectors.  It should be straightforward to follow the example of detectors...(famous last words).&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-bundle/src/test/java/org/apache/tika/bundle/BundleIT.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 23 Feb 2017 14:21:23 +0000" id="580" opendate="Thu, 23 Feb 2017 14:18:05 +0000">
		<buginformation>
			<summary>Remove ParseContext field from AbstractParser</summary>
			<description>&lt;p&gt;AbstractParser should not have a ParseContext field.  If any parsers actually relied on it, this could cause mayhem in multithreading environments.   &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/chm/ChmParser.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/parser/AbstractParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 23 Feb 2017 14:21:23 +0000" id="581" opendate="Mon, 27 Feb 2017 15:26:49 +0000">
		<buginformation>
			<summary>Clean up extract exception handling in tika-eval</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/tika-eval/src/main/java/org/apache/tika/eval/AbstractProfiler.java</file>
			<file>/tika-eval/src/main/java/org/apache/tika/eval/io/ExtractReaderException.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 8 Mar 2017 02:38:28 +0000" id="582" opendate="Mon, 6 Mar 2017 22:12:45 +0000">
		<buginformation>
			<summary>PDFParser 'ocr' properties cannot be set via headers when using Tika JAXRS</summary>
			<description>&lt;p&gt;I have created a stackoverflow question on this topic &lt;a href=&quot;http://stackoverflow.com/questions/42602834/x-tika-pdfocrstrategy-is-an-invalid-x-tika-ocr-header-error&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;here &lt;/a&gt;, but I'll reiterate the main issue. &lt;/p&gt;

&lt;p&gt;I am trying to use TikaJAXRS and add headers for setting PDFParser properties. Specifically the ocrStrategy property. However, when I add the header using X-Tika-PDFocrStrategy, I get an error stating that it is an invalid X-Tika-OCR header.&lt;/p&gt;

&lt;p&gt;After looking into the source code, I believe the issue might be with the 'fillParseContext' method in the TikaResource.java file.&lt;/p&gt;

&lt;p&gt;The if statement first looks for a key that starts with the OCR header prefix, and since the PDFParser's property name contains 'ocr', it is trying to find a property named 'ocrStrategy' in the OCRParser class, which doesn't exist.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-server/src/main/java/org/apache/tika/server/resource/TikaResource.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 12 Mar 2017 22:07:55 +0000" id="583" opendate="Tue, 7 Mar 2017 11:17:55 +0000">
		<buginformation>
			<summary>Update CXF version to 3.0.12</summary>
			<description>&lt;p&gt;This is the last version in the CXF 3.0.x line which supports Java 6&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-langdetect/pom.xml</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 13 Mar 2017 12:08:09 +0000" id="584" opendate="Sat, 11 Mar 2017 08:18:28 +0000">
		<buginformation>
			<summary>Add Lingo24 Language Detector</summary>
			<description>&lt;p&gt;Add LanguageDetector for the Lingo24 Premium MT API's /langid resource&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-langdetect/src/main/java/org/apache/tika/langdetect/Lingo24LangDetector.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 6 Jul 2017 01:13:01 +0000" id="585" opendate="Tue, 14 Mar 2017 18:35:35 +0000">
		<buginformation>
			<summary>To improve object recognition parser so that it may work without external RESTful service setup</summary>
			<description>&lt;p&gt;When ObjectRecognitionParser was built to do image recognition, there wasn't&lt;br/&gt;
good support for Java frameworks.  All the popular neural networks were in&lt;br/&gt;
C++ or python.  Since there was nothing that runs within JVM, we tried&lt;br/&gt;
several ways to glue them to Tika (like CLI, JNI, gRPC, REST).&lt;br/&gt;
However, this game is changing slowly now. Deeplearning4j, the most famous&lt;br/&gt;
neural network library for JVM, now supports importing models that are&lt;br/&gt;
pre-trained in python/C++ based kits &lt;span class=&quot;error&quot;&gt;&amp;#91;5&amp;#93;&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Improvement:&lt;/b&gt;&lt;br/&gt;
It will be nice to have an implementation of ObjectRecogniser that&lt;br/&gt;
doesn't require any external setup(like installation of native libraries or&lt;br/&gt;
starting REST services). Reasons: easy to distribute and also to cut the IO&lt;br/&gt;
time.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-dl/src/test/java/org/apache/tika/dl/imagerec/DL4JVGG16NetTest.java</file>
			<file>/tika-dl/src/main/java/org/apache/tika/dl/imagerec/DL4JVGG16Net.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 21 Mar 2017 19:23:54 +0000" id="586" opendate="Wed, 15 Mar 2017 11:13:33 +0000">
		<buginformation>
			<summary>Can't tell if a zip file is encrypted</summary>
			<description>&lt;p&gt;When Tika processes a zip file that is protected with a password, it will return the list of file names within the zip but no indication (as an exception or in metadata) that the file is encrypted. &lt;/p&gt;

&lt;p&gt;From stepping through the code, I can see that the information needed to determine whether the archive is encrypted is available inside ZipArchiveEntry#getGeneralPurposeBit#usesEncryption, but needs to be relayed back to PackageParser somehow&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pkg/PackageParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 27 Mar 2017 14:18:26 +0000" id="587" opendate="Thu, 16 Mar 2017 14:00:00 +0000">
		<buginformation>
			<summary>Make handling of macros equivalent btwn VBA in MSOffice and JS in PDFs</summary>
			<description>&lt;p&gt;The current default behavior is to extract VBA macros from MSOffice files but not to extract JS from PDFs.  Now that we have a config for MSOffice files, I propose changing the default behavior to NOT extract VBA macros from MSOffice files.  Users can opt in to extraction of macros via configuration.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/microsoft/ExcelParserTest.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 4 Jun 2018 12:09:58 +0000" id="588" opendate="Thu, 16 Mar 2017 15:33:51 +0000">
		<buginformation>
			<summary>PDFParser with optional bookmarks text extraction</summary>
			<description>&lt;p&gt;I would like to parse an PDF without extract its bookmarks and outlines.&lt;/p&gt;

&lt;p&gt;I was thinking about create a new PDFParser parameter in PDFParserConfig with a option such as 'ExtractBookmarks'. And check it out on 'AbstractPDF2XHTML'&lt;/p&gt;

&lt;p&gt;I can do it, and I would like to present you a patch with this change.&lt;/p&gt;

&lt;p&gt;Thanks in advance.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/AbstractPDF2XHTML.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 25 Apr 2017 16:55:37 +0000" id="589" opendate="Thu, 23 Mar 2017 13:49:55 +0000">
		<buginformation>
			<summary>New Detector and Parser classes for Time Stamped Data Envelope file format</summary>
			<description>&lt;p&gt;Hello,&lt;br/&gt;
I'm Fabio Evangelista from Rome. I'm working for an italian Public Administration company and i'm using Apache Tika in my Java applications to detect and parse a broad kinds of file formats. During that activity, after following your good guide on Tika project page, I've made with success new type of Detector and Parser classes for a particular crypto timestamp type with these caracteristics:&lt;/p&gt;

&lt;p&gt;Format name:               Time Stamped Data Envelope&lt;/p&gt;

&lt;p&gt;Mime Type:                   application/timestamped-data&lt;/p&gt;

&lt;p&gt;File extension:              .tsd&lt;/p&gt;

&lt;p&gt;TSD file hax magic code at the start of the file:   30 80 06 0B 2A 86 48 86 F7&lt;/p&gt;

&lt;p&gt;I've integrated and tested successfully with my applications those new classes in Tika 1.13 tika-core.jar and tika-parsers.jar. What should I do to submit my new classes to you? Should I to push those in a particular git branch or, is there a particular process to follow to submit my classes?&lt;/p&gt;

&lt;p&gt;Thank you for you patience and best regards.&lt;br/&gt;
Fabio.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 1 May 2017 19:23:28 +0000" id="590" opendate="Mon, 27 Mar 2017 14:56:46 +0000">
		<buginformation>
			<summary>[Mp3Parser] expose fields form ID3TagsAndAudio </summary>
			<description>&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;First of all that's my  first issue in ASF jira so sorry for mistakes.&lt;/p&gt;

&lt;p&gt;Currently I am working on some custom Parsers for MP3 files. The reason I would like to have access to fields in this class is that the system from which I am transforming data depends on availability of particular version ID3 tags and this class easily allow me to do that. &lt;/p&gt;

&lt;p&gt;Moreover in current code base the Mp3Parser expose method &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt; &lt;span class=&quot;code-keyword&quot;&gt;protected&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; ID3TagsAndAudio getAllTagHandlers(InputStream stream, ContentHandler handler)
           &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException, SAXException, TikaException {
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;and return object which haven't any accessible field. That's make me strange.&lt;/p&gt;

&lt;p&gt;Is there any reason why is it that?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 1 May 2017 19:23:28 +0000" id="591" opendate="Tue, 28 Mar 2017 07:49:11 +0000">
		<buginformation>
			<summary>Old Word document (Word 6.0, 1997) has a badly encoded(?) output.</summary>
			<description>&lt;p&gt;I've a really old Word document (last date of modification is December 1997) which was written with Microsoft Word 6.0.&lt;/p&gt;

&lt;p&gt;When I attempt to use Tika to extract the contents of the document, I receive an incorrect output. The output seems to be in Chinese, but I actually believe that the encoding of the document is not correctly mapped with the output encoding which causes characters to be thrown off. I'm a complete beginner in document encodings so could be wrong here!&lt;/p&gt;

&lt;p&gt;I did see &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-721&quot; title=&quot;UTF16-LE not detected&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-721&quot;&gt;TIKA-721&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-2038&quot; title=&quot;A more accurate facility for detecting Charset Encoding of HTML documents&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-2038&quot;&gt;TIKA-2038&lt;/a&gt;, but neither seem to be related to older documents. I've also read that Tika should support Word 6.0 so not sure.&lt;/p&gt;

&lt;p&gt;My guess for the moment is that the encoding within the document has incorrect character mappings. It's possible using an incompatible mapping that, when Tika converts into its UTF-16 output, maps to Chinese characters instead of the correct ones.&lt;/p&gt;

&lt;p&gt;What's interesting is that Tika correctly extracts all the metadata, including the document title, which is presumably in the same encoding as the document body.&lt;/p&gt;

&lt;p&gt;I have 2 questions:&lt;br/&gt;
1. Is there something I can pass to Tika to help out in detecting the encoding?&lt;br/&gt;
2. Is there a way of detecting this kind of bad output? In my application the number of documents like this is very small, but I don't have a very reliable way of detecting that the output is garbage.&lt;/p&gt;

&lt;p&gt;Like I said, quite a beginner with Tika so if there's any further commands you would like me to run please say.&lt;/p&gt;

&lt;p&gt;Here is the output of:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;java -jar tika-app-1.14.jar old.DOC&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;&amp;lt;html xmlns=&quot;http://www.w3.org/1999/xhtml&quot;&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;meta name=&quot;cp:revision&quot; content=&quot;3&quot;/&amp;gt;
&amp;lt;meta name=&quot;date&quot; content=&quot;1997-12-12T12:57:00Z&quot;/&amp;gt;
&amp;lt;meta name=&quot;meta:word-count&quot; content=&quot;38&quot;/&amp;gt;
&amp;lt;meta name=&quot;dc:creator&quot; content=&quot;Preferred Customer&quot;/&amp;gt;
&amp;lt;meta name=&quot;meta:print-date&quot; content=&quot;1997-12-12T11:31:00Z&quot;/&amp;gt;
&amp;lt;meta name=&quot;Word-Count&quot; content=&quot;38&quot;/&amp;gt;
&amp;lt;meta name=&quot;dcterms:created&quot; content=&quot;1997-12-12T11:31:00Z&quot;/&amp;gt;
&amp;lt;meta name=&quot;dcterms:modified&quot; content=&quot;1997-12-12T12:57:00Z&quot;/&amp;gt;
&amp;lt;meta name=&quot;Last-Modified&quot; content=&quot;1997-12-12T12:57:00Z&quot;/&amp;gt;
&amp;lt;meta name=&quot;Last-Save-Date&quot; content=&quot;1997-12-12T12:57:00Z&quot;/&amp;gt;
&amp;lt;meta name=&quot;meta:character-count&quot; content=&quot;227&quot;/&amp;gt;
&amp;lt;meta name=&quot;Template&quot; content=&quot;C:\MSOFFICE\WINWORD\MODELES\FAXLYON.DOT&quot;/&amp;gt;
&amp;lt;meta name=&quot;meta:save-date&quot; content=&quot;1997-12-12T12:57:00Z&quot;/&amp;gt;
&amp;lt;meta name=&quot;dc:title&quot; content=&quot;KATALYSE&quot;/&amp;gt;
&amp;lt;meta name=&quot;Application-Name&quot; content=&quot;Microsoft Word 6.0&quot;/&amp;gt;
&amp;lt;meta name=&quot;modified&quot; content=&quot;1997-12-12T12:57:00Z&quot;/&amp;gt;
&amp;lt;meta name=&quot;Edit-Time&quot; content=&quot;8400000000&quot;/&amp;gt;
&amp;lt;meta name=&quot;Content-Length&quot; content=&quot;20480&quot;/&amp;gt;
&amp;lt;meta name=&quot;Content-Type&quot; content=&quot;application/msword&quot;/&amp;gt;
&amp;lt;meta name=&quot;X-Parsed-By&quot; content=&quot;org.apache.tika.parser.DefaultParser&quot;/&amp;gt;
&amp;lt;meta name=&quot;X-Parsed-By&quot; content=&quot;org.apache.tika.parser.microsoft.OfficeParser&quot;/&amp;gt;
&amp;lt;meta name=&quot;creator&quot; content=&quot;Preferred Customer&quot;/&amp;gt;
&amp;lt;meta name=&quot;meta:author&quot; content=&quot;Preferred Customer&quot;/&amp;gt;
&amp;lt;meta name=&quot;extended-properties:Application&quot; content=&quot;Microsoft Word 6.0&quot;/&amp;gt;
&amp;lt;meta name=&quot;meta:creation-date&quot; content=&quot;1997-12-12T11:31:00Z&quot;/&amp;gt;
&amp;lt;meta name=&quot;Last-Printed&quot; content=&quot;1997-12-12T11:31:00Z&quot;/&amp;gt;
&amp;lt;meta name=&quot;meta:last-author&quot; content=&quot;Preferred Customer&quot;/&amp;gt;
&amp;lt;meta name=&quot;Creation-Date&quot; content=&quot;1997-12-12T11:31:00Z&quot;/&amp;gt;
&amp;lt;meta name=&quot;xmpTPg:NPages&quot; content=&quot;1&quot;/&amp;gt;
&amp;lt;meta name=&quot;resourceName&quot; content=&quot;old.DOC&quot;/&amp;gt;
&amp;lt;meta name=&quot;Last-Author&quot; content=&quot;Preferred Customer&quot;/&amp;gt;
&amp;lt;meta name=&quot;Character Count&quot; content=&quot;227&quot;/&amp;gt;
&amp;lt;meta name=&quot;Page-Count&quot; content=&quot;1&quot;/&amp;gt;
&amp;lt;meta name=&quot;Revision-Number&quot; content=&quot;3&quot;/&amp;gt;
&amp;lt;meta name=&quot;extended-properties:Template&quot; content=&quot;C:\MSOFFICE\WINWORD\MODELES\FAXLYON.DOT&quot;/&amp;gt;
&amp;lt;meta name=&quot;Author&quot; content=&quot;Preferred Customer&quot;/&amp;gt;
&amp;lt;meta name=&quot;meta:page-count&quot; content=&quot;1&quot;/&amp;gt;
&amp;lt;title&amp;gt;KATALYSE&amp;lt;/title&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;&amp;lt;p&amp;gt;䅋䅔奌䕓഍഍䅄䕔㨠䐍瑡ݥ䐓呁⁅䁜樠⽪䵍愯ᑡ㈱ㄯ⼲㜹ܕ܇܇⁁❌呁䕔呎佉⁎䕄㨠名⁯牍⁳牯&amp;lt;/p&amp;gt;
&amp;lt;p&amp;gt;㈱㈱&amp;lt;/p&amp;gt;
&amp;lt;p&amp;gt;䴯&amp;lt;/p&amp;gt;
&amp;lt;p&amp;gt;㈱㜹&amp;lt;/p&amp;gt;
&amp;lt;p&amp;gt;愯&amp;lt;/p&amp;gt;
&amp;lt;p&amp;gt;㜹ᨍ&amp;lt;/p&amp;gt;
&amp;lt;p&amp;gt;ܕ܇܇⁁❌呁䕔呎佉⁎䕄㨠名⁯牍⁳牯䴠ݲ⹍䈠剅䡔䑏܍܇܇剏䅇䥎䵓⁅ഺ潃灭湡ݹ潓楣瓩⃩偁䱐䍉䱏剏܍܇܇끎䐠⁅䕔䕌佃䥐啅⁒ഺ慆⁸渠ް㐰㜠‹㌸㈠‰㔴܍܇܇䅐䕇ⱓ夠䌠䵏剐卉䌠䱅䕌䌭⁉ഺ慐敧ⱳ椠据畬楤杮琠楨⁳湯ݥܱ܇܇䕄䰠⁁䅐呒䐠⁅㨠䘍潲ݭ畇⁹䕌佃䕌܇഍഍഍潍獮敩牵ബ഍畓瑩⁥⃠潮牴⁥散瑮挠湯慴瑣琠泩烩潨楮畱ⱥ樠愧⁩敬瀠慬獩物搠⁥潣普物敭⁲潮牴⁥敲摮穥瘭畯⁳畤ㄠ‹散扭敲瀠潲档楡⁮⃠㔱と‰慤獮瘠獯氠捯畡⁸敤嘠杯慬獮മ䨍⁥潶獵瀠敳瑮牥楡氠牯⁳敤挠瑥整爠痩楮湯氠獥挠湯汣獵潩獮搠⁵牰ⷩ楤条潮瑳捩猠牴瑡柩煩敵മ഍慄獮挠瑥整愠瑴湥整‬敪瘠畯⁳牰敩搠⁥牣楯敲‬潍獮敩牵‬⃠❬獡畳慲据⁥敤洠獥猠湥楴敭瑮⁳敬⁳敭汩敬牵⹳഍഍഍഍䜉奕䰠䍅䱏൅഍ㄱ‬畲⁥畇汩潬摵ⴠ㘠〹㌰䰠余⁎‭⹬㨠〠⸴㈷㘮⸸㠰〮‸‭慆⁸›㐰㜮⸲㠶〮⸳㘶匍䄮‮畡挠灡瑩污搠⁥㘵‶〰‰剆⹓删䌮匮‮慐楲⁳䈠㌠㤷㔠㘶㜠ㄷ഍഍�ઙ莤ꔮ䇈誦꜅֊厨꤃֊ƌ贀�㈱㈱㜹ᨍ餀ꐊ⺃좥ꙁ֊誧ꠅ͓誩谅�ƍĀǀȿăǀ☿܁܃耡Ā揠à܁聁缀｣ǰăƀ⤟ἡ䏼~⨃܁Ἥ⇸＇́쀁㼁Ǡ☿ἁ쀁Ａǰ⤏ā܇Ｓೀ˿āȀ̃萀㼡⇰＇༁︃ἢ䏸àćĀⅿ܀老ἡǸćↀ缥ǰ䔃️℀쀃㼁༂�㼡⏰à�༡˼Ӽā⇀️ﰃā쀋̡෿ǰȃćↀＥǀԟāǀȏā⇠︇�쀁̅︁Ēč︉�Ⅻ́耄༤߼⇀️㼂︣Ā⌏þ༁ＦǠ━ﰟ㼂耄ἡ˸˼П⇀︇܅̅老܁考ć老Ą㼊︦Ā܏ʀӸăƀЇƀ⤃ĉＡᎀ⾁︃܃︃�ɽǀȁğ␀༂耄ﰂ༥ϼ⇼ﰏﰃ쀆ἂ�ἤϸϸↀﰏ܂＆̀�̉�&amp;lt;/p&amp;gt;
&amp;lt;p&amp;gt;ᨍ&amp;lt;/p&amp;gt;
&amp;lt;p&amp;gt;഍&amp;lt;/p&amp;gt;
&amp;lt;p&amp;gt;ㄍ&amp;lt;/p&amp;gt;
&amp;lt;p&amp;gt;ㄱ‬畲⁥畇汩潬摵ⴠ㘠〹㌰䰠余⁎‭⹬㨠〠⸴㈷㘮⸸㠰〮‸‭慆⁸›㐰㜮⸲㠶〮⸳㘶匍䄮‮畡挠灡瑩污搠⁥㘵‶〰‰剆⹓删䌮匮‮慐楲⁳䈠㌠㤷㔠㘶㜠ㄷ഍഍�ઙ莤ꔮ䇈誦꜅֊厨꤃֊ƌ贀�㈱㈱㜹ᨍ餀ꐊ⺃좥ꙁ֊誧ꠅ͓誩谅�ƍĀǀȿăǀ☿܁܃耡Ā揠à܁聁缀｣ǰăƀ⤟ἡ䏼~⨃܁Ἥ⇸＇́쀁&amp;lt;/p&amp;gt;
&amp;lt;p&amp;gt;഍&amp;lt;/p&amp;gt;
&amp;lt;p&amp;gt;഍&amp;lt;/p&amp;gt;
&amp;lt;p&amp;gt;഍&amp;lt;/p&amp;gt;
&amp;lt;p&amp;gt;ᨍ&amp;lt;/p&amp;gt;
&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 2 May 2017 13:52:24 +0000" id="592" opendate="Tue, 11 Apr 2017 16:50:52 +0000">
		<buginformation>
			<summary>Video labeling using existing ObjectRecognition</summary>
			<description>&lt;p&gt;Currently TIKA supports ObjectRecognition in Images. I am proposing to extend this to support videos. &lt;/p&gt;

&lt;p&gt;Idea is -&lt;br/&gt;
1. Extract frames from video and run IncV3 to get labels for these frames. &lt;br/&gt;
2. We average confidence scores of same labels for each frame. &lt;br/&gt;
3. Return results in sorted order of confidence score. &lt;/p&gt;

&lt;p&gt;I am writing code for different modes of frame extractions -&lt;br/&gt;
1. Extract center image.&lt;br/&gt;
2. Extract frames after every fixed interval.&lt;br/&gt;
3. Extract N frames equally divided across video.&lt;/p&gt;

&lt;p&gt;We used this approach in &lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;. Code in &lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt; &lt;a href=&quot;https://github.com/USCDataScience/hadoop-pot&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/USCDataScience/hadoop-pot&lt;/a&gt;&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; &lt;a href=&quot;https://github.com/USCDataScience/video-recognition&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/USCDataScience/video-recognition&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/resources/org/apache/tika/parser/recognition/tf/InceptionVideoRestDockerfile</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 20 Apr 2017 01:21:18 +0000" id="593" opendate="Wed, 19 Apr 2017 20:33:51 +0000">
		<buginformation>
			<summary>Prevent preventable OOM in CompressorInputStream</summary>
			<description>&lt;p&gt;On &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1631&quot; title=&quot;OutOfMemoryException in ZipContainerDetector&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1631&quot;&gt;TIKA-1631&lt;/a&gt;, users noted that merely detecting an &lt;tt&gt;x-compress&lt;/tt&gt; file could cause an OOM because we were instantiating the stream as part of detection. &lt;br/&gt;
On &lt;a href=&quot;https://issues.apache.org/jira/browse/COMPRESS-382&quot; title=&quot;OutOfMemoryError from CompressorStreamFactory&quot; class=&quot;issue-link&quot; data-issue-key=&quot;COMPRESS-382&quot;&gt;&lt;del&gt;COMPRESS-382&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lfcnassif&quot; class=&quot;user-hover&quot; rel=&quot;lfcnassif&quot;&gt;Luis Filipe Nassif&lt;/a&gt; noted that something similar happens with LZMA.&lt;/p&gt;

&lt;p&gt;Let's work with the Compress project to:&lt;br/&gt;
1) add a static &lt;tt&gt;detect&lt;/tt&gt; that doesn't instantiate the streams (&lt;a href=&quot;https://issues.apache.org/jira/browse/COMPRESS-385&quot; title=&quot;Add detect() to CompressorStreamFactory&quot; class=&quot;issue-link&quot; data-issue-key=&quot;COMPRESS-385&quot;&gt;&lt;del&gt;COMPRESS-385&lt;/del&gt;&lt;/a&gt;)&lt;br/&gt;
2) allow a parameterizable limit on the amount of allocated space for &lt;tt&gt;x-compress&lt;/tt&gt; (&lt;a href=&quot;https://issues.apache.org/jira/browse/COMPRESS-386&quot; title=&quot;Consider adding sanity check to maxCodeSize in ZCompressorInputStream&quot; class=&quot;issue-link&quot; data-issue-key=&quot;COMPRESS-386&quot;&gt;&lt;del&gt;COMPRESS-386&lt;/del&gt;&lt;/a&gt;) and &lt;tt&gt;LZMA&lt;/tt&gt; (&lt;a href=&quot;https://issues.apache.org/jira/browse/COMPRESS-382&quot; title=&quot;OutOfMemoryError from CompressorStreamFactory&quot; class=&quot;issue-link&quot; data-issue-key=&quot;COMPRESS-382&quot;&gt;&lt;del&gt;COMPRESS-382&lt;/del&gt;&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Until we have a chance to make these changes in the compress project, let's temporarily copy/paste/update from Compress to fix these within Tika.&lt;/p&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/exception/TikaMemoryLimitException.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 20 Apr 2017 01:21:32 +0000" id="594" opendate="Wed, 19 Apr 2017 20:38:39 +0000">
		<buginformation>
			<summary>Upgrade RTFParser to allow configuration of max bytes per embedded object</summary>
			<description>&lt;p&gt;Handling of embedded objects for RTF was added before we had the parameterizable &lt;tt&gt;@Field&lt;/tt&gt; option.  Now that we have that and now that we will soon have a &lt;tt&gt;TikaMemoryLimitException&lt;/tt&gt;, let's update the RTFParser to use both of these.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/rtf/RTFEmbObjHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 20 Apr 2017 01:42:49 +0000" id="595" opendate="Thu, 20 Apr 2017 01:36:46 +0000">
		<buginformation>
			<summary>Upgrade SQLite to 3.16.1</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 8 Mar 2018 12:16:06 +0000" id="596" opendate="Mon, 24 Apr 2017 13:24:50 +0000">
		<buginformation>
			<summary>Change Scope of Jai-ImageIO-Core dependency</summary>
			<description>&lt;p&gt;Looks like jai-imageio-core from github (&lt;a href=&quot;https://github.com/jai-imageio/jai-imageio-core&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/jai-imageio/jai-imageio-core&lt;/a&gt;) which we depend on with test scope is Apache compatible.&lt;/p&gt;

&lt;p&gt;Note that is a fork from the original Jai project which is referenced by PDFBox. The github fork has extracted jpeg2000 and other code with license issues to a diferent project.&lt;/p&gt;

&lt;p&gt;Let's remove test scope from jai-imageio-core dependency, so we will provide support for tiff and other image formats (except jpeg2000) out of the box.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-dl/pom.xml</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 4 May 2017 01:31:35 +0000" id="597" opendate="Thu, 27 Apr 2017 11:18:07 +0000">
		<buginformation>
			<summary>--text-main in tika-server</summary>
			<description>&lt;p&gt;Looking at Tika-Server wiki (&lt;a href=&quot;https://wiki.apache.org/tika/TikaJAXRS&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://wiki.apache.org/tika/TikaJAXRS&lt;/a&gt;), I don't see a --text-main tika-app equivalent.&lt;/p&gt;

&lt;p&gt;Is there such thing?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-server/src/main/java/org/apache/tika/server/resource/TikaResource.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 27 Apr 2017 14:47:04 +0000" id="598" opendate="Thu, 27 Apr 2017 13:56:32 +0000">
		<buginformation>
			<summary>TikaConfigSerializer should expose EncodingDetector details</summary>
			<description>&lt;p&gt;We're now supporting loading of an EncodingDetector from the Tika Config XML. We should therefore also serialise it back out in TikaConfigSerializer&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/config/TikaConfigSerializer.java</file>
			<file>/tika-core/src/test/java/org/apache/tika/config/TikaConfigSerializerTest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 27 Apr 2017 14:47:04 +0000" id="599" opendate="Thu, 27 Apr 2017 15:45:32 +0000">
		<buginformation>
			<summary>Allow Office format parsers to exclude parsing shapes</summary>
			<description>&lt;p&gt;The Office format parsers support including or excluding of deleted text and moved text. It would be good to also support something similar for shape-based text, though probably not for PPT / PPTX as that's almost all shape-based!&lt;/p&gt;

&lt;p&gt;(This has been done hackily in the Alfresco fork of Tika at  &lt;a href=&quot;https://github.com/Alfresco/tika/commit/32aca3fd96816ad49b869a82c9ba0f02265f8744&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/Alfresco/tika/commit/32aca3fd96816ad49b869a82c9ba0f02265f8744&lt;/a&gt; but would be good to do properly)&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/XWPFWordExtractorDecorator.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/AbstractOfficeParser.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/AbstractPOIFSExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 24 Nov 2017 01:12:31 +0000" id="600" opendate="Fri, 28 Apr 2017 15:23:41 +0000">
		<buginformation>
			<summary>Underlined text is not decorated as such when extracting from word documents</summary>
			<description>&lt;p&gt;When extracting from doc and docx bold and italic text decoration is extracted, however underlining is not.  Can be demonstrated in WordParserTest or OOXMLParserTest (change to docx) with the following test case.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-style: solid;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeHeader panelHeader&quot; style=&quot;border-bottom-width: 1px;border-bottom-style: solid;&quot;&gt;&lt;b&gt;WordParserTest.java&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    @Test
    &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void testTextDecoration() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; Exception {
      XMLResult result = getXML(&lt;span class=&quot;code-quote&quot;&gt;&quot;testWORD_various.doc&quot;&lt;/span&gt;);
      &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; xml = result.xml;

      assertTrue(xml.contains(&lt;span class=&quot;code-quote&quot;&gt;&quot;&amp;lt;b&amp;gt;Bold&amp;lt;/b&amp;gt;&quot;&lt;/span&gt;));
      assertTrue(xml.contains(&lt;span class=&quot;code-quote&quot;&gt;&quot;&amp;lt;i&amp;gt;italic&amp;lt;/i&amp;gt;&quot;&lt;/span&gt;));
      assertTrue(xml.contains(&lt;span class=&quot;code-quote&quot;&gt;&quot;&amp;lt;u&amp;gt;underline&amp;lt;/u&amp;gt;&quot;&lt;/span&gt;));

    }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/XWPFWordExtractorDecorator.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 5 May 2017 14:05:50 +0000" id="601" opendate="Fri, 5 May 2017 13:58:03 +0000">
		<buginformation>
			<summary>Temporarily prevent duplication of sheets in some xlsx POI-61034</summary>
			<description>&lt;p&gt;&lt;a href=&quot;https://bz.apache.org/bugzilla/show_bug.cgi?id=61034&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://bz.apache.org/bugzilla/show_bug.cgi?id=61034&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Signals are clear in tika-eval reports...should have caught this...argh...&lt;/p&gt;

&lt;p&gt;We can easily add a temporary workaround at the Tika level until POI 3.17-beta1 is out.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/XSSFExcelExtractorDecorator.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 8 May 2017 15:20:19 +0000" id="602" opendate="Mon, 8 May 2017 12:54:31 +0000">
		<buginformation>
			<summary>Allow Tesseract PSM up to 13</summary>
			<description>&lt;p&gt;From &lt;a href=&quot;https://github.com/apache/tika/pull/177&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/tika/pull/177&lt;/a&gt; by Rafael Ferreira  &lt;/p&gt;

&lt;p&gt;Extend support for increased PSM options up to 13 for modern versions of Tesseract.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;$ tesseract --version
tesseract 3.05.00
 leptonica-1.74.1
  libjpeg 8d : libpng 1.6.29 : libtiff 4.0.7 : zlib 1.2.8

$ tesseract --help-psm
Page segmentation modes:
  0    Orientation and script detection (OSD) only.
  1    Automatic page segmentation with OSD.
  2    Automatic page segmentation, but no OSD, or OCR.
  3    Fully automatic page segmentation, but no OSD. (Default)
  4    Assume a single column of text of variable sizes.
  5    Assume a single uniform block of vertically aligned text.
  6    Assume a single uniform block of text.
  7    Treat the image as a single text line.
  8    Treat the image as a single word.
  9    Treat the image as a single word in a circle.
 10    Treat the image as a single character.
 11    Sparse text. Find as much text as possible in no particular order.
 12    Sparse text with OSD.
 13    Raw line. Treat the image as a single text line, bypassing hacks that are Tesseract-specific.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 10 May 2017 00:14:13 +0000" id="603" opendate="Tue, 9 May 2017 19:44:37 +0000">
		<buginformation>
			<summary>Avoid bundling dl4j with tika-app and tika-server</summary>
			<description>&lt;p&gt;If we include dl4j in tika-app, the jar size goes from 56MB to 270MB, and we'd be including all sorts of native libs.  Let's require users to add the tika-dl module to their dependencies.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 10 May 2017 00:14:13 +0000" id="604" opendate="Thu, 11 May 2017 13:49:17 +0000">
		<buginformation>
			<summary>Extreme slow parsing on the attachment attached</summary>
			<description>&lt;p&gt;i have 93s for parsing this document using 1.14 in server or in cli mode.&lt;/p&gt;

&lt;p&gt;Java:&lt;br/&gt;
java version &quot;1.8.0_121&quot;&lt;br/&gt;
Java(TM) SE Runtime Environment (build 1.8.0_121-b13)&lt;br/&gt;
Java HotSpot(TM) 64-Bit Server VM (build 25.121-b13, mixed mode)&lt;/p&gt;

&lt;p&gt;debian-jessie, 8GB ram in a docker container, current xeon 3GHz, so decent (2 cores limited)&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/ocr/TesseractOCRParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 18 May 2017 10:45:37 +0000" id="605" opendate="Mon, 15 May 2017 14:42:40 +0000">
		<buginformation>
			<summary>Handle SentimentParser resource failure more robustly</summary>
			<description>&lt;p&gt;The SentimentParser tests currently require a network call to github.  For those working behind a proxy or would prefer Tika not to make unexpected network calls, can we please turn this off by default?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/sentiment/analysis/SentimentParserTest.java</file>
			<file>/tika-parsers/src/main/resources/META-INF/services/org.apache.tika.parser.Parser</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 16 May 2017 10:08:34 +0000" id="606" opendate="Tue, 16 May 2017 02:37:33 +0000">
		<buginformation>
			<summary>Upgrade to PDFBox 2.0.6</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 16 May 2017 10:08:34 +0000" id="607" opendate="Tue, 16 May 2017 10:44:27 +0000">
		<buginformation>
			<summary>Skipping Header and Footer data from documents</summary>
			<description>&lt;p&gt;Is there any method to skip header and footer data of documents(pdf,docx,doc,odt)?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 16 May 2017 10:08:34 +0000" id="608" opendate="Tue, 16 May 2017 12:13:28 +0000">
		<buginformation>
			<summary>Skip image recognition test if network call fails</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/tika-dl/src/test/java/org/apache/tika/dl/imagerec/DL4JInceptionV3NetTest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 16 May 2017 18:57:14 +0000" id="609" opendate="Tue, 16 May 2017 12:22:14 +0000">
		<buginformation>
			<summary>Clean up printstacktrace</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/tika-eval/src/main/java/org/apache/tika/eval/AbstractProfiler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 16 May 2017 19:01:17 +0000" id="610" opendate="Tue, 16 May 2017 18:55:23 +0000">
		<buginformation>
			<summary>Avoid npe in wmf</summary>
			<description>&lt;p&gt;Found in regression runs.&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Caused by: java.lang.NullPointerException
	at org.apache.tika.parser.microsoft.WMFParser.parse(WMFParser.java:74)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/WMFParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 16 May 2017 19:01:17 +0000" id="611" opendate="Wed, 17 May 2017 12:15:28 +0000">
		<buginformation>
			<summary>Clean up SentimentParser dependencies</summary>
			<description>&lt;p&gt;Is there any way to avoid reliance on edu.usc.ir's sentiment-analysis-parser?  I ask because:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;[WARNING] sentiment-analysis-parser-0.1.jar, tika-parsers-1.15-SNAPSHOT.jar define 1 overlapping classes: 
[WARNING]   - org.apache.tika.parser.sentiment.analysis.SentimentParser
[WARNING] tika-core-1.15-SNAPSHOT.jar, tika-translate-1.15-SNAPSHOT.jar define 4 overlapping classes: 
[WARNING]   - org.apache.tika.language.translate.DefaultTranslator$1
[WARNING]   - org.apache.tika.language.translate.EmptyTranslator
[WARNING]   - org.apache.tika.language.translate.DefaultTranslator
[WARNING]   - org.apache.tika.language.translate.Translator
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We should be ok keeping things as they are and excluding SentimentParser and tika-translate, but can we easily move the code that's still in edu.usc.ir's package into Tika?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/sentiment/SentimentAnalysisParser.java</file>
			<file>/tika-parsers/pom.xml</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 18 May 2017 10:27:19 +0000" id="612" opendate="Thu, 18 May 2017 10:16:02 +0000">
		<buginformation>
			<summary>Close Font in TrueTypeParser</summary>
			<description>&lt;p&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=icirellik&quot; class=&quot;user-hover&quot; rel=&quot;icirellik&quot;&gt;Cameron Rollheiser&lt;/a&gt; opened &lt;a href=&quot;https://github.com/apache/tika/pull/181&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/tika/pull/181&lt;/a&gt; to point out that we're not closing TrueTypeFont correctly, and thus potentially leaving resources open.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 18 May 2017 10:27:19 +0000" id="613" opendate="Thu, 18 May 2017 19:28:58 +0000">
		<buginformation>
			<summary>OSX DMG support</summary>
			<description>&lt;p&gt;As noted at ApacheCon today, the Flex project are interested in extracting data from Apple OSX DMG files. &lt;/p&gt;

&lt;p&gt;We may have issues with finding a suitably licensed library for this, but it'd be good to add support (similar to the other archive formats) for DMG&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/resources/test-documents/test-documents.dmg</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 22 May 2017 17:33:35 +0000" id="614" opendate="Mon, 22 May 2017 14:34:32 +0000">
		<buginformation>
			<summary>Fix licenses via rat before 1.15 release</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 22 May 2017 17:33:35 +0000" id="615" opendate="Mon, 22 May 2017 16:52:50 +0000">
		<buginformation>
			<summary>Tika App -z should extract PDF inline images by default</summary>
			<description>&lt;p&gt;As discussed on dev@ - If you use the Tika App with the default config and the &lt;tt&gt;-z&lt;/tt&gt; extract option, it will extract embedded resources, except PDF inline images. This is unexpected for new users, who won't know that they'd need to pass in a custom config with the &lt;tt&gt;extractInlineImages&lt;/tt&gt; PDF parser option set&lt;/p&gt;

&lt;p&gt;If the user passes in an explicit config to the app, we should respect that. However, if they don't pass one in and take the default, the -z option should (but only that one) enable whatever options are needed to make extraction work properly + fully (currently just &lt;tt&gt;extractInlineImages&lt;/tt&gt;)&lt;/p&gt;

&lt;p&gt;If possible/easy, the -z option should print out some info to let affected users know that the default config was tweaked to give extra embedded resources&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 5 Jul 2017 11:56:29 +0000" id="616" opendate="Wed, 5 Jul 2017 10:29:50 +0000">
		<buginformation>
			<summary>English ASCII text classified as video/quicktime</summary>
			<description>&lt;p&gt;The attached file was created by the following steps:&lt;/p&gt;

&lt;p&gt;Download &lt;a href=&quot;http://www.gutenberg.org/files/6688/6688-0.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.gutenberg.org/files/6688/6688-0.txt&lt;/a&gt; from Project Gutenberg&lt;/p&gt;

&lt;p&gt;Discard the first 151 lines: tail -n +151 6688-0.txt &amp;gt; 6688-1.txt&lt;/p&gt;

&lt;p&gt;Running org.apache.tika.detect.DefaultDetector on this file returns “video/quicktime.” I am unsure as to why.&lt;/p&gt;

&lt;p&gt;  f = org.apache.tika.io.TikaInputStream.get(java.io.FileInputStream(fullname));&lt;br/&gt;
  detector = org.apache.tika.detect.DefaultDetector;&lt;br/&gt;
  md = org.apache.tika.metadata.Metadata;&lt;br/&gt;
  fulltype = string(detector.detect(f,md));&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 3 Jul 2017 11:30:30 +0000" id="617" opendate="Mon, 30 Nov 2015 10:41:59 +0000">
		<buginformation>
			<summary>Tika use no free json.org</summary>
			<description>&lt;p&gt;Hi&lt;br/&gt;
Your project is licensed under Apache License Version 2,&lt;br/&gt;
but your code pulls in code from json.org under Douglas Crockford’s bad licence &lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; , and is non-free &lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt;.&lt;br/&gt;
Such usage restriction makes the license incompatible with The Open Source Definition and&lt;br/&gt;
The Free Software Definition. Because Tika binary distribution includes this software,&lt;br/&gt;
it effectively becomes proprietary software itself.&lt;br/&gt;
You may also comment that the json.org license is valid for You but for many Linux distributions it is not acceptable.&lt;/p&gt;

&lt;p&gt;I hope to continue to maintain Tika for Fedora, without having to run into these problems.&lt;/p&gt;

&lt;p&gt;Please try to replace it with one of the many free alternatives.&lt;/p&gt;

&lt;p&gt;Regards&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;&lt;br/&gt;
./tika-1.11/tika-parsers/src/main/java/org/apache/tika/parser/journal/GrobidRESTParser.java&lt;br/&gt;
./tika-1.11/tika-parsers/src/main/java/org/apache/tika/parser/journal/JournalParser.java&lt;br/&gt;
./tika-1.11/tika-parsers/src/main/java/org/apache/tika/parser/journal/TEIParser.java&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt;&lt;br/&gt;
&lt;a href=&quot;https://wiki.debian.org/qa.debian.org/jsonevil&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://wiki.debian.org/qa.debian.org/jsonevil&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;http://www.sonatype.com/people/2012/03/use-json-well-youd-better-not-be-evil/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.sonatype.com/people/2012/03/use-json-well-youd-better-not-be-evil/&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;http://tanguy.ortolo.eu/blog/article46/json-license&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://tanguy.ortolo.eu/blog/article46/json-license&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/journal/TEITest.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 3 Jul 2017 17:37:03 +0000" id="618" opendate="Thu, 22 Sep 2016 01:18:06 +0000">
		<buginformation>
			<summary>Macros not extracted from ppt files</summary>
			<description>&lt;p&gt;On &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-2069&quot; title=&quot;Extract Macro text from Microsoft Office documents&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-2069&quot;&gt;&lt;del&gt;TIKA-2069&lt;/del&gt;&lt;/a&gt;, I found that macros weren't extracted from the one ppt test file that I generated.  There's a chance something is wrong with the test file.  If not, let's use this to track poi 60162.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/HSLFExtractor.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 23 Mar 2017 23:41:21 +0000" id="619" opendate="Thu, 19 Jan 2017 11:29:36 +0000">
		<buginformation>
			<summary>Standardise logging</summary>
			<description>&lt;p&gt;Tika parsers sometimes use Log4j's Logger, sometimes the JUL (java.util.logging) Logger and sometimes SLF4j.&lt;/p&gt;

&lt;p&gt;It would be better to standardise on a single facade, for the sake of not having to configure multiple loggers.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/pom.xml</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 3 Jul 2017 16:47:47 +0000" id="620" opendate="Fri, 21 Apr 2017 12:17:06 +0000">
		<buginformation>
			<summary>Extract path info from Excel 2013 .xlsx and .xlsb</summary>
			<description>&lt;p&gt;We can do this in Tika for xlsx, but not xlsb.  We should consider updating POI for both.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 3 Jul 2017 16:48:02 +0000" id="621" opendate="Fri, 21 Apr 2017 13:03:48 +0000">
		<buginformation>
			<summary>Upgrade to POI 3.17-beta1 when available</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 1 Jun 2017 19:19:01 +0000" id="622" opendate="Wed, 26 Apr 2017 15:17:00 +0000">
		<buginformation>
			<summary>Upgrade to commons compress 1.14 when available</summary>
			<description>&lt;p&gt;For Tika 1.15, we added temporary copy/pastes of updated CompressorInputStreamFactory and ArchiveInputStreamFactory. &lt;/p&gt;

&lt;p&gt;There's now a static &lt;tt&gt;detect()&lt;/tt&gt; method which should prevent OOMs while detecting some corrupt files.&lt;/p&gt;

&lt;p&gt;There's also now a &lt;tt&gt;memoryLimitInKb&lt;/tt&gt; parameter that limits the amount of memory that can be allocated.  This is can be used to protect against OOM for corrupt files that might appear to allege enormous block sizes.&lt;/p&gt;

&lt;p&gt;Many thanks to &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bodewig&quot; class=&quot;user-hover&quot; rel=&quot;bodewig&quot;&gt;Stefan Bodewig&lt;/a&gt; and the commons-compress community for their quick responses to PRs!&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/LICENSE.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 1 Jun 2017 04:38:41 +0000" id="623" opendate="Wed, 31 May 2017 07:58:28 +0000">
		<buginformation>
			<summary>tika-bundle 1.15 has wrong import of org.sfl4j.event package which does not exists</summary>
			<description>&lt;p&gt;The new release 1.15 now fails in Apache Camel when we run our OSGi tests&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;test(org.apache.camel.itest.karaf.CamelTikaTest)  Time elapsed: 7.212 sec  &amp;lt;&amp;lt;&amp;lt; ERROR!
org.ops4j.pax.exam.WrappedTestContainerException: [test(org.apache.camel.itest.karaf.CamelTikaTest): Unable to resolve root: missing requirement [root] osgi.identity; osgi.identity=camel-tika; type=karaf.feature; version=&lt;span class=&quot;code-quote&quot;&gt;&quot;[2.20.0.SNAPSHOT,2.20.0.SNAPSHOT]&quot;&lt;/span&gt;; filter:=&lt;span class=&quot;code-quote&quot;&gt;&quot;(&amp;amp;(osgi.identity=camel-tika)(type=karaf.feature)(version&amp;gt;=2.20.0.SNAPSHOT)(version&amp;lt;=2.20.0.SNAPSHOT))&quot;&lt;/span&gt; [caused by: Unable to resolve camel-tika/2.20.0.SNAPSHOT: missing requirement [camel-tika/2.20.0.SNAPSHOT] osgi.identity; osgi.identity=org.apache.camel.camel-tika; type=osgi.bundle; version=&lt;span class=&quot;code-quote&quot;&gt;&quot;[2.20.0.SNAPSHOT,2.20.0.SNAPSHOT]&quot;&lt;/span&gt;; resolution:=mandatory [caused by: Unable to resolve org.apache.camel.camel-tika/2.20.0.SNAPSHOT: missing requirement [org.apache.camel.camel-tika/2.20.0.SNAPSHOT] osgi.wiring.&lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt;; filter:=&lt;span class=&quot;code-quote&quot;&gt;&quot;(osgi.wiring.&lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt;=org.apache.tika.parser.html)&quot;&lt;/span&gt; [caused by: Unable to resolve org.apache.tika.bundle/1.15.0: missing requirement [org.apache.tika.bundle/1.15.0] osgi.wiring.&lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt;; filter:=&lt;span class=&quot;code-quote&quot;&gt;&quot;(&amp;amp;(osgi.wiring.&lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt;=org.slf4j.event)(version&amp;gt;=1.7.0)(!(version&amp;gt;=2.0.0)))&quot;&lt;/span&gt;]]]]
	at org.apache.felix.resolver.ResolutionError.toException(ResolutionError.java:42)
	at org.apache.felix.resolver.ResolverImpl.doResolve(ResolverImpl.java:389)
	at org.apache.felix.resolver.ResolverImpl.resolve(ResolverImpl.java:375)
	at org.apache.felix.resolver.ResolverImpl.resolve(ResolverImpl.java:347)
	at org.apache.karaf.features.internal.region.SubsystemResolver.resolve(SubsystemResolver.java:218)
	at org.apache.karaf.features.internal.service.Deployer.deploy(Deployer.java:285)
	at org.apache.karaf.features.internal.service.FeaturesServiceImpl.doProvision(FeaturesServiceImpl.java:1170)
	at org.apache.karaf.features.internal.service.FeaturesServiceImpl.lambda$doProvisionInThread$0(FeaturesServiceImpl.java:1069)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:748)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The problem is that tika-bundle has an import on package&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;org.slf4j.event;version=&lt;span class=&quot;code-quote&quot;&gt;&quot;[1.7,2)&quot;&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And that package does not exists in 1.7.x. It looks like its a new thing that comes in slf4j 1.8 onwards but they have only alpha releases&lt;br/&gt;
&lt;a href=&quot;http://search.maven.org/#search%7Cga%7C1%7Cfc%3A%22org.slf4j.event%22&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://search.maven.org/#search%7Cga%7C1%7Cfc%3A%22org.slf4j.event%22&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It would be good to get this fixed. I wonder if that event package is really needed? And if not then please remove that import in the OSGi manifest file.&lt;/p&gt;

&lt;p&gt;Otherwise you would need to depend on slf4j-api version 1.8 which is still not released in GA and not widely in use. I would suggest to be compatible with slfj4 1.7 so the Tika upgrade from eg 1.14 to 1.15 is a smooth upgrade for end users.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-bundle/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 3 Jul 2017 11:24:01 +0000" id="624" opendate="Wed, 31 May 2017 14:13:20 +0000">
		<buginformation>
			<summary>Upgrade to Jackcess 2.1.8 when available</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 31 May 2017 20:45:26 +0000" id="625" opendate="Wed, 31 May 2017 20:15:11 +0000">
		<buginformation>
			<summary>Include tika-eval artifact in release</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 5 Jun 2017 18:28:30 +0000" id="626" opendate="Fri, 2 Jun 2017 13:16:47 +0000">
		<buginformation>
			<summary>Double close of InputStream in accept text/plain in tika-server</summary>
			<description>&lt;p&gt;As reported by Haris Osmanagic on the user list, TikaResource closes the InputStream twice on requests for text/plain.  &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-server/src/main/java/org/apache/tika/server/resource/TikaResource.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/parser/DigestingParser.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 5 Jun 2017 18:28:30 +0000" id="627" opendate="Fri, 2 Jun 2017 18:58:43 +0000">
		<buginformation>
			<summary>Tesseract OCR rotation.py not run</summary>
			<description>&lt;p&gt;It appears that even if Python is installed, the rotation.py that calculates rotation angle of the image does not run because of indentation/spacing errors in the Python script.&lt;/p&gt;

&lt;p&gt;Also recommend making this a configurable parameter since it does add time and can produce unexpected results if the supplied image contains more than just plain text.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/ocr/TesseractOCRParser.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 5 Jun 2017 18:27:57 +0000" id="628" opendate="Mon, 5 Jun 2017 17:18:10 +0000">
		<buginformation>
			<summary>Improve digest options</summary>
			<description>&lt;p&gt;CommonCrawl uses base 32 encoding of SHA1.  It would be handy to allow users to configure this, with say: &quot;md5,sha1:32&quot;&lt;/p&gt;

&lt;p&gt;Also, it would be helpful to open up the MessageDigest options available in BouncyCastle...e.g. SHA3.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 5 Jun 2017 18:27:57 +0000" id="629" opendate="Wed, 7 Jun 2017 19:15:34 +0000">
		<buginformation>
			<summary>Add parameter for image scale in rendering of pdf image</summary>
			<description>&lt;p&gt;It would be useful to allow users to modify the scale that is used to render images for OCR of PDFs.  We'll leave the default as 2.0.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/AbstractPDF2XHTML.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 3 Nov 2017 20:59:28 +0000" id="630" opendate="Fri, 9 Jun 2017 08:29:26 +0000">
		<buginformation>
			<summary>Warn log level is pretty strong for missing JBIG2ImageReader</summary>
			<description>&lt;p&gt;Given the license of jbig2-imageio many projects (Apache or LGPL projects for example) won't include it and will always end up with a warning because of it while they probably don't really care that much about this image format.&lt;/p&gt;

&lt;p&gt;Ideally ImageParser should probably be made more extensible and jbig2 part moved in an optional module but in the meantime is this warning that necessary ?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/config/Initializable.java</file>
			<file>/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 16 Jun 2017 18:16:02 +0000" id="631" opendate="Fri, 9 Jun 2017 18:45:54 +0000">
		<buginformation>
			<summary>Extract &lt;script&gt; elements in html as &quot;attachment&quot; type MACRO like we do in the PDFParser</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 7 Feb 2018 14:51:50 +0000" id="632" opendate="Fri, 16 Jun 2017 14:40:54 +0000">
		<buginformation>
			<summary>Wrong version of tika-langdetect as transitive dependency of tika-parsers</summary>
			<description>&lt;p&gt;Because of sentiment-analysis-parser tika-parser end up with a tika-langdetect 1.13 dependency instead of 1.15.&lt;/p&gt;

&lt;p&gt;To avoid this issue tika-parser should setup a &amp;lt;dependencyManagement&amp;gt; for tika-langdetect so that it overwrite the version declared in sentiment-analysis-parser.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 7 Jul 2017 15:39:24 +0000" id="633" opendate="Mon, 19 Jun 2017 17:51:38 +0000">
		<buginformation>
			<summary>Version conflict with non-ASL jai-imageio-jpeg2000 and edu.ucar jj2000</summary>
			<description>&lt;p&gt;For users who want to extract jp2000 from PDFs for inline-image OCR, they have to add non- ASL 2.0 compatible:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;&amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;com.github.jai-imageio&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;jai-imageio-jpeg2000&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;1.3.0&amp;lt;/version&amp;gt;
  &amp;lt;/dependency&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;However, this creates a conflict with GRIB's jj2000:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;   &amp;lt;dependency&amp;gt;
      &amp;lt;groupId&amp;gt;edu.ucar&amp;lt;/groupId&amp;gt;
      &amp;lt;artifactId&amp;gt;jj2000&amp;lt;/artifactId&amp;gt;
      &amp;lt;version&amp;gt;5.2&amp;lt;/version&amp;gt;
    &amp;lt;/dependency&amp;gt; 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mcaruanagalizia&quot; class=&quot;user-hover&quot; rel=&quot;mcaruanagalizia&quot;&gt;Matthew Caruana Galizia&lt;/a&gt; (I'm guessing?) identified this conflict &lt;a href=&quot;https://github.com/ICIJ/extract/blob/master/pom.xml&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt; and fixes it by upgrading jj2000 to 5.3.  However, that doesn't exist in maven central, but only in &lt;a href=&quot;http://example.com&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;Boundless&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;What do we do?&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;We could exclude the jj2000 dependency from GRIB, and that functionality won't work for GRIB folks&lt;/li&gt;
	&lt;li&gt;We could add a warning if we see &lt;tt&gt;jai-imageio-jpeg2000&lt;/tt&gt; is on the classpath to instruct users to exclude jj2000.&lt;/li&gt;
	&lt;li&gt;Other options?&lt;/li&gt;
&lt;/ol&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 3 Jul 2017 16:09:54 +0000" id="634" opendate="Mon, 3 Jul 2017 02:14:24 +0000">
		<buginformation>
			<summary>RTF parser is tagging non-bold text as bold</summary>
			<description>&lt;p&gt;While parsing some RTF files I'm finding that the RTF parser tags many text spans as bold even if they are not. I am attaching a sample RTF file that exhibits this behavior. When parsing the file the first line is correctly tagged as bold. However the second line (the phone number) which is not supposed to be bold is tagged as bold.&lt;/p&gt;

&lt;p&gt;The following code demonstrates the problem.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;InputStream inputStream = &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.currentThread().getContextClassLoader()
                .getResourceAsStream(&lt;span class=&quot;code-quote&quot;&gt;&quot;sample-rtf.rtf&quot;&lt;/span&gt;);

Parser parser = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; RTFParser();
ContentHandler contentHandler = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ToXMLContentHandler();
Metadata metadata = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Metadata();
ParseContext context = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ParseContext();

parser.parse(inputStream, contentHandler, metadata, context);
&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; xml = contentHandler.toString();
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/rtf/TextExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 3 Jul 2017 17:57:36 +0000" id="635" opendate="Mon, 3 Jul 2017 13:01:07 +0000">
		<buginformation>
			<summary>Clean up tika-bundle</summary>
			<description>&lt;p&gt;We're getting:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;[WARNING] Embed-Dependency: clause &quot;opennlp-maxent&quot; did not match any dependencies
[WARNING] Embed-Dependency: clause &quot;jwnl&quot; did not match any dependencies
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;when building tika-bundle.  Should we delete these or do we need to fix them?&lt;/p&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-bundle/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 5 Jul 2017 11:56:29 +0000" id="636" opendate="Wed, 5 Jul 2017 13:57:30 +0000">
		<buginformation>
			<summary>Try HTML mime magic on broken XML files</summary>
			<description>&lt;p&gt;As noticed from the latest common crawl work, some url-hosted HTML files are being detected as text/plain then specialised out to their programming language url extension&lt;/p&gt;

&lt;p&gt;This is caused broken XML in the HTML, and by us having dropped the magic priority of HTML to 40 (below that of XML), to avoid it matching for HTML-containing other types like emails. Because these files have broken XML (eg an empty encoding on the xml tag), the XML root extractor doesn't run, and they get downmixed to text plain then specialised by filename&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/mime/MimeTypes.java</file>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 6 Jul 2017 12:55:38 +0000" id="637" opendate="Thu, 6 Jul 2017 09:56:32 +0000">
		<buginformation>
			<summary>Improve detection of Graphviz *.dot format</summary>
			<description>&lt;p&gt;Detection of Graphviz document formats could be improved by adding&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;either *.dot as glob pattern (conflicts with the more frequent MSWord templates)&lt;/li&gt;
	&lt;li&gt;a magic pattern which catches the &lt;a href=&quot;http://www.graphviz.org/content/dot-language&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;.dot language&lt;/a&gt; grammar, eg. &lt;tt&gt;^\s*(?:strict\s+)?(?:di)?graph\b&lt;/tt&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Seen with Common Crawl data (see also discussions on &lt;a href=&quot;https://lists.apache.org/thread.html/1e4f4b6c249618a446f2e92f56ef90e6bfa0dfe51ce10197461df3d9@%3Cuser.tika.apache.org%3E&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;user@tika&lt;/a&gt; and &lt;a href=&quot;https://lists.apache.org/thread.html/7e0c25a389a03011eabce81e933f17a6093390138f4890fa77c36a59@%3Cdev.poi.apache.org%3E&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;dev@poi&lt;/a&gt;): web server sends &quot;text/vnd.graphviz&quot; (often wrong) and Tika detects &quot;application/msword&quot; (sometimes wrong), see &lt;a href=&quot;https://commoncrawl-dev.s3.amazonaws.com/tika-content-type-detection/test/tika_dot_graphviz_msword.warc.gz&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;WARC file&lt;/a&gt;). &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 10 Jul 2017 17:43:51 +0000" id="638" opendate="Mon, 10 Jul 2017 14:25:45 +0000">
		<buginformation>
			<summary>Fix locale-dependent test in xlsb unit test</summary>
			<description>&lt;p&gt;As &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lfcnassif&quot; class=&quot;user-hover&quot; rel=&quot;lfcnassif&quot;&gt;Luis Filipe Nassif&lt;/a&gt; pointed out on the user list, one xlsb test fails because it relies on US.locale/number formatting. &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/microsoft/ooxml/OOXMLParserTest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 18 Sep 2017 13:03:07 +0000" id="639" opendate="Fri, 28 Jul 2017 16:39:20 +0000">
		<buginformation>
			<summary>Support for GZIP-compressed EMF files</summary>
			<description>&lt;p&gt;Tika is currently detecting EMZ (compressed EMF) files as simple gzip files. These files should instead be detected as EMF files and the EMFParser should perform decompression transparently.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 28 Nov 2017 14:23:10 +0000" id="640" opendate="Mon, 27 Nov 2017 16:41:50 +0000">
		<buginformation>
			<summary>Embedded MP3 file in PPTX document no longer identified</summary>
			<description>&lt;p&gt;I'm attaching a sample PPTX file with an embedded MP3 file along with JSON files produced by Tika App (versions 1.14 and 1.15).&lt;br/&gt;
Notice that the 1.14 output identifies the embedded MP3 file while the 1.15 version does not.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 31 Jul 2018 10:41:51 +0000" id="641" opendate="Wed, 1 Aug 2018 09:55:58 +0000">
		<buginformation>
			<summary>Text is not extracted properly from WMF files</summary>
			<description>&lt;p&gt;Text is always extracted assuming it is in cp-1252 encoding. The attached thumbnail_1.wmf has text in Shift JIS and is extracted incorrectly. Should be 普林斯.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/microsoft/WMFParserTest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 27 Oct 2015 12:47:41 +0000" id="642" opendate="Mon, 26 Oct 2015 14:45:27 +0000">
		<buginformation>
			<summary>XHTMLContentHandler doesn't pass attributes of html element</summary>
			<description>&lt;p&gt;XHTMLContentHandler.startElement() uses lazyHead() for the html element because it's defined in the AUTO Set. As a consequence, attributes of the html element are not passed to downstream content handlers.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 4 Nov 2015 16:00:49 +0000" id="643" opendate="Wed, 4 Nov 2015 15:33:38 +0000">
		<buginformation>
			<summary>Downgrade logging severity in FileResourceConsumer and fix handling of illegal xml characters</summary>
			<description>&lt;p&gt;FileResourceConsumer logs an xmlified snippet to record problems encountered during parsing.  If a parser includes illegal xml characters in the ParseException, this exception is caught by the xmlification code and then logged as an error.&lt;/p&gt;

&lt;p&gt;The xmlification code should be robust against illegal characters and we should downgrade logging severity from error to warnings when there wasn't an actual error thrown by a parser.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-batch/src/main/java/org/apache/tika/batch/FileResourceConsumer.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 15 Nov 2015 20:04:58 +0000" id="644" opendate="Tue, 10 Nov 2015 00:32:32 +0000">
		<buginformation>
			<summary>URI is not hierarchical exception when location model resource is inside a jar in classpath</summary>
			<description>&lt;div class=&quot;code panel&quot; style=&quot;border-style: solid;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeHeader panelHeader&quot; style=&quot;border-bottom-width: 1px;border-bottom-style: solid;&quot;&gt;&lt;b&gt;Stacktrace&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;The following error happens when location NER model resource is packaged inside a jar and GeoTopicParser is enabled.

Caused by: java.lang.IllegalArgumentException: URI is not hierarchical
	at java.io.File.&amp;lt;init&amp;gt;(File.java:418)
	at org.apache.tika.parser.geo.topic.GeoParserConfig.&amp;lt;init&amp;gt;(GeoParserConfig.java:33)
	at org.apache.tika.parser.geo.topic.GeoParser.&amp;lt;init&amp;gt;(GeoParser.java:54)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;.newInstance(&lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;.java:442)
	at org.apache.tika.config.TikaConfig$XmlLoader.loadOne(TikaConfig.java:559)
	at org.apache.tika.config.TikaConfig$XmlLoader.loadOverall(TikaConfig.java:492)
	at org.apache.tika.config.TikaConfig.&amp;lt;init&amp;gt;(TikaConfig.java:166)
	at org.apache.tika.config.TikaConfig.&amp;lt;init&amp;gt;(TikaConfig.java:149)
	at org.apache.tika.config.TikaConfig.&amp;lt;init&amp;gt;(TikaConfig.java:142)
	at org.apache.tika.config.TikaConfig.&amp;lt;init&amp;gt;(TikaConfig.java:138)
	at edu.usc.cs.ir.cwork.tika.Parser.&amp;lt;init&amp;gt;(Parser.java:45)

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Refernces :&lt;br/&gt;
&lt;a href=&quot;http://stackoverflow.com/questions/18055189/why-my-uri-is-not-hierarchical&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://stackoverflow.com/questions/18055189/why-my-uri-is-not-hierarchical&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/geo/topic/GeoParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 10 Nov 2015 16:19:08 +0000" id="645" opendate="Tue, 10 Nov 2015 14:50:26 +0000">
		<buginformation>
			<summary>Add ASiC-E and ASiC-S mime types</summary>
			<description>&lt;p&gt;These are the references:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;&lt;a href=&quot;http://www.iana.org/assignments/media-types/application/vnd.etsi.asic-e+zip&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.iana.org/assignments/media-types/application/vnd.etsi.asic-e+zip&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;http://www.iana.org/assignments/media-types/application/vnd.etsi.asic-s+zip&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.iana.org/assignments/media-types/application/vnd.etsi.asic-s+zip&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;My &lt;tt&gt;custom-mimetypes.xml&lt;/tt&gt; is:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-xml&quot;&gt;&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;?xml version=&lt;span class=&quot;code-quote&quot;&gt;&quot;1.0&quot;&lt;/span&gt; encoding=&lt;span class=&quot;code-quote&quot;&gt;&quot;UTF-8&quot;&lt;/span&gt;?&amp;gt;&lt;/span&gt;
&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;mime-info&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;mime-type type=&lt;span class=&quot;code-quote&quot;&gt;&quot;application/vnd.etsi.asic-e+zip&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;acronym&amp;gt;&lt;/span&gt;ASiC-E&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/acronym&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;_comment&amp;gt;&lt;/span&gt;Extended Associated Signature Container&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/_comment&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;magic priority=&lt;span class=&quot;code-quote&quot;&gt;&quot;50&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;match value=&lt;span class=&quot;code-quote&quot;&gt;&quot;PK\003\004&quot;&lt;/span&gt; type=&lt;span class=&quot;code-quote&quot;&gt;&quot;string&quot;&lt;/span&gt; offset=&lt;span class=&quot;code-quote&quot;&gt;&quot;0&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;match value=&lt;span class=&quot;code-quote&quot;&gt;&quot;mimetypeapplication/vnd.etsi.asic-e+zip&quot;&lt;/span&gt; type=&lt;span class=&quot;code-quote&quot;&gt;&quot;string&quot;&lt;/span&gt; offset=&lt;span class=&quot;code-quote&quot;&gt;&quot;30&quot;&lt;/span&gt; /&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/match&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/magic&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;glob pattern=&lt;span class=&quot;code-quote&quot;&gt;&quot;*.asice&quot;&lt;/span&gt; /&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/mime-type&amp;gt;&lt;/span&gt;

  &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;mime-type type=&lt;span class=&quot;code-quote&quot;&gt;&quot;application/vnd.etsi.asic-s+zip&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;acronym&amp;gt;&lt;/span&gt;ASiC-S&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/acronym&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;_comment&amp;gt;&lt;/span&gt;Simple Associated Signature Container&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/_comment&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;magic priority=&lt;span class=&quot;code-quote&quot;&gt;&quot;50&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;match value=&lt;span class=&quot;code-quote&quot;&gt;&quot;PK\003\004&quot;&lt;/span&gt; type=&lt;span class=&quot;code-quote&quot;&gt;&quot;string&quot;&lt;/span&gt; offset=&lt;span class=&quot;code-quote&quot;&gt;&quot;0&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;match value=&lt;span class=&quot;code-quote&quot;&gt;&quot;mimetypeapplication/vnd.etsi.asic-s+zip&quot;&lt;/span&gt; type=&lt;span class=&quot;code-quote&quot;&gt;&quot;string&quot;&lt;/span&gt; offset=&lt;span class=&quot;code-quote&quot;&gt;&quot;30&quot;&lt;/span&gt; /&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/match&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/magic&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;glob pattern=&lt;span class=&quot;code-quote&quot;&gt;&quot;*.asics&quot;&lt;/span&gt; /&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/mime-type&amp;gt;&lt;/span&gt;
&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/mime-info&amp;gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 14 Nov 2015 20:26:31 +0000" id="646" opendate="Sat, 14 Nov 2015 14:31:19 +0000">
		<buginformation>
			<summary>Email file (.eml extension - &quot;message/rfc822&quot;) detected as text/html</summary>
			<description>&lt;p&gt;My email (.eml) file that was exported from Thunderbird is not being recognized by Tika's as message/rfc822.&lt;/p&gt;

&lt;p&gt;Email file is attached to this issue.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 16 Nov 2015 16:22:26 +0000" id="647" opendate="Mon, 16 Nov 2015 16:20:43 +0000">
		<buginformation>
			<summary>RTFParser can double Metadata.CONTENT_TYPE entry in Metadata</summary>
			<description>&lt;p&gt;RTFParser should &lt;tt&gt;set&lt;/tt&gt; not &lt;tt&gt;add&lt;/tt&gt; mime type.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/rtf/RTFParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 2 Feb 2016 20:21:01 +0000" id="648" opendate="Mon, 23 Nov 2015 14:25:13 +0000">
		<buginformation>
			<summary>Upgrade to POI 3.14-Beta1 when available</summary>
			<description>&lt;p&gt;Should be out in the next week or two.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/XSLFPowerPointExtractorDecorator.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 2 Mar 2016 15:33:29 +0000" id="649" opendate="Tue, 22 Dec 2015 00:11:01 +0000">
		<buginformation>
			<summary>Extracts entire file content for ASCII DXF files</summary>
			<description>&lt;p&gt;By definition, ASCII DXF files are encoded in plain text.  However. the vast majority of their content is not intended to be human readable (see &lt;a href=&quot;https://en.wikipedia.org/wiki/AutoCAD_DXF&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://en.wikipedia.org/wiki/AutoCAD_DXF&lt;/a&gt;).  Unfortunately for these files, Tika simply &quot;extracts&quot; the entire content of the file instead of the human-readable portions (i.e. comments etc.) that a CAD tool would render.  This results in massive amounts of rubbish data being returned with dire consequences for applications that rely on this.&lt;/p&gt;

&lt;p&gt;It would be nice if only the human-readable text fields were extracted.  Failing this, it would still be nice if no text was extracted from these files at all.  &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/LICENSE.txt</file>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 4 Jan 2016 17:57:51 +0000" id="650" opendate="Tue, 29 Dec 2015 22:34:00 +0000">
		<buginformation>
			<summary>Upgrade rome to 1.5.1</summary>
			<description>&lt;p&gt;It would appear that the Rome project has had somewhat of an overhaul and a new breath of life which is great.&lt;br/&gt;
They have apparently fixed &lt;a href=&quot;https://github.com/rometools/rome/issues/130#issuecomment-166118607&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;an issue&lt;/a&gt; where Rome was failing to load properties from the rome.properties file.&lt;br/&gt;
We can upgrade to the most recent 1.5.1 version and fix any deprecation we encounter. &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 21 Jan 2016 19:35:27 +0000" id="651" opendate="Tue, 19 Jan 2016 12:03:59 +0000">
		<buginformation>
			<summary>LinkContentHandler skips iframe and rel tags</summary>
			<description>&lt;p&gt;As simple as it gets, link and iframe tags were never implemented in LinkContentHandler. &lt;a href=&quot;https://issues.apache.org/jira/browse/NUTCH-1233&quot; title=&quot;Rely on Tika for outlink extraction&quot; class=&quot;issue-link&quot; data-issue-key=&quot;NUTCH-1233&quot;&gt;&lt;del&gt;NUTCH-1233&lt;/del&gt;&lt;/a&gt; kind of requires it.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/sax/Link.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 26 May 2016 14:50:57 +0000" id="652" opendate="Wed, 20 Jan 2016 06:49:31 +0000">
		<buginformation>
			<summary>HtmlEncodingDetector wrongly detects charset from commented meta</summary>
			<description>&lt;p&gt;The org.apache.tika.parser.html.HtmlEncodingDetector class will grab the first meta tag that has a charset in it matching the pattern defined in HTTP_META_PATTERN. The problem encountered is when there are multiple such meta tags but the first ones are commented.  &lt;/p&gt;

&lt;p&gt;In my mind the detector should not consider commented code for this detection. &lt;/p&gt;

&lt;p&gt;Real example encountered in an HTML page:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-xml&quot;&gt;   &lt;span class=&quot;code-tag&quot;&gt;&lt;span class=&quot;code-comment&quot;&gt;&amp;lt;!--&amp;lt;meta http-equiv=&lt;span class=&quot;code-quote&quot;&gt;&quot;Content-Type&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;text/html; charset=ISO-8859-1&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt; --&amp;gt;&lt;/span&gt;
   &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;meta http-equiv=&lt;span class=&quot;code-quote&quot;&gt;&quot;Content-Type&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;text/html; charset=utf-8&quot;&lt;/span&gt; /&amp;gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The detector currently detects &lt;tt&gt;ISO-8859-1&lt;/tt&gt; while it should detect &lt;tt&gt;utf-8&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Fix:&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;As opposed to modify the meta-detection regex, I recommend to first strip comments, taking into consideration the substring from the input stream may not hold the closing characters &lt;tt&gt;--&amp;gt;&lt;/tt&gt;.  This has been tested to work:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-style: solid;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeHeader panelHeader&quot; style=&quot;border-bottom-width: 1px;border-bottom-style: solid;&quot;&gt;&lt;b&gt;HtmlEncodingDetector.java, line 104+&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;        &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; head = ASCII.decode(ByteBuffer.wrap(buffer, 0, n)).toString();

        &lt;span class=&quot;code-comment&quot;&gt;// START FIX:
&lt;/span&gt;        head = head.replaceAll(&lt;span class=&quot;code-quote&quot;&gt;&quot;&amp;lt;!--.*?(--&amp;gt;|$)&quot;&lt;/span&gt;, &quot;&quot;);
        &lt;span class=&quot;code-comment&quot;&gt;// END FIX
&lt;/span&gt;
        Matcher equiv = HTTP_META_PATTERN.matcher(head);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/html/HtmlEncodingDetector.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 3 Feb 2016 17:00:00 +0000" id="653" opendate="Mon, 1 Feb 2016 14:33:12 +0000">
		<buginformation>
			<summary>Unable to extract content from certain RTFs using tika-server versions since 1.5 </summary>
			<description>&lt;p&gt;I have some patient letters that are RTF documents.  When I extract the text from these documents using tika-server-1.5.jar, it works fine.&lt;/p&gt;

&lt;p&gt;However, in tika-server-1.6.jar and later versions (I've tried 1.6, 1.9 and 1.11), it fails with the stack trace and error shown below.&lt;/p&gt;

&lt;p&gt;I can provide a sample RTF that is failing. &lt;/p&gt;

&lt;p&gt;I wondered whether the error might be related to the following change that was introduced in 1.6?:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Made RTFParser's list handling slightly more robust against corrupt&lt;br/&gt;
    list metadata (&lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1305&quot; title=&quot;New list processing changes appear to be causing RTFParser exception&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1305&quot;&gt;&lt;del&gt;TIKA-1305&lt;/del&gt;&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;It's possible that there is some issue with the RTF documents, but they are real patient letters and they open in Microsoft Word without any problems.&lt;/p&gt;

&lt;p&gt;Many thanks&lt;br/&gt;
Ian&lt;/p&gt;


&lt;p&gt;Steps to reproduce issue&lt;br/&gt; ====================&lt;/p&gt;

&lt;p&gt;1. HTTP PUT to Tika server using curl:&lt;/p&gt;

&lt;p&gt;C:\Downloads\Apache Tika&amp;gt;curl -X PUT --data-binary @test-anonymised-letter.rtf &lt;a href=&quot;http://localhost:9998/tika&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://localhost:9998/tika&lt;/a&gt; --header &quot;Content-Type: application/rtf&quot; --header &quot;Accept: text/plain&quot;&lt;/p&gt;

&lt;p&gt;--&amp;gt; this works fine when running tika-server-1.5.jar, but fails with tika-server-1.6.jar&lt;/p&gt;


&lt;p&gt;2. Screen capture from the server:&lt;br/&gt;
INFO: Starting Apache Tika 1.9 server&lt;br/&gt;
Feb 01, 2016 2:26:10 PM org.apache.cxf.endpoint.ServerImpl initDestination&lt;br/&gt;
INFO: Setting the server's publish address to be &lt;a href=&quot;http://localhost:9998/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://localhost:9998/&lt;/a&gt;&lt;br/&gt;
Feb 01, 2016 2:26:10 PM org.slf4j.impl.JCLLoggerAdapter info&lt;br/&gt;
INFO: jetty-8.y.z-SNAPSHOT&lt;br/&gt;
Feb 01, 2016 2:26:10 PM org.slf4j.impl.JCLLoggerAdapter info&lt;br/&gt;
INFO: Started SelectChannelConnector@localhost:9998&lt;br/&gt;
Feb 01, 2016 2:26:10 PM org.apache.tika.server.TikaServerCli main&lt;br/&gt;
INFO: Started&lt;br/&gt;
Feb 01, 2016 2:26:24 PM org.apache.tika.server.resource.TikaResource logRequest&lt;br/&gt;
INFO: tika (application/rtf)&lt;br/&gt;
Feb 01, 2016 2:26:25 PM org.apache.tika.server.resource.TikaResource parse&lt;br/&gt;
WARNING: tika: Text extraction failed&lt;br/&gt;
org.apache.tika.exception.TikaException: Unexpected RuntimeException from org.apache.tika.parser.rtf.RTFParser@32a6dc&lt;br/&gt;
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:283)&lt;br/&gt;
        at org.apache.tika.parser.ParserDecorator.parse(ParserDecorator.java:163)&lt;br/&gt;
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:281)&lt;br/&gt;
        at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)&lt;br/&gt;
        at org.apache.tika.server.resource.TikaResource.parse(TikaResource.java:244)&lt;br/&gt;
        at org.apache.tika.server.resource.TikaResource$4.write(TikaResource.java:321)&lt;br/&gt;
        at org.apache.cxf.jaxrs.provider.BinaryDataProvider.writeTo(BinaryDataProvider.java:164)&lt;br/&gt;
        at org.apache.cxf.jaxrs.utils.JAXRSUtils.writeMessageBody(JAXRSUtils.java:1363)&lt;br/&gt;
        at org.apache.cxf.jaxrs.interceptor.JAXRSOutInterceptor.serializeMessage(JAXRSOutInterceptor.java:244)&lt;br/&gt;
        at org.apache.cxf.jaxrs.interceptor.JAXRSOutInterceptor.processResponse(JAXRSOutInterceptor.java:117)&lt;br/&gt;
        at org.apache.cxf.jaxrs.interceptor.JAXRSOutInterceptor.handleMessage(JAXRSOutInterceptor.java:80)&lt;br/&gt;
        at org.apache.cxf.phase.PhaseInterceptorChain.doIntercept(PhaseInterceptorChain.java:307)&lt;br/&gt;
        at org.apache.cxf.interceptor.OutgoingChainInterceptor.handleMessage(OutgoingChainInterceptor.java:83)&lt;br/&gt;
        at org.apache.cxf.phase.PhaseInterceptorChain.doIntercept(PhaseInterceptorChain.java:307)&lt;br/&gt;
        at org.apache.cxf.transport.ChainInitiationObserver.onMessage(ChainInitiationObserver.java:121)&lt;br/&gt;
        at org.apache.cxf.transport.http.AbstractHTTPDestination.invoke(AbstractHTTPDestination.java:251)&lt;br/&gt;
        at org.apache.cxf.transport.http_jetty.JettyHTTPDestination.doService(JettyHTTPDestination.java:261)&lt;br/&gt;
        at org.apache.cxf.transport.http_jetty.JettyHTTPHandler.handle(JettyHTTPHandler.java:70)&lt;br/&gt;
        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1088)&lt;br/&gt;
        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1024)&lt;br/&gt;
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)&lt;br/&gt;
        at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:255)&lt;br/&gt;
        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)&lt;br/&gt;
        at org.eclipse.jetty.server.Server.handle(Server.java:370)&lt;br/&gt;
        at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)&lt;br/&gt;
        at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)&lt;br/&gt;
        at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)&lt;br/&gt;
        at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:651)&lt;br/&gt;
        at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)&lt;br/&gt;
        at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)&lt;br/&gt;
        at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:696)&lt;br/&gt;
        at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:53)&lt;br/&gt;
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)&lt;br/&gt;
        at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)&lt;br/&gt;
        at java.lang.Thread.run(Unknown Source)&lt;br/&gt;
Caused by: java.lang.NullPointerException&lt;br/&gt;
        at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:113)&lt;br/&gt;
        at org.apache.tika.parser.DelegatingParser.parse(DelegatingParser.java:72)&lt;br/&gt;
        at org.apache.tika.extractor.ParsingEmbeddedDocumentExtractor.parseEmbedded(ParsingEmbeddedDocumentExtractor.java:103)&lt;br/&gt;
        at org.apache.tika.parser.rtf.RTFEmbObjHandler.extractObj(RTFEmbObjHandler.java:230)&lt;br/&gt;
        at org.apache.tika.parser.rtf.RTFEmbObjHandler.handleCompletedObject(RTFEmbObjHandler.java:198)&lt;br/&gt;
        at org.apache.tika.parser.rtf.TextExtractor.processGroupEnd(TextExtractor.java:1357)&lt;br/&gt;
        at org.apache.tika.parser.rtf.TextExtractor.extract(TextExtractor.java:456)&lt;br/&gt;
        at org.apache.tika.parser.rtf.TextExtractor.extract(TextExtractor.java:439)&lt;br/&gt;
        at org.apache.tika.parser.rtf.RTFParser.parse(RTFParser.java:86)&lt;br/&gt;
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:281)&lt;br/&gt;
        ... 34 more&lt;/p&gt;

&lt;p&gt;Feb 01, 2016 2:26:25 PM org.apache.cxf.jaxrs.utils.JAXRSUtils logMessageHandlerProblem&lt;br/&gt;
SEVERE: Problem with writing the data, class org.apache.tika.server.resource.TikaResource$4, ContentType: text/plain&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/rtf/RTFEmbObjHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 1 Mar 2017 20:39:17 +0000" id="654" opendate="Mon, 22 Feb 2016 20:49:17 +0000">
		<buginformation>
			<summary>Save sender email address in Outlook MSG metadata</summary>
			<description>&lt;p&gt;Sender email address is lost when extracting metadata from Outlook msg files. Currently only sender name is extracted. That is an important information to be extracted for search engines.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/mail/MailContentHandler.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/metadata/Message.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/mail/MailUtil.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 8 May 2016 21:17:44 +0000" id="655" opendate="Thu, 3 Mar 2016 01:30:22 +0000">
		<buginformation>
			<summary>Tika MIME updates for *.cdf and *.xar and custom zero length file detector based on TREC-DD-Polar</summary>
			<description>&lt;p&gt;Updated tika-mimetypes.xml and detector to identify new file types in TREC DD Polar dataset.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/detect/ZeroSizeFileDetector.java</file>
			<file>/tika-core/src/test/java/org/apache/tika/detect/ZeroSizeFileDetectorTest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 19 Apr 2016 12:41:19 +0000" id="656" opendate="Tue, 19 Apr 2016 06:48:31 +0000">
		<buginformation>
			<summary>NPE in WordParser when trying to getPicOffset</summary>
			<description>&lt;p&gt;Tika-server gives 422 error:&lt;br/&gt;
/rmeta throws 422 error, &lt;br/&gt;
/tika gives text but its partial,&lt;br/&gt;
The text is parsed till beginning of an image.&lt;br/&gt;
This is the last text which is parsed. &lt;br/&gt;
&amp;lt;h4&amp;gt;17.5.7.1&lt;br/&gt;
BM-SC Initiated Multicast Service Deactivation&lt;br/&gt;
&amp;lt;/h4&amp;gt;&lt;br/&gt;
&amp;lt;p&amp;gt;&lt;/p&gt;

			</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 26 Apr 2017 14:37:39 +0000" id="657" opendate="Wed, 28 Sep 2016 05:31:38 +0000">
		<buginformation>
			<summary>Tar files without magic bytes are sporadically detected as text</summary>
			<description>&lt;p&gt;When a tar is created with 7 Zip 9.20 the magic bytes &quot;ustar&quot; are not added. Everything seems to work file if the tar contains Microsoft Office files. But when only text files are contained Tika sporadically recognices it as text/plain. It also seems to depend on the size of the first file in the tar. This has to be several KB big.&lt;br/&gt;
The problem was found in version 1.11 and also exists in the latest 1.14-SNAPSHOT.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pkg/PackageParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 8 Sep 2017 16:48:35 +0000" id="658" opendate="Thu, 7 Sep 2017 07:16:51 +0000">
		<buginformation>
			<summary>Possibility to add custom-mimetypes.xml (and/or also other configuration files) from location outside classpath</summary>
			<description>&lt;p&gt;I would like to be able to pass to tika the custom-mimetypes.xml from outside classpath, because it is more flexible.&lt;/p&gt;

&lt;p&gt;Our application is based on eclipse/osgi and it's composed of multiple plugins/bundles.&lt;br/&gt;
One of these plugins contains also the tika library (almost all settings are default)&lt;br/&gt;
We usually provide some configuration, from outside to these plugins.&lt;br/&gt;
And we would like to do the same with tika, because we recently encountered: &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-2443&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/TIKA-2443&lt;/a&gt; and had to provide a custom type to workaround a mismatched detection. &lt;br/&gt;
There might be other potential mismatches and it would be good to give this possibility to pass configuration to tika from outside our application. Only that for the osgi setup, on classpath means inside the folder plugin. &lt;br/&gt;
and from our point of view, that is not a good place, because these plugins get replaced at every release, so this patching would have to be maintained all the time. &lt;br/&gt;
This is the reason  why it would be good, if tika itself had this possibility. Thank you.  &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 14 Apr 2010 09:40:55 +0000" id="659" opendate="Mon, 12 Nov 2007 01:40:33 +0000">
		<buginformation>
			<summary>Image metadata extraction</summary>
			<description>&lt;p&gt;The Sanselan library (&lt;a href=&quot;http://www.fightingquaker.com/sanselan/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.fightingquaker.com/sanselan/&lt;/a&gt;, currently incubating as &lt;a href=&quot;http://incubator.apache.org/sanselan/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://incubator.apache.org/sanselan/&lt;/a&gt;) provides a nice way to extract metadata from a variety of image formats.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/src/main/java/org/apache/tika/parser/image/ImageParser.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/image/ImageParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 1 Nov 2010 22:50:25 +0000" id="660" opendate="Tue, 26 Jan 2010 16:36:43 +0000">
		<buginformation>
			<summary>Upgrade to POI 3.7</summary>
			<description>&lt;p&gt;The next POI release will have a few fixes that at least the patches in &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-361&quot; title=&quot;Update OutlookExtractor to match new POI API&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-361&quot;&gt;&lt;del&gt;TIKA-361&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-371&quot; title=&quot;Excel formatting depends on the default locale&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-371&quot;&gt;&lt;del&gt;TIKA-371&lt;/del&gt;&lt;/a&gt; depend on. I'm filing this issue already now so we can properly track the dependency to the next POI release.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 12 Apr 2010 15:59:44 +0000" id="661" opendate="Mon, 5 Apr 2010 20:12:55 +0000">
		<buginformation>
			<summary>netCDF Tika Parser</summary>
			<description>&lt;p&gt;Along with &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-399&quot; title=&quot;HDF4/5 Tika Parser&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-399&quot;&gt;&lt;del&gt;TIKA-399&lt;/del&gt;&lt;/a&gt;, netCDF is also a widely used scientific data format. I'm going to throw up a Tika parser that can deal with netCDF.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 7 Jul 2010 07:10:31 +0000" id="662" opendate="Sat, 10 Apr 2010 17:15:57 +0000">
		<buginformation>
			<summary>Support for iWork documents</summary>
			<description>&lt;p&gt;It would be nice to have support for documents created by Apple's Keynote and Pages applications. Both file formats are described in &lt;a href=&quot;http://developer.apple.com/mac/library/documentation/AppleApplications/Conceptual/iWork2-0_XML/Chapter01/Introduction.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://developer.apple.com/mac/library/documentation/AppleApplications/Conceptual/iWork2-0_XML/Chapter01/Introduction.html&lt;/a&gt;. I'm not sure if there already are open source parser libraries for these formats or if we'd need to directly process the XML content.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/iwork/IWorkParserTest.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/sax/ContentHandlerDecorator.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/iwork/IWorkParser.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/parser/ParseContext.java</file>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/iwork/KeynoteContentHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 19 Apr 2010 16:35:26 +0000" id="663" opendate="Mon, 19 Apr 2010 16:33:14 +0000">
		<buginformation>
			<summary>Exclude the xml-apis dependency</summary>
			<description>&lt;p&gt;POI depends on dom4j that in turn depends on the xml-apis jar for some XML-related interfaces that are nowadays a part of the JRE. Normally having such an extra jar around doesn't harm anything as normal class loaders will always use the classes provided by the JRE. However some application servers like JBoss allow applications to override javax.* interfaces, which causes all sorts of trouble. Thus it's better if we exclude the xml-apis dependency from Tika.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 26 May 2010 12:14:03 +0000" id="664" opendate="Tue, 20 Apr 2010 15:34:58 +0000">
		<buginformation>
			<summary>DWG Parser</summary>
			<description>&lt;p&gt;Attached is a parser for DWG files, along with a unit test and 3 sample files&lt;/p&gt;

&lt;p&gt;No integration with the mime/detection framework has been done yet. Someone would still need to do that, then uncomment the content type line in the tests&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/dwg/DWGParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 18 Jan 2011 15:34:41 +0000" id="665" opendate="Fri, 30 Apr 2010 14:38:05 +0000">
		<buginformation>
			<summary>Out-of-process text extraction</summary>
			<description>&lt;p&gt;There's currently no easy way to guard against JVM crashes or excessive memory or CPU use caused by parsing very large, broken or intentionally malicious input documents. To better protect against such cases and to generally improve the manageability of resource consumption by Tika it would be great if we had a way to run Tika parsers in separate JVM processes. This could be handled either as a separate &quot;Tika parser daemon&quot; or as an explicitly managed pool of forked JVMs.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/parser/EmptyParser.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/fork/ForkClient.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/fork/ForkServer.java</file>
			<file>/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java</file>
			<file>/tika-core/src/test/java/org/apache/tika/fork/ForkParserTest.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/fork/ClassLoaderResource.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/detect/Detector.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/fork/ClassLoaderProxy.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/fork/MemoryURLStreamHandler.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/fork/ForkParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 4 May 2010 16:07:47 +0000" id="666" opendate="Tue, 4 May 2010 15:55:39 +0000">
		<buginformation>
			<summary>Allow parser lookup from a custom class loader</summary>
			<description>&lt;p&gt;Since &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-317&quot; title=&quot;Service provider -based Tika configuration&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-317&quot;&gt;&lt;del&gt;TIKA-317&lt;/del&gt;&lt;/a&gt; we've used the javax.imageio.spi.ServiceRegistry.lookupProviders(Class&amp;lt;?&amp;gt;) method to look up all the currently available Parser implementations. This method uses the context class loader of the current thread for looking up Parser classes.&lt;/p&gt;

&lt;p&gt;As seen in &lt;a href=&quot;https://issues.apache.org/jira/browse/NUTCH-810&quot; title=&quot;Upgrade to Tika 0.7&quot; class=&quot;issue-link&quot; data-issue-key=&quot;NUTCH-810&quot;&gt;&lt;del&gt;NUTCH-810&lt;/del&gt;&lt;/a&gt; and discussed on tika-users@, this is troublesome for applications with more complex class loading mechanisms. To solve the issue, it would be good to allow the client to optionally specify an explicit class loader instance from which parsers are to be looked up.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/config/TikaConfig.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 12 Jul 2010 17:33:02 +0000" id="667" opendate="Fri, 7 May 2010 20:14:35 +0000">
		<buginformation>
			<summary>[PATCH] Integration of boilerpipe: Boilerplate Removal and Fulltext Extraction from HTML pages</summary>
			<description>&lt;p&gt;Hi all,&lt;/p&gt;

&lt;p&gt;while Tika already provides a parser for HTML that extracts the plain text from it, the output generally contains all text portions, including the surplus &quot;clutter&quot; such as navigation menus, links to related pages etc. around the actual main content. This &quot;boilerplate text&quot; typically is not related to the main content and may deteriorate search precision.&lt;/p&gt;

&lt;p&gt;I think Tika should be able to automatically detect and remove the boilerplate text. I propose to use &quot;boilerpipe&quot; for this purpose, an Apache 2.0 licensed Java library written by me. Boilerpipe provides both generic and specific strategies for common tasks (for example: news article extraction) and may also be easily extended for individual problem settings.&lt;/p&gt;

&lt;p&gt;Extracting content is very fast (milliseconds), just needs the input document (no global or site-level information required) and is usually quite accurate. In fact, it outperformed the state-of-the-art approaches for several test collections.&lt;/p&gt;

&lt;p&gt;The algorithms used by the library are based on (and extending) some concepts of my paper &quot;Boilerplate Detection using Shallow Text Features&quot;, presented at WSDM 2010 &amp;#8211; The Third ACM International Conference on Web Search and Data Mining New York City, NY USA. (online at &lt;a href=&quot;http://www.l3s.de/~kohlschuetter/boilerplate/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.l3s.de/~kohlschuetter/boilerplate/&lt;/a&gt; )&lt;/p&gt;

&lt;p&gt;To use boilerpipe with Tika, I have developed a custom ContentHandler (BoilerpipeContentHandler; provided as a patch to tika-parsers) that can simply be passed to HtmlParser#parse. The BoilerpipeContentHandler can be configured in several ways, particularly which extraction strategy should be used and where the extracted content should go &amp;#8211; into Metadata or to a Writer).&lt;/p&gt;

&lt;p&gt;I also provide a patch to TikaCLI, such that you can use boilerpipe via Tika from the command line (use a capital &quot;-T&quot; flag instead of &quot;-t&quot; to extract the main content only).&lt;/p&gt;

&lt;p&gt;I must note that boilerplate removal is considered a research problem:&lt;/p&gt;

&lt;p&gt;While one can always find clever rules to extract the main content from particular web pages with 100% accuracy, applying it to random, previously unseen pages on the web is non-trivial.&lt;/p&gt;

&lt;p&gt;In my paper, I have shown that on the Web (i.e. independent of a particular site owner, page layout etc.), textual content can apparently be grouped into two classes, long text (i.e., a lot of subsequent words without markup &amp;#8211; most likely the actual content) and short text (i.e., a few words between two HTML tags, most likely navigational boilerplate text) respectively. Removing the words from the short text class alone already is a good strategy for cleaning boilerplate and using a combination of multiple shallow text features achieves an almost perfect accuracy. To a large extent the detection of boilerplate text does not require any inter-document knowledge (frequency of text blocks, common page layout etc.) nor any training at token level. The costs for detecting boilerplates are negligible, as it comes down simply to counting words.&lt;/p&gt;

&lt;p&gt;The algorithms provided in my paper seem to generally work well and especially for news article-like pages (for a Zipf-representative collection of English news pages crawled via Google News: 90-95% F1 on average, 95-98% F1 median), well ahead of the competition (78-89% avg. F1, 87-95% median F1).&lt;/p&gt;

&lt;p&gt;Patches are attached, questions welcome.&lt;/p&gt;

&lt;p&gt;Best,&lt;br/&gt;
Christian&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 16 Jun 2010 19:09:31 +0000" id="668" opendate="Tue, 15 Jun 2010 18:53:44 +0000">
		<buginformation>
			<summary>Sometimes, tika not working (crashed) because of null classloader</summary>
			<description>&lt;p&gt;I used Tika inside application, that should run as MS Windows service via Apache Commons Daemon - procrun (&lt;a href=&quot;http://commons.apache.org/daemon/procrun.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://commons.apache.org/daemon/procrun.html&lt;/a&gt;). When it was running under procrun, then Thread.currentThread().getContextClassLoader() returned null instead ClassLoader's instance. I made a workaround in my app by explicitly setting classloader for current thread.  I'll attach fix, that should use system class loader if getContextClassLoader will return null&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/config/TikaConfig.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 31 Oct 2010 23:10:29 +0000" id="669" opendate="Mon, 28 Jun 2010 17:26:24 +0000">
		<buginformation>
			<summary>Upgrade to PDFBox 1.3.1</summary>
			<description>&lt;p&gt;Apache PDFBox 1.2.0 is just about to be released. We should upgrade when it becomes available on Maven Central.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/pom.xml</file>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 13 Aug 2010 17:11:28 +0000" id="670" opendate="Wed, 7 Jul 2010 15:28:01 +0000">
		<buginformation>
			<summary>HTMLParser gets an early &lt;/body&gt; event</summary>
			<description>&lt;p&gt;I am using the IdentityMapper in the HTMLparser with this simple document:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&amp;lt;html&amp;gt;&amp;lt;head&amp;gt;&amp;lt;title&amp;gt; my title &amp;lt;/title&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
&amp;lt;frameset rows=\&lt;span class=&quot;code-quote&quot;&gt;&quot;20,*\&quot;&lt;/span&gt;&amp;gt; 
&amp;lt;frame src=\&lt;span class=&quot;code-quote&quot;&gt;&quot;top.html\&quot;&lt;/span&gt;&amp;gt;
&amp;lt;/frame&amp;gt;
&amp;lt;frameset cols=\&lt;span class=&quot;code-quote&quot;&gt;&quot;20,*\&quot;&lt;/span&gt;&amp;gt;
&amp;lt;frame src=\&lt;span class=&quot;code-quote&quot;&gt;&quot;left.html\&quot;&lt;/span&gt;&amp;gt;
&amp;lt;/frame&amp;gt;
&amp;lt;frame src=\&lt;span class=&quot;code-quote&quot;&gt;&quot;invalid.html\&quot;&lt;/span&gt;/&amp;gt;
&amp;lt;/frame&amp;gt;
&amp;lt;frame src=\&lt;span class=&quot;code-quote&quot;&gt;&quot;right.html\&quot;&lt;/span&gt;&amp;gt;
&amp;lt;/frame&amp;gt;
&amp;lt;/frameset&amp;gt;
&amp;lt;/frameset&amp;gt;
&amp;lt;/body&amp;gt;&amp;lt;/html&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Strangely the HTMLHandler is getting a call to endElement on the body &lt;b&gt;BEFORE&lt;/b&gt;  we reach frameset. As a result the variable bodylevel is decremented back to 0 and the remaining entities are ignored due to the logic implemented in HTMLHandler.&lt;/p&gt;

&lt;p&gt;Any idea?&lt;/p&gt;

			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/sax/XHTMLContentHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 5 Nov 2010 13:53:38 +0000" id="671" opendate="Fri, 9 Jul 2010 21:39:31 +0000">
		<buginformation>
			<summary>Add Boilerpipe 1.0.4 to Maven central and remove java.net repository from parser pom</summary>
			<description>&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-420&quot; title=&quot;[PATCH] Integration of boilerpipe: Boilerplate Removal and Fulltext Extraction from HTML pages&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-420&quot;&gt;&lt;del&gt;TIKA-420&lt;/del&gt;&lt;/a&gt; adds the BoilerpipeContentHandler to Tika. This requires the boilerpipe 1.0.4 jar, which in turn means the tika-parsers pom has to reference the java.net Maven repo, which is where this jar is currently located.&lt;/p&gt;

&lt;p&gt;But this means Tika can no longer be pushed to the Maven central repo, as no external dependencies are allowed.&lt;/p&gt;

&lt;p&gt;So prior to the 0.8 release, we should do a one-shot upload of boilerpipe 1.0.4 to Sonatype, which in turn will sync it to Maven central.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 17 Aug 2010 15:06:15 +0000" id="672" opendate="Mon, 12 Jul 2010 20:15:44 +0000">
		<buginformation>
			<summary>HtmlParser doesn't extract links from img, map, object, frame, iframe, area, link</summary>
			<description>&lt;p&gt;All of the listed HTML elements can have URLs as attributes, and thus we'd want to extract those links, if possible.&lt;/p&gt;

&lt;p&gt;For elements that aren't valid as XHTML 1.0, there might be some challenges in the right way to handle this.&lt;/p&gt;

&lt;p&gt;But if XHTML 1.0 means the union of &quot;transitional and frameset&quot; variants, then all of the above are valid, and thus should be emitted by the parser,&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/html/DefaultHtmlMapper.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/sax/XHTMLContentHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 16 Jul 2010 17:53:05 +0000" id="673" opendate="Fri, 16 Jul 2010 11:20:17 +0000">
		<buginformation>
			<summary>Feed Parser</summary>
			<description>&lt;p&gt;We currently have no parsers for feeds in Tika and since we are progressively getting rid of our legacy parsers in Nutch I thought it could make sense to have one.&lt;/p&gt;

&lt;p&gt;The patch attached is based on the ROME feed parser (&lt;a href=&quot;https://rome.dev.java.net/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://rome.dev.java.net/&lt;/a&gt;) which is under Apache License. Rome provides a unified API for different feed formats and seems well maintained.&lt;/p&gt;

&lt;p&gt;The implementation of the FeedParser is by no means complete but should serve as a basis for further improvements. It currently stores the title and description from the feed and stores them in the metadata and uses the following XHTML representation for the entries : &lt;/p&gt;

&lt;p&gt;&amp;lt;A href=&quot;ENTRY_URL&quot;&amp;gt;ENTRY_TITLE&amp;lt;/A&amp;gt;&lt;br/&gt;
&amp;lt;P&amp;gt;&lt;br/&gt;
ENTRY_DESCRIPTION&lt;br/&gt;
&amp;lt;/P&amp;gt; &lt;/p&gt;

&lt;p&gt;This is pretty basic but should at least allow us to retrieve the outlinks in Nutch as well as some text. &lt;/p&gt;

&lt;p&gt;J. &lt;/p&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/feed/FeedParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 26 Jul 2010 12:19:53 +0000" id="674" opendate="Thu, 22 Jul 2010 16:29:26 +0000">
		<buginformation>
			<summary>Tika App command line option to list the registered parsers and their supported mime types</summary>
			<description>&lt;p&gt;I was demoing the tika app to some people today, and one of them asked which parsers it supported, and how they can easily check that into the future following upgrades&lt;/p&gt;

&lt;p&gt;With this in mind, I'm planning to add two new options to the command line interface of tika app. The simpler one will be to list the parsers that will be used in processing your documents. The longer one will be to list the parsers, and for each parser which mime types it supports&lt;/p&gt;

&lt;p&gt;I'm thinking of options of:&lt;br/&gt;
  --list-parsers = list the parsers&lt;br/&gt;
  --list-parser-details = list the parsers and their mime types&lt;/p&gt;

&lt;p&gt;If anyone has any suggestions on option names, or anything similar that should be added at the same time, do please comment &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 9 Aug 2010 18:05:51 +0000" id="675" opendate="Mon, 9 Aug 2010 16:50:05 +0000">
		<buginformation>
			<summary>Add page count to metadata </summary>
			<description>&lt;p&gt;Very small patch, which adds page count to PDF's metadata. &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/SummaryExtractor.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/metadata/PagedText.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 25 Aug 2010 09:39:07 +0000" id="676" opendate="Tue, 24 Aug 2010 00:42:09 +0000">
		<buginformation>
			<summary>Metadata constructor is slow</summary>
			<description>&lt;p&gt;Metadata constructor initialises a bunch of SimpleTimeFormat objects. This is slow - they need to parse their configuration strings, system must be queried for timezone, and so on. In my simple test in which I create new Metadata object for each new document this accounts for approximately 25-30% of all time spent in Tika.&lt;/p&gt;

&lt;p&gt;As pointed out by Nick Burch, SimpleTimeFormat is not threadsafe, so they can't be static.&lt;/p&gt;

&lt;p&gt;Possible solutions include using org.apache.commons.lang.time.FastDateFormat instead (if its features are sufficient), or using thread local objects.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/metadata/Metadata.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 1 Nov 2010 06:14:54 +0000" id="677" opendate="Thu, 2 Sep 2010 10:55:50 +0000">
		<buginformation>
			<summary>Add a ContentHandler for collecting links from parser output</summary>
			<description>&lt;p&gt;It would be nice to have a LinkContentHandler class that would automatically collect any &amp;lt;a href=&quot;...&quot;&amp;gt;...&amp;lt;/a&amp;gt; elements from the parser output.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/sax/Link.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/sax/LinkContentHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 30 Sep 2010 11:19:23 +0000" id="678" opendate="Thu, 30 Sep 2010 00:42:17 +0000">
		<buginformation>
			<summary>DWG parser throws ArrayIndexOutOfBoundsException when address to the header is 0x00</summary>
			<description>&lt;p&gt;For some AutoCad 2004 files, the address on 0x20, which should point to the summary info is 0x00. This causes an ArrayIndexOutOfBoundException as stated below. Attached is a document that is causing this issue and a proposed patch for the file.&lt;/p&gt;

&lt;p&gt;java.lang.ArrayIndexOutOfBoundsException: -1&lt;br/&gt;
	at org.apache.tika.parser.dwg.DWGParser.get2004Props(DWGParser.java:103)&lt;br/&gt;
	at org.apache.tika.parser.dwg.DWGParser.parse(DWGParser.java:77)&lt;br/&gt;
	at com.ravn.test.tika.AutCadTester.parseOrg(AutCadTester.java:78)&lt;br/&gt;
	at com.ravn.test.AutoCadTester.main(AutoCadTester.java:32)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:597)&lt;br/&gt;
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:115)&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/dwg/DWGParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 12 Oct 2010 20:27:04 +0000" id="679" opendate="Sat, 9 Oct 2010 11:07:28 +0000">
		<buginformation>
			<summary>Reuse tagsoup HtmlSchema instance across HtmlParsers (performance improvement)</summary>
			<description>&lt;p&gt;While parsing a set of small HTML files (email messages), I noticed using a profiler that about a third of the time was being spent in the construction of tagsoup's HTMLSchema class.&lt;/p&gt;

&lt;p&gt;Since this is (or appears to be to me) simply a data structure, it is thread safe and can be reused. Fortunately this can be done easily, as shown in the patch I will attach to this issue.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/html/HtmlParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 8 Nov 2011 19:07:39 +0000" id="680" opendate="Tue, 12 Oct 2010 01:00:10 +0000">
		<buginformation>
			<summary>IBM420 charset detection's isLamAlef is allocation-happy</summary>
			<description>&lt;p&gt;Two IBM420 charset detectors (rtl and ltr) run isLamAlef() for each byte of detection buffer.&lt;/p&gt;

&lt;p&gt;The code is allocating and filling a bytes array every time it runs, which makes it responsible for approximately 70% of all object allocations in my current test case (many text files).&lt;/p&gt;

&lt;p&gt;Since array is identical every time, and the entire thing can be achieved without any array, this is wasteful.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/txt/CharsetRecog_sbcs.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 10 Feb 2011 13:07:10 +0000" id="681" opendate="Sun, 17 Oct 2010 22:22:19 +0000">
		<buginformation>
			<summary>Mis-detection of zip files as application/vnd.apple.iwork</summary>
			<description>&lt;p&gt;It appears that, at least in some circumstances, a zip file containing only another zip file is being mis-detected as  application/vnd.apple.iwork.&lt;br/&gt;
In addition, for such files, the command-line parser does not return any output at all.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 18 Mar 2011 17:01:19 +0000" id="682" opendate="Mon, 18 Oct 2010 22:53:24 +0000">
		<buginformation>
			<summary>MetadataException: Unsupported component id error parsing jpg</summary>
			<description>&lt;p&gt;I seem to be getting repeated errors parsing JPEG images that load perfectly well in Firefox.&lt;br/&gt;
The main problem seems to be an unsupported component ID, though the actual ID number varies&lt;br/&gt;
from jpg to jpg. Example image to follow.&lt;/p&gt;

&lt;p&gt;Exception in thread &quot;main&quot; org.apache.tika.exception.TikaException: Can't read TIFF/JPEG metadata&lt;br/&gt;
        at org.apache.tika.parser.image.ImageMetadataExtractor.parse(ImageMetada&lt;br/&gt;
taExtractor.java:91)&lt;br/&gt;
        at org.apache.tika.parser.image.ImageMetadataExtractor.parseJpeg(ImageMetadataExtractor.java:69)&lt;br/&gt;
        at org.apache.tika.parser.jpeg.JpegParser.parse(JpegParser.java:56)&lt;br/&gt;
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:197)&lt;br/&gt;
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:197)&lt;br/&gt;
        at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:137)&lt;br/&gt;
        at org.apache.tika.cli.TikaCLI.process(TikaCLI.java:213)&lt;br/&gt;
        at org.apache.tika.cli.TikaCLI.main(TikaCLI.java:73)&lt;br/&gt;
Caused by: com.drew.metadata.MetadataException: Unsupported component id: 152&lt;br/&gt;
        at com.drew.metadata.jpeg.JpegComponent.getComponentName(Unknown Source)&lt;br/&gt;
        at com.drew.metadata.jpeg.JpegDescriptor.getComponentDataDescription(Unknown Source)&lt;br/&gt;
        at com.drew.metadata.jpeg.JpegDescriptor.getDescription(Unknown Source)&lt;br/&gt;
        at com.drew.metadata.Directory.getDescription(Unknown Source)&lt;br/&gt;
        at com.drew.metadata.Tag.getDescription(Unknown Source)&lt;br/&gt;
        at org.apache.tika.parser.image.ImageMetadataExtractor.parse(ImageMetadataExtractor.java:85)&lt;br/&gt;
        ... 7 more&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/image/ImageMetadataExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 19 May 2011 17:53:08 +0000" id="683" opendate="Tue, 19 Oct 2010 09:52:26 +0000">
		<buginformation>
			<summary>Implement Apache project branding requirements</summary>
			<description>&lt;p&gt;We should implement the requirements from &lt;a href=&quot;http://www.apache.org/foundation/marks/pmcs.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.apache.org/foundation/marks/pmcs.html&lt;/a&gt; latest in Q1 next year.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/README.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 6 Nov 2010 23:46:30 +0000" id="684" opendate="Thu, 4 Nov 2010 13:46:29 +0000">
		<buginformation>
			<summary>Remove rome 1.0 dependency on java.net repository</summary>
			<description>&lt;p&gt;The feeds parser (see &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-466&quot; title=&quot;Feed Parser&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-466&quot;&gt;&lt;del&gt;TIKA-466&lt;/del&gt;&lt;/a&gt;) has a dependency on Rome 1.0, as added to the tika-parser pom.xml with revision 964885.&lt;/p&gt;

&lt;p&gt;This does not exist in the Maven central repository (that's only versions up to 0.9).&lt;/p&gt;

&lt;p&gt;It worked previously due to java.net's maven repo being in the pom.xml, which was there for boilerpipe.&lt;/p&gt;

&lt;p&gt;That repository was removed (as part of &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-462&quot; title=&quot;Add Boilerpipe 1.0.4 to Maven central and remove java.net repository from parser pom&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-462&quot;&gt;&lt;del&gt;TIKA-462&lt;/del&gt;&lt;/a&gt;), so now the build fails if you don't already have rome 1.0 in your local Maven repo.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 18 Nov 2010 18:11:59 +0000" id="685" opendate="Thu, 11 Nov 2010 19:38:04 +0000">
		<buginformation>
			<summary>PDF content extracted as single line</summary>
			<description>&lt;p&gt;Rev 1029510 introduces a regression in PDF content parsing, now present in 0.8 RC. Paragraphs from the PDF are no longer separated by newline. This is a problem both for reading and for indexing. See the attached test.&lt;/p&gt;

&lt;p&gt;Note that it seems like PDFBox 1.3.1 extracts correctly, at least from command line. Here's from a sample file with a headline followed by a one word paragraph:&lt;br/&gt;
$&amp;gt; java -jar pdfbox-app-1.3.1.jar ExtractText -console docs/shortpdf.pdf&lt;br/&gt;
1   -    untitled 3   -    2010-02-13 09:52   -    Staffan Olsson&lt;br/&gt;
PDF Title For Short Document&lt;br/&gt;
veryshortpdfcontents&lt;/p&gt;

&lt;p&gt;But Tika prints:&lt;br/&gt;
$&amp;gt; java -jar tika-app-0.9-20101110.175016-3.jar docs/shortpdf.pdf&lt;br/&gt;
...&lt;br/&gt;
&amp;lt;p&amp;gt;1   -    untitled 3   -    2010-02-13 09:52   -    Staffan OlssonPDF&lt;br/&gt;
Title For Short Documentveryshortpdfcontents&amp;lt;/p&amp;gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDF2XHTML.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 12 Nov 2010 12:06:25 +0000" id="686" opendate="Fri, 12 Nov 2010 11:50:15 +0000">
		<buginformation>
			<summary>There is no support for extracting OLE-shapes from PPT</summary>
			<description>&lt;p&gt;There is no support for extracting OLE-shapes from PPT&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/OfficeParser.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/HSLFExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 12 Nov 2010 12:32:14 +0000" id="687" opendate="Fri, 12 Nov 2010 12:15:35 +0000">
		<buginformation>
			<summary>Add stable filenames for extracted embedded files from Office binaries</summary>
			<description>&lt;p&gt;This patch add usage of POI entry names as base for file name of extracted embedded file. This make file names stable and reproducible. This intended for debugging and testing&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/AbstractPOIFSExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 12 Nov 2010 21:27:18 +0000" id="688" opendate="Fri, 12 Nov 2010 20:01:42 +0000">
		<buginformation>
			<summary>Automatic license header checks</summary>
			<description>&lt;p&gt;Every now and then we get a few new files without the correct Apache license headers. It would be nice if our build could automatically detect such cases.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 1 Dec 2010 09:41:05 +0000" id="689" opendate="Tue, 23 Nov 2010 02:10:27 +0000">
		<buginformation>
			<summary>image/bmp mime type does not exist</summary>
			<description>&lt;p&gt;There is no such mime type as &quot;image/bmp&quot;. Use &quot;image/x-ms-bmp&quot; instead. Patch attached.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
			<file>/tika-core/src/test/java/org/apache/tika/TikaTest.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/image/ImageParser.java</file>
			<file>/tika-parsers/src/test/java/org/apache/tika/mime/TestMimeTypes.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 1 Dec 2010 07:35:04 +0000" id="690" opendate="Thu, 25 Nov 2010 19:11:06 +0000">
		<buginformation>
			<summary>Improve detection of .mht, Foxmail, and OOXML files</summary>
			<description>&lt;p&gt;I would like to address the following issues&lt;/p&gt;

&lt;p&gt;1. Reduce the priority of the text/html magics. WIth the default priority I have lots of .eml, .emlx, mbox and .mht files which contain html content but should not be classified as XML. The reason for that is that the HTML magic looks for &amp;lt;html&amp;gt; between 0 and 8192 offsets. In Aperture we solved this with an allowsWhiteSpace switch, so that the &amp;lt;html&amp;gt; can be prepended with whitespace but not with other content. Since there is no such switch in Tika, I suggest reducing the priority of the magic in tika-mimetypes. I attach an .mht file from the Aperture test document suite which exhibits the problem.&lt;/p&gt;

&lt;p&gt;2. Add support for detecting Foxmail. They come from Foxmail, a mail client popular in china, they are roughly the same as mbox, but use a different separator. &lt;/p&gt;

&lt;p&gt;3. In case of OOXML files, the container aware detector computes the mimetype by taking the part of &lt;span class=&quot;error&quot;&gt;&amp;#91;Content_Types.xml&amp;#93;&lt;/span&gt;, namely:&lt;/p&gt;

&lt;p&gt;&amp;lt;Default Extension=&quot;bin&quot; ContentType=&quot;application/vnd.ms-excel.sheet.binary.macroEnabled.main&quot;/&amp;gt;&lt;/p&gt;

&lt;p&gt;then it takes the default content type and returns it with the part after the last dot removed. There are two issues with this approach&lt;/p&gt;

&lt;p&gt; a. some documents use macroEnabled, while other use macroenabled so the case is not standard&lt;br/&gt;
 b. the &quot;official&quot; mime types, contain a '12' suffix at the end, as shown at: &lt;a href=&quot;http://technet.microsoft.com/en-us/library/ee309278%28office.12%29.aspx&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://technet.microsoft.com/en-us/library/ee309278%28office.12%29.aspx&lt;/a&gt;. I suggest to standardize on lowercase and add the '12' to the appropriate files.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/detect/ContainerAwareDetector.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 1 Dec 2010 08:57:05 +0000" id="691" opendate="Tue, 30 Nov 2010 23:05:58 +0000">
		<buginformation>
			<summary>In tika-mimetypes.xml OpenXML types should have x-tika-ooxml as their parent</summary>
			<description>&lt;p&gt;A couple of file types have application/x-tika-msoffice as their parent, when they should have application/x-tika-ooxml. This error is exhibited when you try to identify those files with both name and data. The data is found to be x-tika-ooxml, while the type found with the name is correct, but since it's not a subtype of x-tika-ooxml - it is not returned.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 1 Dec 2010 09:28:28 +0000" id="692" opendate="Tue, 30 Nov 2010 23:23:46 +0000">
		<buginformation>
			<summary>.vor files are Staroffice Templates, not Staroffice Writer documents</summary>
			<description>&lt;p&gt;The current tika-mimetypes.xml states that *.vor files are from vnd.stardivision.writer. This is not true. The vor extension is used by templates from all staroffice applications. Moreover all of them have the msoffice magic number.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 1 Dec 2010 00:45:21 +0000" id="693" opendate="Wed, 1 Dec 2010 00:21:54 +0000">
		<buginformation>
			<summary>Support returning original markup in BoilerpipeContentHandler</summary>
			<description>&lt;p&gt;Currently the BoilerpipeContentHandler emits all non-boilerplate text (as defined by Boilerpipe) as a series of &amp;lt;p&amp;gt;xxx&amp;lt;/p&amp;gt; text blocks, without any markup.&lt;/p&gt;

&lt;p&gt;But if you need to find URLs in these blocks, or section headers, then the original markup has to be preserved.&lt;/p&gt;

&lt;p&gt;Unfortunately Boilerpipe currently assumes that you have the ability to replay the input stream, and parse it a second time to match up text with what's been extracted (e.g. see HTMLHighlighter), but with Tika that's not easy to do - the interface is a ContentHandler, so you don't have the original inputstream to spool out.&lt;/p&gt;

&lt;p&gt;Instead I plan to record the minimum SAX events, and replay those.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/html/BoilerpipeContentHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 5 Dec 2010 12:38:55 +0000" id="694" opendate="Sun, 5 Dec 2010 11:48:58 +0000">
		<buginformation>
			<summary>Better convenience methods for type detection</summary>
			<description>&lt;p&gt;As reported by Grant on users@, the getMimeType() convenience methods in the MimeTypes class are not very up to date with the latest best practices in type detection.&lt;/p&gt;

&lt;p&gt;We should fix those methods for existing clients. It would also be good to make such convenience methods better available on the Tika facade, as there the API won't be so closely tied to the way we organize our media type handling code.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/Tika.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 19 Jan 2011 12:41:38 +0000" id="695" opendate="Sun, 5 Dec 2010 13:18:31 +0000">
		<buginformation>
			<summary>Temporary file leak in TikaInputStream</summary>
			<description>&lt;p&gt;The TikaInputStream.get(InputStream) method can end up leaking temporary files in cases like the Detector classes that will not close the returned stream.&lt;/p&gt;

&lt;p&gt;There should be a mechanism to signal that a potential TikaInputStream wrapper will no longer be used even if the underlying stream should not be closed.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/HSLFExtractor.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/AbstractPOIFSExtractor.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/extractor/ParserContainerExtractor.java</file>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/microsoft/ooxml/OOXMLParserTest.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/extractor/ParsingEmbeddedDocumentExtractor.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/io/TemporaryFiles.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 3 Jan 2011 18:14:58 +0000" id="696" opendate="Thu, 9 Dec 2010 15:51:21 +0000">
		<buginformation>
			<summary>More fault-tolerant loading of parsers and detectors</summary>
			<description>&lt;p&gt;Currently Tika will fail to start even if a single configured parser or detector can not be loaded. Such cases occur often when required parser libraries or other dependencies are not available, and it would be good if Tika could degrade more gracefully in such situations.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/config/LoadErrorHandler.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/config/ServiceLoader.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 19 May 2011 17:32:02 +0000" id="697" opendate="Wed, 15 Dec 2010 02:48:39 +0000">
		<buginformation>
			<summary>Update plugin versions in the POM structure</summary>
			<description>&lt;p&gt;The tika build uses an ancient version of the gpg plugin, and probably some other fossils as well.  It would be good to update before the next release.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 17 Dec 2010 10:34:43 +0000" id="698" opendate="Wed, 15 Dec 2010 10:48:08 +0000">
		<buginformation>
			<summary>MimeType.getExtension()</summary>
			<description>&lt;p&gt;This patch adds getExtension() method to MimeType and support for reading mime-types from mime.types format.&lt;/p&gt;

&lt;p&gt;I added mime.types file from Fedora Linux, license says that it is public domain file:&lt;/p&gt; &lt;p&gt;===&lt;br/&gt;
Red Hat disclaims any copyright on the &quot;mailcap&quot; and &quot;mime-types&quot; files and places them in the public domain. You are &lt;br/&gt;
free to do whatever you wish with these files.&lt;/p&gt;

&lt;p&gt;The mailcap.4 man page is under an MIT license:&lt;/p&gt;

&lt;p&gt;Copyright (c) 1991 Bell Communications Research, Inc. (Bellcore)&lt;/p&gt;

&lt;p&gt;Permission to use, copy, modify, and distribute this material&lt;br/&gt;
for any purpose and without fee is hereby granted, provided&lt;br/&gt;
that the above copyright notice and this permission notice&lt;br/&gt;
appear in all copies, and that the name of Bellcore not be&lt;br/&gt;
used in advertising or publicity pertaining to this&lt;br/&gt;
material without the specific, prior written permission&lt;br/&gt;
of an authorized representative of Bellcore.  BELLCORE&lt;br/&gt;
MAKES NO REPRESENTATIONS ABOUT THE ACCURACY OR SUITABILITY&lt;br/&gt;
OF THIS MATERIAL FOR ANY PURPOSE.  IT IS PROVIDED &quot;AS IS&quot;,&lt;br/&gt;
WITHOUT ANY EXPRESS OR IMPLIED WARRANTIES.&lt;/p&gt;


&lt;p&gt;Tom Callaway, Fedora Legal, Red Hat&lt;br/&gt;
Thu Sep 17, 2009&lt;br/&gt; ===&lt;/p&gt;

&lt;p&gt;(we do not need man page, only mime.types file)&lt;/p&gt;

&lt;p&gt;getExtension() method can be used for creating friendly filename for OLE-embedded files, streams and other cases when name is not known&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 19 Dec 2010 14:33:13 +0000" id="699" opendate="Thu, 16 Dec 2010 23:19:35 +0000">
		<buginformation>
			<summary>Support for IBM866 (CP866) encoding in TXTParser</summary>
			<description>&lt;p&gt;There's no recognizer for CP866 (DOS russian encoding) in tika yet.&lt;/p&gt;

			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/resources/test-documents/russian.cp866.txt</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/txt/CharsetDetector.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 7 Oct 2011 09:07:37 +0000" id="700" opendate="Mon, 3 Jan 2011 22:56:55 +0000">
		<buginformation>
			<summary>Parser fails on files that parsed with v0.7</summary>
			<description>&lt;p&gt;I am including 3 files. Two of these (Employee Application.pdf; pertoc.html) parsed without error under Tika 0.7. The third (I-9.pdf) fails under both versions and is included more as another test case for PDF parse errors.&lt;/p&gt;

&lt;p&gt;Perltoc.html fails with a Null Pointer exception in org.apache.tika.parser.html.HtmlParser@53786b79. Employee Application.pdf fails in  org.apache.tika.parser.pdf.PDFParser@39fb9fb3, also with a Null Pointer exception. Both cases repro with tikaapp.jar built from the 0.8 release.&lt;/p&gt;

&lt;p&gt;I-9 (which also fails under 0.7, but for different reasons) also fails with a Null Pointer exception at the same location as Employee Application.pdf.&lt;/p&gt;

&lt;p&gt;Looks like I have to create the bug before I can attach files...&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/sax/xpath/MatchingContentHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 19 Jan 2011 12:53:33 +0000" id="701" opendate="Mon, 17 Jan 2011 14:47:13 +0000">
		<buginformation>
			<summary>AudioParser Fails with NPE on fileFormat.properties</summary>
			<description>&lt;p&gt;AudioParserTests fail because the AudioParser does not check for existence of properties on the fileFormat. (In the parse method, about line 110. The code is:&lt;br/&gt;
                for (Entry&amp;lt;String, Object&amp;gt; entry : fileFormat.properties().entrySet()) &lt;/p&gt;
{
                    metadata.set(entry.getKey(), entry.getValue().toString());
                }&lt;br/&gt;
                for (Entry&amp;lt;String, Object&amp;gt; entry : audioFormat.properties().entrySet()) {                    metadata.set(entry.getKey(), entry.getValue().toString());                }
&lt;p&gt;Method throws NPE on fileFormat.properties().entrySet() when there are no properties.&lt;br/&gt;
should be something like:&lt;br/&gt;
            if( fileFormat.properties() != null &amp;amp;&amp;amp; fileFormat.properties().size() &amp;gt; 0) {&lt;br/&gt;
                for (Entry&amp;lt;String, Object&amp;gt; entry : fileFormat.properties().entrySet()) &lt;/p&gt;
{
                    metadata.set(entry.getKey(), entry.getValue().toString());
                }&lt;br/&gt;
            }&lt;br/&gt;
            if( audioFormat.properties() != null &amp;amp;&amp;amp; audioFormat.properties().size() &amp;gt; 0) {&lt;br/&gt;
                for (Entry&amp;lt;String, Object&amp;gt; entry : audioFormat.properties().entrySet()) {                    metadata.set(entry.getKey(), entry.getValue().toString());                }
&lt;p&gt;            }&lt;br/&gt;
Or am i missing something?&lt;/p&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/audio/AudioParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 19 Jan 2011 12:45:06 +0000" id="702" opendate="Tue, 18 Jan 2011 16:17:58 +0000">
		<buginformation>
			<summary>NullPointerException in OutlookExtractor on missing chunks</summary>
			<description>&lt;p&gt;java.lang.NullPointerException&lt;br/&gt;
	at org.apache.tika.parser.microsoft.OutlookExtractor.header(OutlookExtractor.java:150)&lt;/p&gt;

&lt;p&gt;String value might be null - see line 51: msg.setReturnNullOnMissingChunk(true);&lt;/p&gt;

&lt;p&gt;In my case, the message is missing the &quot;from&quot; chunk.&lt;/p&gt;

&lt;p&gt;Possible fixes include assigning a default value &quot;&quot; for relevant chunks, if missing or a simple check for null in the header-method.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/xml/XMLParser.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/OutlookExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 18 Mar 2011 23:17:23 +0000" id="703" opendate="Tue, 1 Feb 2011 10:35:27 +0000">
		<buginformation>
			<summary>Unsupported AutoCAD drawing version: AC1015</summary>
			<description>&lt;p&gt;D:\Downloads\apache-tika-0.8-src&amp;gt;java &lt;del&gt;jar tika-app/target/tika-app&lt;/del&gt;*.jar --gui &quot;R:\3D CAD (BE)\Montages\850\2D montage 850.dwg&quot;&lt;br/&gt;
Exception in thread &quot;main&quot; org.apache.tika.exception.TikaException: Unsupported AutoCAD drawing version: AC1015&lt;br/&gt;
        at org.apache.tika.parser.dwg.DWGParser.parse(DWGParser.java:84)&lt;br/&gt;
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:197)&lt;br/&gt;
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:197)&lt;br/&gt;
        at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:137)&lt;br/&gt;
        at org.apache.tika.cli.TikaCLI.process(TikaCLI.java:231)&lt;br/&gt;
        at org.apache.tika.cli.TikaCLI.main(TikaCLI.java:81)&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/dwg/DWGParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 19 Nov 2014 10:55:15 +0000" id="704" opendate="Thu, 10 Feb 2011 13:44:04 +0000">
		<buginformation>
			<summary>HtmlHandler does not support multivalue metadata</summary>
			<description>&lt;p&gt;The HtmlHandler uses metadata.set(...). So META tags that occure more than once are not handled correctly (DublinCore metadata can be set more than once).&lt;br/&gt;
The handler should use  metadata.add(..) instead.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/html/HtmlHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 2 Mar 2011 17:23:39 +0000" id="705" opendate="Mon, 14 Feb 2011 18:29:51 +0000">
		<buginformation>
			<summary>Bogus exception handler in org.apache.tika.parser.mail.MailContentHandler.body(BodyDescriptor, InputStream)</summary>
			<description>&lt;p&gt;org.apache.tika.parser.mail.MailContentHandler.body(BodyDescriptor, InputStream) &lt;/p&gt;

&lt;p&gt;contains an exception handler that calls printStackTrace instead of rethrowing&lt;br/&gt;
as a RuntimeException. Should it be 'throws TikaException' in any case?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/mail/MailContentHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 22 May 2011 01:57:27 +0000" id="706" opendate="Thu, 10 Jul 2008 15:54:15 +0000">
		<buginformation>
			<summary>The ExcelParsing should scan the cell comments</summary>
			<description>&lt;p&gt;During the scanning of Excel documents it might be helpful to scan or analyze the cell comments of an excel worksheet as well.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ExcelExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 20 May 2011 22:58:31 +0000" id="707" opendate="Wed, 30 Jul 2008 08:45:02 +0000">
		<buginformation>
			<summary>Allow passing of files or memory buffers to parsers</summary>
			<description>&lt;p&gt;Some of our parsers need to be able to go back and forth within a source document, so need either a file or (for smaller documents) an in-memory buffer that contains the full document. Currently we use temporary files for such cases, which in some cases means doing an extra copy of a file before it gets parsed. We should come up with some way for clients to pass in a file or a memory buffer if one is available.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/Tika.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/io/TikaInputStream.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/io/ProxyInputStream.java</file>
			<file>/tika-core/src/test/java/org/apache/tika/io/TikaInputStreamTest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 20 May 2011 16:45:17 +0000" id="708" opendate="Tue, 16 Sep 2008 21:46:48 +0000">
		<buginformation>
			<summary>Support encryption formats</summary>
			<description>&lt;p&gt;There should be a parser that for example uses the javax.crypto.CipherInputStream to decrypt incoming documents. A client application would explicitly configure the parser instance with available decryption keys and other required information.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/parser/CryptoParser.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/io/LookaheadInputStream.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 3 May 2011 07:03:06 +0000" id="709" opendate="Thu, 26 Mar 2009 13:06:55 +0000">
		<buginformation>
			<summary>JSON output from Tika CLI</summary>
			<description>&lt;p&gt;From the ApacheCon: CouchDB seems interested in Tika, and they'd like to see an option for producing JSON output from the Tika CLI.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/pom.xml</file>
			<file>/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 20 May 2011 22:23:07 +0000" id="710" opendate="Mon, 13 Jul 2009 20:08:14 +0000">
		<buginformation>
			<summary>Safe parsing of droste.zip</summary>
			<description>&lt;p&gt;In &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-216&quot; title=&quot;Zip bomb prevention&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-216&quot;&gt;&lt;del&gt;TIKA-216&lt;/del&gt;&lt;/a&gt; it was noted that a particularly formatted zip file, known as droste.zip (attached) still causes parsing problems. We should have some failsafe against such files.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/sax/SecureContentHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 22 Sep 2011 18:13:30 +0000" id="711" opendate="Fri, 12 Nov 2010 16:39:29 +0000">
		<buginformation>
			<summary>Further improvements to Word .doc and .docx parsing</summary>
			<description>&lt;p&gt;This is a follow-on to &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-506&quot; title=&quot;Improve doc and docx parsing to include more things&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-506&quot;&gt;&lt;del&gt;TIKA-506&lt;/del&gt;&lt;/a&gt;, to track the enhancements to .doc and .docx parsing between 0.8 and 0.9&lt;/p&gt;

&lt;p&gt;The list includes:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Anchors and bookmarks&lt;/li&gt;
	&lt;li&gt;Floating word .doc pictures (\u0008 rather than \u0001)&lt;/li&gt;
	&lt;li&gt;Nested word .doc tables&lt;/li&gt;
&lt;/ul&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/WordExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 10 Mar 2011 10:45:07 +0000" id="712" opendate="Thu, 3 Feb 2011 21:17:31 +0000">
		<buginformation>
			<summary>Upgrade Tika to pdfbox 1.6.0</summary>
			<description>&lt;p&gt;Secured PDFs cause Tika to throw exceptions. Upgrading to pdfbox 1.4.0 (latest release) allows these PDFs to be parsed correctly.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;update: PDFBox 1.5.0 is available&amp;#93;&lt;/span&gt;&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;update: PDFBox 1.6.0 is available&amp;#93;&lt;/span&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 17 Sep 2011 10:29:15 +0000" id="713" opendate="Thu, 17 Feb 2011 15:10:29 +0000">
		<buginformation>
			<summary>Update HDF parser and NetCDF parser to emit minimal XHTML</summary>
			<description>&lt;p&gt;In line with &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-596&quot; title=&quot;NetCDF and HDF files don&amp;#39;t parse correctly from the command line via tika-app&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-596&quot;&gt;&lt;del&gt;TIKA-596&lt;/del&gt;&lt;/a&gt;, we can improve on the current solution to the Parsers not generating any XHTML in them by simply making parsers that don't generate XHTML do so. I know that the NetCDF and HDF parsers only generate metadata at the moment, so I'll start by fixing them. &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/hdf/HDFParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 9 Mar 2011 18:21:04 +0000" id="714" opendate="Sat, 19 Feb 2011 23:28:24 +0000">
		<buginformation>
			<summary>Thread issue with autodetect parser</summary>
			<description>&lt;p&gt;When using tika to index web crawls, I seem to have run across a thread-safety issue with the autodetect parser. My indexer has gone into an apparent loop with all 5 threads in the same bit of code:&lt;/p&gt;

&lt;p&gt;&quot;Thread-5&quot; prio=3 tid=0x0898a400 nid=0x19 runnable &lt;span class=&quot;error&quot;&gt;&amp;#91;0x75238000&amp;#93;&lt;/span&gt;&lt;br/&gt;
   java.lang.Thread.State: RUNNABLE&lt;br/&gt;
        at java.util.HashMap.get(HashMap.java:303)&lt;br/&gt;
        at org.ccil.cowan.tagsoup.Schema.getElementType(Schema.java:122)&lt;br/&gt;
        at org.ccil.cowan.tagsoup.Parser.gi(Parser.java:959)&lt;br/&gt;
        at org.ccil.cowan.tagsoup.HTMLScanner.scan(HTMLScanner.java:505)&lt;br/&gt;
        at org.ccil.cowan.tagsoup.Parser.parse(Parser.java:449)&lt;br/&gt;
        at org.apache.tika.parser.html.HtmlParser.parse(HtmlParser.java:198)&lt;br/&gt;
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:197)&lt;br/&gt;
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:197)&lt;br/&gt;
        at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:135)&lt;br/&gt;
        ....&lt;/p&gt;

&lt;p&gt;This is the same line in HashMap as we see in &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-374&quot; title=&quot;AutoDetectParser not thread-safe?&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-374&quot;&gt;&lt;del&gt;TIKA-374&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Let me know if you need any more information. This is with the latest tika-0.9.&lt;/p&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/html/HtmlParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 9 Mar 2011 17:16:28 +0000" id="715" opendate="Sun, 20 Feb 2011 02:17:47 +0000">
		<buginformation>
			<summary>[patch] suspect transferable code </summary>
			<description>&lt;p&gt;the importData code has suspect code for urlListFlavor handling.&lt;/p&gt;

&lt;p&gt;the files variable is never set in this case.&lt;/p&gt;

&lt;p&gt;which caused the NPE... Patch fixes the npe.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/main/java/org/apache/tika/gui/ParsingTransferHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 9 Mar 2011 17:30:20 +0000" id="716" opendate="Sun, 20 Feb 2011 02:39:52 +0000">
		<buginformation>
			<summary>[patch] objects that compareTo each other, should also equals each other</summary>
			<description>&lt;p&gt;various classes implement compareTo but don't implement equals. These two methods should work the same as some built-in class in the jdk have switched implementations thru different versions to use compareTo vs equals. In addition, added hashCode methods for there classes.&lt;/p&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/mime/Magic.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 9 Mar 2011 17:32:57 +0000" id="717" opendate="Sun, 20 Feb 2011 02:46:55 +0000">
		<buginformation>
			<summary>[patch] use short-cuircuiting rel ops</summary>
			<description>&lt;p&gt;code uses non shortcircuiting &amp;amp; --&amp;gt; patch switches to &amp;amp;&amp;amp;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/txt/CharsetRecog_UTF8.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 4 Mar 2011 16:09:00 +0000" id="718" opendate="Wed, 23 Feb 2011 13:32:04 +0000">
		<buginformation>
			<summary>NumberFormatException when parsing an mp3-file</summary>
			<description>&lt;p&gt;When parsing an mp3-file with lyrics in the metadata I get the following error:&lt;/p&gt;

&lt;p&gt;org.apache.tika.exception.TikaException: Unexpected RuntimeException from org.apache.tika.parser.mp3.Mp3Parser@cf546f8&lt;br/&gt;
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:199)&lt;br/&gt;
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:197)&lt;br/&gt;
	at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:137)&lt;br/&gt;
	at org.apache.tika.gui.TikaGUI.importStream(TikaGUI.java:186)&lt;br/&gt;
	at org.apache.tika.gui.ParsingTransferHandler.importData(ParsingTransferHandler.java:89)&lt;br/&gt;
	at javax.swing.TransferHandler.importData(TransferHandler.java:762)&lt;br/&gt;
	at javax.swing.TransferHandler$DropHandler.drop(TransferHandler.java:1485)&lt;br/&gt;
	at java.awt.dnd.DropTarget.drop(DropTarget.java:446)&lt;br/&gt;
	at javax.swing.TransferHandler$SwingDropTarget.drop(TransferHandler.java:1210)&lt;br/&gt;
	at sun.awt.dnd.SunDropTargetContextPeer.processDropMessage(SunDropTargetContextPeer.java:517)&lt;br/&gt;
	at sun.awt.X11.XDropTargetContextPeer.processDropMessage(XDropTargetContextPeer.java:183)&lt;br/&gt;
	at sun.awt.dnd.SunDropTargetContextPeer$EventDispatcher.dispatchDropEvent(SunDropTargetContextPeer.java:830)&lt;br/&gt;
	at sun.awt.dnd.SunDropTargetContextPeer$EventDispatcher.dispatchEvent(SunDropTargetContextPeer.java:754)&lt;br/&gt;
	at sun.awt.dnd.SunDropTargetEvent.dispatch(SunDropTargetEvent.java:48)&lt;br/&gt;
	at java.awt.Component.dispatchEventImpl(Component.java:4324)&lt;br/&gt;
	at java.awt.Container.dispatchEventImpl(Container.java:2163)&lt;br/&gt;
	at java.awt.Component.dispatchEvent(Component.java:4295)&lt;br/&gt;
	at java.awt.LightweightDispatcher.retargetMouseEvent(Container.java:4461)&lt;br/&gt;
	at java.awt.LightweightDispatcher.processDropTargetEvent(Container.java:4196)&lt;br/&gt;
	at java.awt.LightweightDispatcher.dispatchEvent(Container.java:4050)&lt;br/&gt;
	at java.awt.Container.dispatchEventImpl(Container.java:2149)&lt;br/&gt;
	at java.awt.Window.dispatchEventImpl(Window.java:2478)&lt;br/&gt;
	at java.awt.Component.dispatchEvent(Component.java:4295)&lt;br/&gt;
	at java.awt.EventQueue.dispatchEvent(EventQueue.java:604)&lt;br/&gt;
	at java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:275)&lt;br/&gt;
	at java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:200)&lt;br/&gt;
	at java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:190)&lt;br/&gt;
	at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:185)&lt;br/&gt;
	at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:177)&lt;br/&gt;
	at java.awt.EventDispatchThread.run(EventDispatchThread.java:138)&lt;br/&gt;
Caused by: java.lang.NumberFormatException: For input string: &quot;peg is&quot;&lt;br/&gt;
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)&lt;br/&gt;
	at java.lang.Integer.parseInt(Integer.java:481)&lt;br/&gt;
	at java.lang.Integer.parseInt(Integer.java:514)&lt;br/&gt;
	at org.apache.tika.parser.mp3.LyricsHandler.&amp;lt;init&amp;gt;(LyricsHandler.java:92)&lt;br/&gt;
	at org.apache.tika.parser.mp3.LyricsHandler.&amp;lt;init&amp;gt;(LyricsHandler.java:42)&lt;br/&gt;
	at org.apache.tika.parser.mp3.Mp3Parser.getAllTagHandlers(Mp3Parser.java:151)&lt;br/&gt;
	at org.apache.tika.parser.mp3.Mp3Parser.parse(Mp3Parser.java:64)&lt;br/&gt;
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:197)&lt;br/&gt;
	... 29 more&lt;/p&gt;


&lt;p&gt;I tried to do it with tika version 0.8 and 0.9. They had the same result.&lt;br/&gt;
I can send you the mp3's if necessary.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/mp3/LyricsHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 9 Mar 2011 16:31:53 +0000" id="719" opendate="Sat, 26 Feb 2011 01:31:07 +0000">
		<buginformation>
			<summary>ParseUtils.getStringContent( ) of a text file - parser is null </summary>
			<description>&lt;p&gt;Hey, I'm trying to get content of a text file (mysql config file).&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;	&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void testTikaParserUtils() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; Exception {
		&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; resourceLocation = &lt;span class=&quot;code-quote&quot;&gt;&quot;files/my.cnf&quot;&lt;/span&gt;;
		&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; content = ParseUtils.getStringContent(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; File(resourceLocation), &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; TikaConfig());
		&lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.out.println(content);
	}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;OR&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;	&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void testTikaParserUtils() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; Exception {
		&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; resourceLocation = &lt;span class=&quot;code-quote&quot;&gt;&quot;files/my.cnf&quot;&lt;/span&gt;;
		&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; content = ParseUtils.getStringContent(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; File(resourceLocation), TikaConfig.getDefaultConfig());
		&lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.out.println(content);
	}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;but I get null pointer exception, because &quot;parser&quot; is null&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-style: solid;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeHeader panelHeader&quot; style=&quot;border-bottom-width: 1px;border-bottom-style: solid;&quot;&gt;&lt;b&gt;ParseUtils.java&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; getStringContent(
            InputStream stream, TikaConfig config, &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; mimeType)
            &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; TikaException, IOException {
        &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
            Parser parser = config.getParser(MediaType.parse(mimeType));
            ContentHandler handler = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; BodyContentHandler();
            parser.parse(stream, handler, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Metadata());
            &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; handler.toString();
        } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; (SAXException e) {
            &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; TikaException(&lt;span class=&quot;code-quote&quot;&gt;&quot;Unexpected SAX error&quot;&lt;/span&gt;, e);
        }
    }}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt; 
&lt;p&gt;&lt;font color=&quot;red&quot;&gt; &lt;br/&gt;
java.lang.NullPointerException&lt;br/&gt;
	at org.apache.tika.utils.ParseUtils.getStringContent(ParseUtils.java:112)&lt;br/&gt;
	at org.apache.tika.utils.ParseUtils.getStringContent(ParseUtils.java:171)&lt;br/&gt;
	at org.apache.tika.utils.ParseUtils.getStringContent(ParseUtils.java:189)&lt;br/&gt;
	at cz.instance.transl.tests.TikaTest.testTikaParserUtils(TikaTest.java:53)&lt;br/&gt;
	at org.apache.maven.surefire.testng.TestNGExecutor.run(TestNGExecutor.java:73)&lt;br/&gt;
	at org.apache.maven.surefire.testng.TestNGXmlTestSuite.execute(TestNGXmlTestSuite.java:95)&lt;br/&gt;
	at org.apache.maven.surefire.testng.TestNGProvider.invoke(TestNGProvider.java:101)&lt;br/&gt;
	at org.apache.maven.surefire.booter.ProviderFactory$ClassLoaderProxy.invoke(ProviderFactory.java:101)&lt;br/&gt;
	at $Proxy0.invoke(Unknown Source)&lt;br/&gt;
	at org.apache.maven.surefire.booter.SurefireStarter.invokeProvider(SurefireStarter.java:139)&lt;br/&gt;
	at org.apache.maven.surefire.booter.SurefireStarter.runSuitesInProcess(SurefireStarter.java:82)&lt;br/&gt;
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:81)&lt;br/&gt;
... Removed 24 stack frames&lt;/font&gt;&lt;/p&gt;

&lt;p&gt;It works only if I specifically determine the type of parser &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;	@Test
	&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void testTikaParserUtils() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; Exception {
		Tika tika = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Tika(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; TextDetector());
		&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; content = tika.parseToString(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; File(txt));
		&lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.out.println(content);
	}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/utils/ParseUtils.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 9 Mar 2011 15:32:42 +0000" id="720" opendate="Wed, 2 Mar 2011 01:51:47 +0000">
		<buginformation>
			<summary>IOException from jempbox</summary>
			<description>&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;$ java -jar tika-app-0.9.jar ChateauFrontenacQC.jpg 
[Fatal Error] :47:39: Character reference &quot;&amp;amp;#x5&quot; is an invalid XML character.
Exception in thread &quot;main&quot; org.apache.tika.exception.TikaException: TIKA-198: Illegal IOException from org.apache.tika.parser.jpeg.JpegParser@17353249
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:203)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:197)
	at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:135)
	at org.apache.tika.cli.TikaCLI$OutputType.process(TikaCLI.java:107)
	at org.apache.tika.cli.TikaCLI.process(TikaCLI.java:302)
	at org.apache.tika.cli.TikaCLI.main(TikaCLI.java:91)
Caused by: java.io.IOException: Character reference &quot;&amp;amp;#x5&quot; is an invalid XML character.
	at org.apache.jempbox.impl.XMLUtil.parse(XMLUtil.java:100)
	at org.apache.jempbox.xmp.XMPMetadata.load(XMPMetadata.java:538)
	at org.apache.tika.parser.image.xmp.JempboxExtractor.parse(JempboxExtractor.java:59)
	at org.apache.tika.parser.jpeg.JpegParser.parse(JpegParser.java:69)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:197)
	... 5 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Interestingly, accessing via HTTP gives a different error:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;$ java -jar tika-app-0.9.jar http://www.aace.org/conf/cities/quebecCity/ChateauFrontenacQC.jpg 
Exception in thread &quot;main&quot; org.apache.tika.exception.TikaException: Can't read JPEG metadata
	at org.apache.tika.parser.image.ImageMetadataExtractor.parseJpeg(ImageMetadataExtractor.java:92)
	at org.apache.tika.parser.jpeg.JpegParser.parse(JpegParser.java:66)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:197)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:197)
	at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:135)
	at org.apache.tika.cli.TikaCLI$OutputType.process(TikaCLI.java:107)
	at org.apache.tika.cli.TikaCLI.process(TikaCLI.java:302)
	at org.apache.tika.cli.TikaCLI.main(TikaCLI.java:91)
Caused by: com.drew.imaging.jpeg.JpegProcessingException: segment size would extend beyond file stream length
	at com.drew.imaging.jpeg.JpegSegmentReader.readSegments(Unknown Source)
	at com.drew.imaging.jpeg.JpegSegmentReader.&amp;lt;init&amp;gt;(Unknown Source)
	at com.drew.imaging.jpeg.JpegMetadataReader.readMetadata(Unknown Source)
	at org.apache.tika.parser.image.ImageMetadataExtractor.parseJpeg(ImageMetadataExtractor.java:87)
	... 7 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/image/ImageMetadataExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 9 Mar 2011 09:05:22 +0000" id="721" opendate="Mon, 7 Mar 2011 13:25:42 +0000">
		<buginformation>
			<summary>PDFParser mixes the text from separate columns</summary>
			<description>&lt;p&gt;As reported on the dev list by  Michael Schmitz :&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I don't think the current snapshot is parsing articles (pdfs with columns/beads) correctly.  The text is not in the write order as it intermixes text from different beads.  Try it on an academic paper. &lt;a href=&quot;http://turing.cs.washington.edu/papers/acl08.pdf&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://turing.cs.washington.edu/papers/acl08.pdf&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This can be fixed by changing the value of setSortByPosition to false, which is the default value in PDFTextStripper. This line (PDF2XHTML:82) had been added as part of the commit rev 1029510, see &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-446?focusedCommentId=12926787&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-12926787&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/TIKA-446?focusedCommentId=12926787&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-12926787&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Ideally we could specify what value to set for these parameters via the Context object, but for the time being wouldn't it make sense to set setSortByPosition to the default value of false? I think that this would be the best option for most cases where docs have columns.&lt;/p&gt;



			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDF2XHTML.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 17 Nov 2011 17:25:01 +0000" id="722" opendate="Wed, 9 Mar 2011 09:02:51 +0000">
		<buginformation>
			<summary>Specify PDFBox options via ParseContext </summary>
			<description>&lt;p&gt;See &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-611&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/TIKA-611&lt;/a&gt;. The options used by PDFBox are currently hardwritten in the PDFParser code, we will allow them to be specified via the ParseContext objects&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 11 Apr 2011 11:50:59 +0000" id="723" opendate="Mon, 14 Mar 2011 15:18:55 +0000">
		<buginformation>
			<summary>Support TNEF (winmail.dat) via POI</summary>
			<description>&lt;p&gt;As of POI 3.8 beta 2, POI will support decoding TNEF (winamil.dat) files. (The code is in SVN, but just missed being in 3.8 beta 1)&lt;/p&gt;

&lt;p&gt;This issue is to track adding support to Tika for getting:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;message contents (the RTF of the message body)&lt;/li&gt;
	&lt;li&gt;message metadata (probably just the subject for now)&lt;/li&gt;
	&lt;li&gt;attachment contents + metadata (via the usual container contents method)&lt;/li&gt;
&lt;/ul&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/TNEFParser.java</file>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 24 Mar 2011 15:36:15 +0000" id="724" opendate="Thu, 24 Mar 2011 09:55:59 +0000">
		<buginformation>
			<summary>Parsers and non-canonical mimetypes</summary>
			<description>&lt;p&gt;As discovered though &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-555&quot; title=&quot;image/bmp mime type does not exist&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-555&quot;&gt;&lt;del&gt;TIKA-555&lt;/del&gt;&lt;/a&gt;, we weren't correctly handling the case of non canonical mimetypes properly&lt;/p&gt;

&lt;p&gt;There are three bits to this:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;The default parser as created by TikaConfig.getDefaultConfig() needs the full mime registry passing in automatically, so it can walk the mime tree. Fixed in r1084801&lt;/li&gt;
	&lt;li&gt;The Composite Parser needs to handle child parsers that declare they support an alias rather than the canonical mimetype. Initial fix in r1084798, Jukka has an idea for a cleaner way&lt;/li&gt;
	&lt;li&gt;When Composite Parser looks for a parser for a mime type, it'll need to canonicalise this before checking. (this isn't a problem for the Auto Detect parser, but can be for others)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;We also probably need a couple more unit tests for this, as &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-555&quot; title=&quot;image/bmp mime type does not exist&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-555&quot;&gt;&lt;del&gt;TIKA-555&lt;/del&gt;&lt;/a&gt; had broken auto detect parsing of bmp (though not direct ImageParser), though none of our tests noticed...&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/config/TikaConfig.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/parser/CompositeParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 13 Apr 2011 17:08:21 +0000" id="725" opendate="Thu, 24 Mar 2011 23:05:06 +0000">
		<buginformation>
			<summary>RTF parsing fails with Java 7 early access on 64bit platforms</summary>
			<description>&lt;p&gt;I've run across an RTF documents which tika is failing to convert on 64bit platforms (Windows and Linux) using the Java 7 early access version. The same document is successfully converted on 32bit Windows and Linux, and using Java 6.&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;java -jar tika-app-0.9.jar -t full.rtf 
Exception in thread &quot;main&quot; org.apache.tika.exception.TikaException: Unexpected RuntimeException from org.apache.tika.parser.rtf.RTFParser@1fa78298
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:199)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:197)
	at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:135)
	at org.apache.tika.cli.TikaCLI$OutputType.process(TikaCLI.java:107)
	at org.apache.tika.cli.TikaCLI.process(TikaCLI.java:302)
	at org.apache.tika.cli.TikaCLI.main(TikaCLI.java:91)
Caused by: java.lang.NullPointerException
	at javax.swing.text.GapContent.compare(Unknown Source)
	at javax.swing.text.GapContent.findSortIndex(Unknown Source)
	at javax.swing.text.GapContent.createPosition(Unknown Source)
	at javax.swing.text.AbstractDocument.createPosition(Unknown Source)
	at javax.swing.text.AbstractDocument$LeafElement.&amp;lt;init&amp;gt;(Unknown Source)
	at javax.swing.text.AbstractDocument.createLeafElement(Unknown Source)
	at javax.swing.text.DefaultStyledDocument$ElementBuffer.insertElement(Unknown Source)
	at javax.swing.text.DefaultStyledDocument$ElementBuffer.insertUpdate(Unknown Source)
	at javax.swing.text.DefaultStyledDocument$ElementBuffer.insert(Unknown Source)
	at javax.swing.text.DefaultStyledDocument.insertUpdate(Unknown Source)
	at javax.swing.text.AbstractDocument.handleInsertString(Unknown Source)
	at javax.swing.text.AbstractDocument.insertString(Unknown Source)
	at org.apache.tika.parser.rtf.RTFParser$CustomStyledDocument.insertString(RTFParser.java:376)
	at javax.swing.text.rtf.RTFReader$DocumentDestination.deliverText(Unknown Source)
	at javax.swing.text.rtf.RTFReader$TextHandlingDestination.handleText(Unknown Source)
	at javax.swing.text.rtf.RTFReader.handleText(Unknown Source)
	at javax.swing.text.rtf.RTFParser.write(Unknown Source)
	at javax.swing.text.rtf.AbstractFilter.write(Unknown Source)
	at javax.swing.text.rtf.AbstractFilter.readFromStream(Unknown Source)
	at javax.swing.text.rtf.RTFEditorKit.read(Unknown Source)
	at org.apache.tika.parser.rtf.RTFParser.parse(RTFParser.java:112)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:197)
	... 5 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The document in question is available at &lt;a href=&quot;http://public.funnelback.com/full.rtf&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://public.funnelback.com/full.rtf&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/rtf/RTFParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 5 Oct 2011 09:04:32 +0000" id="726" opendate="Fri, 25 Mar 2011 15:46:14 +0000">
		<buginformation>
			<summary>Switch from POIFSFileSystem to NPOIFSFileSystem, for speed and memory improvements</summary>
			<description>&lt;p&gt;With POI 3.8 beta 1, there's an alternate OLE2 implementation NPOIFSFileSystem. From an API level, it's very similar to the existing POIFSFileSystem class, but internally it's lower memory and generally faster.&lt;/p&gt;

&lt;p&gt;While NPOIFSFileSystem is currently read only (the write support has a couple of gaps), for Tika's needs that isn't an issue&lt;/p&gt;

&lt;p&gt;We should therefore switch our uses of POIFSFileSystem to NPOIFSFileSystem to reduce memory and enhance speed.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/detect/POIFSContainerDetector.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 13 Apr 2011 16:48:33 +0000" id="727" opendate="Wed, 30 Mar 2011 16:30:28 +0000">
		<buginformation>
			<summary>Easier XML parser extensibility</summary>
			<description>&lt;p&gt;The DcXMLParser class uses our streaming XPath mechanism to locate Dublin Core elements from a stream of SAX events. While powerful, that mechanism is a bit cumbersome to use for simple use cases where you'd just want to map the contents of a specific XML element or attribute into a metadata field. To make this simpler (and to remove the XPath processing overhead), I'd like to add new Attribute- and ElementMetadataHandler utility classes that focus on this specific use case.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/xml/AbstractMetadataHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 31 Mar 2011 16:15:30 +0000" id="728" opendate="Wed, 30 Mar 2011 18:07:59 +0000">
		<buginformation>
			<summary>Add an AbstractParser class</summary>
			<description>&lt;p&gt;The deprecated parse() method in the Parser interface causes quite a few repetitive method declarations in all parser classes, so to simplify things I'd like to introduce a BaseParser class that provides reasonable default implementations of all Parser methods and can be used as the base class of any Parser implementation. It would be analogous to the DefaultHandler class in SAX, and would also make it easier for us to introduce new Parser methods if needed later on without necessarily breaking too many existing Parser classes.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/main/java/org/apache/tika/gui/TikaGUI.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 20 May 2011 12:17:51 +0000" id="729" opendate="Thu, 31 Mar 2011 20:54:40 +0000">
		<buginformation>
			<summary>Binary distribution for releases</summary>
			<description>&lt;p&gt;At the moment, we don't produce a binary distribution, only a source one. As discussed on the mailing list a few times, I think we should change this.&lt;/p&gt;

&lt;p&gt;I'd suggest that we build a binary package that contains:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;tika-app, tika-parsers and tika-core jar files&lt;/li&gt;
	&lt;li&gt;javadocs&lt;/li&gt;
	&lt;li&gt;a directory with all the dependencies in it (likely the same list as used in the tika bundle)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This would allow non maven users to quickly get started with using Tika.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 31 Mar 2011 21:50:25 +0000" id="730" opendate="Thu, 31 Mar 2011 21:01:59 +0000">
		<buginformation>
			<summary>WMA and ASF detection</summary>
			<description>&lt;p&gt;We don't currently have mime magic detection for the Microsoft audio and video formats like ASF, WMV and WMA&lt;/p&gt;

&lt;p&gt;From looking at &lt;a href=&quot;http://file.fyicenter.com/File-Extension-WMA-Windows-Media-Audio.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://file.fyicenter.com/File-Extension-WMA-Windows-Media-Audio.html&lt;/a&gt; the magic bytes for the ASF container format are 0x3026b275, and WMV and WMA are stored. From their sample files, we'd want to search for the unicode strings &quot;Windows Media Audio&quot; and &quot;Windows Media Video&quot; to detect within that.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/mime/MimeTypesReader.java</file>
			<file>/tika-parsers/src/test/resources/test-documents/testASF.asf</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 8 Jun 2011 12:45:56 +0000" id="731" opendate="Fri, 1 Apr 2011 15:04:42 +0000">
		<buginformation>
			<summary>Improve handling of Outlook emails which contain html, and those with non-unicode text bodies</summary>
			<description>&lt;p&gt;There are two areas in the outlook parsing that could use some enhancement:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;If the text content is stored as non unicode strings, we should try to have the right encoding used for this&lt;/li&gt;
	&lt;li&gt;If the email contains a html or rtf version of the message body, we should prefer this over the plain text one&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Note - some of these enhancements may need to wait for POI 3.8 beta 3&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/OutlookExtractor.java</file>
			<file>/tika-parsers/pom.xml</file>
			<file>/tika-parsers/src/test/resources/test-documents/testMSG_chinese.msg</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 1 Oct 2011 10:56:23 +0000" id="732" opendate="Fri, 1 Apr 2011 16:13:53 +0000">
		<buginformation>
			<summary>Rtf parsing ignores links</summary>
			<description>&lt;p&gt;I spotted this while working on &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-631&quot; title=&quot;Improve handling of Outlook emails which contain html, and those with non-unicode text bodies&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-631&quot;&gt;&lt;del&gt;TIKA-631&lt;/del&gt;&lt;/a&gt; - an RTF file containing links has the link skipped over - neither the link text nor the link href are output.&lt;/p&gt;

&lt;p&gt;In the attached sample file (which is the RTF contents of /test-documents/test-outlook2003.msg), we should see things like:&lt;/p&gt;

&lt;p&gt;[a href=&quot;http://r.office.microsoft.com/r/rlidOutlookWelcomeMail1?clid=1033&quot;&amp;gt;Streamlined Mail Experience[/a&amp;gt; - Outlook&lt;/p&gt;

&lt;p&gt;Instead, all we get is &quot; - Outlook&quot;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/rtf/RTFParserTest.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 5 Apr 2011 08:48:50 +0000" id="733" opendate="Mon, 4 Apr 2011 14:02:29 +0000">
		<buginformation>
			<summary>NPE in XWPFWordExtractorDecorator.extractHeaders</summary>
			<description>&lt;p&gt;I've got an word document (docx) that was written by POI-3.7 and I'm trying to read it back in and I'm getting this NPE:&lt;/p&gt;

&lt;p&gt;Caused by: java.lang.NullPointerException&lt;br/&gt;
	at org.apache.tika.parser.microsoft.ooxml.XWPFWordExtractorDecorator.extractHeaders(XWPFWordExtractorDecorator.java:234)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.ooxml.XWPFWordExtractorDecorator.buildXHTML(XWPFWordExtractorDecorator.java:71)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.ooxml.AbstractOOXMLExtractor.getXHTML(AbstractOOXMLExtractor.java:99)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.ooxml.OOXMLExtractorFactory.parse(OOXMLExtractorFactory.java:83)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.ooxml.OOXMLParser.parse(OOXMLParser.java:67)&lt;br/&gt;
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:197)&lt;/p&gt;

&lt;p&gt;From what I can see, a document may not necessarily have a headerfooterpolicy.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/XWPFWordExtractorDecorator.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 5 Apr 2011 08:48:50 +0000" id="734" opendate="Wed, 6 Apr 2011 11:55:15 +0000">
		<buginformation>
			<summary>Command Line Parser for Metadata Extraction</summary>
			<description>&lt;p&gt;As discussed on the mailing list:&lt;br/&gt;
&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/tika-dev/201104.mbox/%3Calpine.DEB.2.00.1104052028380.29085@urchin.earth.li%3E&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://mail-archives.apache.org/mod_mbox/tika-dev/201104.mbox/%3Calpine.DEB.2.00.1104052028380.29085@urchin.earth.li%3E&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This issue is to track improvements in the ExternalParser support to handle metadata extraction, and probably easier configuration of an external parser too.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/parser/external/CompositeExternalParser.java</file>
			<file>/tika-parsers/src/main/resources/META-INF/services/org.apache.tika.parser.Parser</file>
			<file>/tika-core/src/main/java/org/apache/tika/config/ServiceLoader.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 20 May 2011 14:46:37 +0000" id="735" opendate="Thu, 7 Apr 2011 09:07:12 +0000">
		<buginformation>
			<summary>Tika GUI improvements</summary>
			<description>&lt;p&gt;I'm doing a couple of Tika demos in the near future and would like to spice up the Tika GUI a bit to make it more demoable and easier for fresh new users to play with. For example I'd like to replace the current tab structure with a menubar for File (open file/url, etc.), View (current tab selection) and About (help, notice, license, etc.) options. It would also be great to have the tika-app jar launch the GUI when double-clicked in a file explorer.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java</file>
			<file>/tika-app/src/main/resources-filtered/org/apache/tika/gui/about.html</file>
			<file>/tika-app/pom.xml</file>
			<file>/tika-app/src/main/java/org/apache/tika/gui/ParsingTransferHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 12 Apr 2011 12:53:03 +0000" id="736" opendate="Tue, 12 Apr 2011 11:51:20 +0000">
		<buginformation>
			<summary>Maximum pool size for ForkParser</summary>
			<description>&lt;p&gt;Currently the poolSize parameter in the ForkParser class just sets the number of processes that are kept in the background even if no active parsing tasks are in progress. New parsing request are always given a new process if there are none left in the pool. In the worst case this can cause lots of processes being forked during peak loads.&lt;/p&gt;

&lt;p&gt;I'd like to change the poolSize parameter to specify the maximum number of background processes that the ForkParser can&lt;br/&gt;
use at any given time. Unused processes would stay in the pool up to the default five-second idle time after which they are reclaimed and the pool shrunk until new requests come in again.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/test/java/org/apache/tika/fork/ForkParserTest.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/fork/ForkParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 17 May 2011 18:13:24 +0000" id="737" opendate="Fri, 15 Apr 2011 21:38:09 +0000">
		<buginformation>
			<summary>RFC822Parser should configure Mime4j not to fail reading mails containing more than 1000 chars in one headers text (even if folded)</summary>
			<description>&lt;p&gt;Standard configuration of Mime4j accepts only 1000 characters per line and 1000 charackters per header. The streaming approach of tika should not need theese limitations, an exception is being thrown and none of the data read is available.&lt;/p&gt;

&lt;p&gt;Solution:&lt;br/&gt;
Replace all occurences of:&lt;/p&gt;

&lt;p&gt;Parser parser = new RFC822Parser();&lt;/p&gt;

&lt;p&gt;by:&lt;/p&gt;

&lt;p&gt;MimeEntityConfig config = new MimeEntityConfig();&lt;br/&gt;
config.setMaxLineLen(-1);&lt;br/&gt;
config.setMaxContentLen(-1);&lt;br/&gt;
Parser parser = new RFC822Parser(config);&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/mail/RFC822Parser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 20 Apr 2011 14:59:29 +0000" id="738" opendate="Wed, 20 Apr 2011 14:20:26 +0000">
		<buginformation>
			<summary>parsing of Microsoft Word doc with style &quot;Heading X&quot; where X&gt;6 creates invalid HTML with tags &lt;h7&gt;,&lt;h8&gt; etc</summary>
			<description>&lt;p&gt;org.apache.tika.parser.microsoft.WordExtractor will translate heading styles to &quot;h&quot; tags with a level greater than 6 which means the xhtml is invalid. The xhtml DTD only defines header elements 1 to 6:&lt;br/&gt;
&amp;lt;!ENTITY % heading &quot;h1|h2|h3|h4|h5|h6&quot;&amp;gt;&lt;/p&gt;

&lt;p&gt;changing line 380 from:&lt;br/&gt;
tag = &quot;h&quot;+num;&lt;br/&gt;
to&lt;br/&gt;
tag = &quot;h&quot;+Math.min(num, 6);&lt;/p&gt;

&lt;p&gt;will resolve this. &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/WordExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 19 May 2011 14:01:45 +0000" id="739" opendate="Thu, 21 Apr 2011 16:03:18 +0000">
		<buginformation>
			<summary>Parsers can't get at an underlying TikaInputStream to get the file if they wanted one</summary>
			<description>&lt;p&gt;Spotted this with the office parser, but it should be general. The user creates a TikaInputStream, and passes that off to the parser framework. The Parser that is called may wish to spot that the input is a File backed TikaInputStream, and take a shortcut to use the file instead of the InputStream.&lt;/p&gt;

&lt;p&gt;However, what the parser gets is a TaggedInputStream wrapping a CountingInputStream wrapping the original TikaInputStream. As such, it can't get at the file.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/io/TikaInputStream.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/io/TaggedInputStream.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 15 May 2011 21:17:53 +0000" id="740" opendate="Fri, 22 Apr 2011 12:38:42 +0000">
		<buginformation>
			<summary>tika command line can't extract metadata for OOXML files</summary>
			<description>&lt;p&gt;Tika CLI application displays metadata on endDocument() event. Some parsers (OOXML for example) fills metadata after text extraction (after endDocument), that data is missed in output.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/sax/EndDocumentShieldingContentHandler.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/OOXMLExtractorFactory.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 28 Apr 2011 02:47:59 +0000" id="741" opendate="Wed, 27 Apr 2011 14:32:17 +0000">
		<buginformation>
			<summary>NPE while parsing a .docx  </summary>
			<description>&lt;p&gt;The method extractHeaders in org.apache.tika.parser.microsoft.ooxml.XWPFWordExtractorDecorator throws a NPE on line 234 as XWPFHeaderFooterPolicy hfPolicy is null.&lt;/p&gt;

			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/XWPFWordExtractorDecorator.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 17 May 2011 18:40:59 +0000" id="742" opendate="Sun, 1 May 2011 05:55:01 +0000">
		<buginformation>
			<summary>Missing required alt attribute on img tag</summary>
			<description>&lt;p&gt;I've written a content handler that prints out the xhtml tags for conversion from a word document with embedded images. For images, it does not generate the &quot;alt&quot; attribute for img tags, which causes validation to fail. alt is a required attribute in XHTML.&lt;/p&gt;

&lt;p&gt;Here's a partial output from &lt;a href=&quot;http://validator.w3.org/check&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://validator.w3.org/check&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Error Line 3, Column 1026: required attribute &quot;alt&quot; not specified&lt;/p&gt;

&lt;p&gt;...meta&amp;gt;&amp;lt;title&amp;gt; &amp;lt;/title&amp;gt;&amp;lt;/head&amp;gt;&amp;lt;body&amp;gt;&amp;lt;p&amp;gt;&amp;lt;img src=&quot;embedded:image63.jpg&quot;&amp;gt;&amp;lt;/img&amp;gt;&amp;lt;/p&amp;gt;&lt;/p&gt;

&lt;p&gt;✉&lt;/p&gt;

&lt;p&gt;The attribute given above is required for an element that you've used, but you have omitted it. For instance, in most HTML and XHTML document types the &quot;type&quot; attribute is required on the &quot;script&quot; element and the &quot;alt&quot; attribute is required for the &quot;img&quot; element.&lt;/p&gt;

&lt;p&gt;Typical values for type are type=&quot;text/css&quot; for &amp;lt;style&amp;gt; and type=&quot;text/javascript&quot; for &amp;lt;script&amp;gt;.&lt;/p&gt;&lt;/blockquote&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/html/HtmlHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 14 Mar 2015 03:12:37 +0000" id="743" opendate="Sun, 1 May 2011 06:01:13 +0000">
		<buginformation>
			<summary>Unescaped attribute value generated</summary>
			<description>&lt;p&gt;I've converted a word document that contains hyperlinks with a complex query component. The &amp;amp; character is not escaped and mozilla complains about that when I write out the XHTML via a content handler that I wrote.&lt;/p&gt;

&lt;p&gt;It's not clear to me whether or not my contenthandler should assume attributes are properly escaped or not.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/sax/ToHTMLContentHandler.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/sax/ToTextContentHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 6 May 2011 06:31:05 +0000" id="744" opendate="Tue, 3 May 2011 05:16:17 +0000">
		<buginformation>
			<summary>Custom metadata from more formats</summary>
			<description>&lt;p&gt;Currently, Tika handles custom metadata from Open Document files. Any custom metadata is returned with a custom: prefix (see OpenOfficeParserTest#testOO2Metadata for example)&lt;/p&gt;

&lt;p&gt;Microsoft file formats don't include custom metadata in the parsing, and nor does PDF&lt;/p&gt;

&lt;p&gt;Assuming we're happy with including custom metadata from Documents in the parsing step, with the custom: prefix, I'll go ahead and add it for the Microsoft (ole2 and ooxml) and PDF parsers&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/metadata/Property.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/metadata/MSOffice.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 6 May 2011 03:44:44 +0000" id="745" opendate="Fri, 6 May 2011 02:51:45 +0000">
		<buginformation>
			<summary>IWorkPackageParser / IWorkParser not registering properly</summary>
			<description>&lt;p&gt;If you try to use AutoDetectParser to handle an iWork document, it'll fail with:&lt;br/&gt;
 org.xml.sax.SAXParseException; lineNumber: 1; columnNumber: 1; Content is not allowed in prolog.&lt;br/&gt;
	at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.createSAXParseException(ErrorHandlerWrapper.java:198)&lt;/p&gt;

&lt;p&gt;However IWorkPackageParser works fine. It seems the IWorkParser needs just the individual zip part, but is registered as the handler for the individual mime types, so breaks.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/detect/ZipContainerDetector.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/detect/XmlRootExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 6 May 2011 05:15:38 +0000" id="746" opendate="Fri, 6 May 2011 04:41:04 +0000">
		<buginformation>
			<summary>Outlook dates using the wrong metadata key</summary>
			<description>&lt;p&gt;Currently, the Outlook extractor fetches the &quot;Accepted By Mail Server&quot; date from POI, and then saves this into Metadata.EDIT_TIME and Metadata.LAST_SAVED, neither of which look right, and neither of which are date properties.&lt;/p&gt;

&lt;p&gt;The rfc822 parser uses Metadata.CREATION_DATE, which is a Date property. The mbox parser uses Metadata.DATE, another (but different) Date property&lt;/p&gt;

&lt;p&gt;All three should probably use the same. I'd suggest that for now, they all output the same value to both CREATION_DATE and DATE&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/SummaryExtractor.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/mbox/MboxParser.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/mail/MailContentHandler.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/metadata/MSOffice.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 14 Oct 2011 13:37:42 +0000" id="747" opendate="Sat, 7 May 2011 22:45:04 +0000">
		<buginformation>
			<summary>Email parser gets into trouble on malformed html in enron corpus</summary>
			<description>&lt;p&gt;There is a very large corpus of email addresses available: &lt;a href=&quot;http://www.cs.cmu.edu/~enron/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.cs.cmu.edu/~enron/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In processing even a subset of this corpus, I see numerous 'unexpected RuntimeException' errors resulting from tagsoup throwing on truly awful html. It seems to me that being able to do something with this entire stack would make a good '1.0' criteria for tika's email parser.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/mail/RFC822Parser.java</file>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/mail/RFC822ParserTest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 10 May 2011 14:26:44 +0000" id="748" opendate="Mon, 9 May 2011 08:09:24 +0000">
		<buginformation>
			<summary>Support pcap file format</summary>
			<description>&lt;p&gt;Currently the Mime checker does not recognize the pcap file format. As I need the recognition for our project, I added it to the tike-mimetypes.xml:&lt;/p&gt;

&lt;p&gt;  &amp;lt;!-- MIME Type for pcap file format, see &lt;a href=&quot;http://www.iana.org/assignments/media-types/application/vnd.tcpdump.pcap&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.iana.org/assignments/media-types/application/vnd.tcpdump.pcap&lt;/a&gt; --&amp;gt;&lt;br/&gt;
  &amp;lt;mime-type type=&quot;application/vnd.tcpdump.pcap&quot;&amp;gt;&lt;br/&gt;
    &amp;lt;magic priority=&quot;50&quot;&amp;gt;&lt;br/&gt;
      &amp;lt;match value=&quot;0xa1b2c3d4&quot; type=&quot;big32&quot; offset=&quot;0&quot; /&amp;gt;&lt;br/&gt;
      &amp;lt;match value=&quot;0xd4c3b2a1&quot; type=&quot;big32&quot; offset=&quot;0&quot; /&amp;gt;&lt;br/&gt;
    &amp;lt;/magic&amp;gt;&lt;br/&gt;
  	&amp;lt;glob pattern=&quot;*.pcap&quot;/&amp;gt;&lt;br/&gt;
  	&amp;lt;glob pattern=&quot;*.cap&quot;/&amp;gt;&lt;br/&gt;
  	&amp;lt;glob pattern=&quot;*.dmp&quot;/&amp;gt;&lt;br/&gt;
  &amp;lt;/mime-type&amp;gt;&lt;/p&gt;

&lt;p&gt;*.cap is already used in image/x-raw-phaseone, so I had to comment it out to pass the tests, but I'm sure there is a better way to deal with the conflict, just did not have the time to look into it.&lt;/p&gt;

&lt;p&gt;With the configuration above pcap recognition works just fine for me, so I just wanted to contribute this, in case anyone finds it useful.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 15 May 2011 20:57:57 +0000" id="749" opendate="Sun, 15 May 2011 20:46:50 +0000">
		<buginformation>
			<summary>ODF tests are for the wrong package</summary>
			<description>&lt;p&gt;The ODF related tests still call the deprecated parser, not the new one, and are in the wrong package&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/odf/ODFParserTest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 18 May 2011 16:03:16 +0000" id="750" opendate="Tue, 17 May 2011 16:45:11 +0000">
		<buginformation>
			<summary>Remove logging of duplicate parser definitions</summary>
			<description>&lt;p&gt;In &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-620&quot; title=&quot;Parsers and non-canonical mimetypes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-620&quot;&gt;&lt;del&gt;TIKA-620&lt;/del&gt;&lt;/a&gt; (revision 1085003) we added logging for cases where more than one parser is defined for the same media type. This is good for catching issues like classpath conflicts, but not for cases like in Jackrabbit where we use configuration &lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; to explicitly override the parsers of certain media types.&lt;/p&gt;

&lt;p&gt;Because of this issue and the general preference of no logging in tika-core I'd like to remove this logging functionality. If it's needed in some use cases, we could instead create a utility tool that goes through a list of parsers and finds all the duplicates. Deployments that want to avoid duplicates could then use this tool to flag them.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; &lt;a href=&quot;http://svn.apache.org/repos/asf/jackrabbit/branches/2.2/jackrabbit-core/src/main/resources/org/apache/jackrabbit/core/query/lucene/tika-config.xml&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://svn.apache.org/repos/asf/jackrabbit/branches/2.2/jackrabbit-core/src/main/resources/org/apache/jackrabbit/core/query/lucene/tika-config.xml&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/test/java/org/apache/tika/mime/PatternsTest.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/parser/CompositeParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 18 May 2011 18:06:06 +0000" id="751" opendate="Wed, 18 May 2011 11:49:12 +0000">
		<buginformation>
			<summary>MimeType class does contain a String with accessor named Extension. This should be a List&lt;String&gt; Extensions due to several reasons.</summary>
			<description>&lt;p&gt;The javadoc for the method suggest that it will return the &quot;preferred extension&quot; for a MimeType. Acutally its the first Extension deserialized from tika-mimetypes.xml. The method would have to be renamed to getFirstExtensionInList(). This pretty much points at the fact that this method makes no sense at all. &lt;br/&gt;
There are several use-cases in which one would want to know ALL known valid extensions for one mime type. I will submit a patch later today.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/mime/MimeType.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 19 May 2011 07:52:16 +0000" id="752" opendate="Wed, 18 May 2011 13:57:33 +0000">
		<buginformation>
			<summary>Prevent creating of ZipInputStreamZipEntrySource when reading files from disk</summary>
			<description>&lt;p&gt;POI provides two ways to open OPCPackage - via InputStream and via File. Creating OPCPackage from InputStream casuses creation of ZipInputStreamZipEntrySource, that buffers all uncompressed data in memory. This takes a lot of memory and it is not needed when we are reading files from disk or when we already copied stream into temporary file.&lt;/p&gt;

&lt;p&gt;This patch removes usage of ZipInputStreamZipEntrySource in this case.&lt;/p&gt;

&lt;p&gt;Unfortunately, it breaks ZIP-bomb prevention for OOXML parser (and other parsers that uses TikaInputStream.getFile()). I think that ZIP-bomb prevention should be additionally implemented for that formats before committing this to SVN.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/OOXMLExtractorFactory.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 1 Mar 2015 22:19:46 +0000" id="753" opendate="Wed, 18 May 2011 16:04:01 +0000">
		<buginformation>
			<summary>JSP files data extraction failed</summary>
			<description>&lt;p&gt;We have worked with tika extraction. In 0.8 jsp file contents extracted well.. But in 0.9 the same files are not extracted well. Pls give the solution&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 20 Jul 2011 14:35:35 +0000" id="754" opendate="Fri, 20 May 2011 14:22:09 +0000">
		<buginformation>
			<summary>Add some more Adobe file formats to the mimetype list</summary>
			<description>&lt;p&gt;Several Adobe formats are missing from the mimetypes file, such as Premiere (PPJ) and SoundBooth (ASND).&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 23 May 2011 16:28:27 +0000" id="755" opendate="Sun, 22 May 2011 13:21:47 +0000">
		<buginformation>
			<summary>NullPointerException from com.sun.org.apache.xml.internal.serializer.ToStream.writeAttrString on some excel files from the CLI</summary>
			<description>&lt;p&gt;I've discovered that a small number of excel files (and possibly others, though I haven't noticed any) will cause com.sun.org.apache.xml.internal.serializer.ToStream.writeAttrString to blow up with a NPE. The text being passed through from the Excel parser looks fine though.&lt;/p&gt;

&lt;p&gt;The full stacktrace when run from the CLI is:&lt;br/&gt;
Exception in thread &quot;main&quot; org.apache.tika.exception.TikaException: Unexpected RuntimeException from org.apache.tika.parser.microsoft.OfficeParser@bf7916&lt;br/&gt;
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:244)&lt;br/&gt;
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)&lt;br/&gt;
	at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:129)&lt;br/&gt;
	at org.apache.tika.cli.TikaCLI$OutputType.process(TikaCLI.java:126)&lt;br/&gt;
	at org.apache.tika.cli.TikaCLI.process(TikaCLI.java:340)&lt;br/&gt;
	at org.apache.tika.cli.TikaCLI.main(TikaCLI.java:97)&lt;br/&gt;
Caused by: java.lang.NullPointerException&lt;br/&gt;
	at com.sun.org.apache.xml.internal.serializer.ToStream.writeAttrString(ToStream.java:1966)&lt;br/&gt;
	at com.sun.org.apache.xml.internal.serializer.ToStream.processAttributes(ToStream.java:1946)&lt;br/&gt;
	at com.sun.org.apache.xml.internal.serializer.ToStream.closeStartTag(ToStream.java:2429)&lt;br/&gt;
	at com.sun.org.apache.xml.internal.serializer.ToStream.characters(ToStream.java:1381)&lt;br/&gt;
	at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerHandlerImpl.characters(TransformerHandlerImpl.java:172)&lt;br/&gt;
	at org.apache.tika.sax.ContentHandlerDecorator.characters(ContentHandlerDecorator.java:146)&lt;br/&gt;
	at org.apache.tika.sax.SecureContentHandler.characters(SecureContentHandler.java:167)&lt;br/&gt;
	at org.apache.tika.sax.ContentHandlerDecorator.characters(ContentHandlerDecorator.java:146)&lt;br/&gt;
	at org.apache.tika.sax.ContentHandlerDecorator.characters(ContentHandlerDecorator.java:146)&lt;br/&gt;
	at org.apache.tika.sax.ContentHandlerDecorator.characters(ContentHandlerDecorator.java:146)&lt;br/&gt;
	at org.apache.tika.sax.SafeContentHandler.access$001(SafeContentHandler.java:39)&lt;br/&gt;
	at org.apache.tika.sax.SafeContentHandler$1.write(SafeContentHandler.java:61)&lt;br/&gt;
	at org.apache.tika.sax.SafeContentHandler.filter(SafeContentHandler.java:113)&lt;br/&gt;
	at org.apache.tika.sax.SafeContentHandler.characters(SafeContentHandler.java:151)&lt;br/&gt;
	at org.apache.tika.sax.XHTMLContentHandler.characters(XHTMLContentHandler.java:261)&lt;br/&gt;
	at org.apache.tika.sax.XHTMLContentHandler.characters(XHTMLContentHandler.java:287)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.TextCell.render(TextCell.java:35)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.CellDecorator.render(CellDecorator.java:34)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.LinkedCell.render(LinkedCell.java:36)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.ExcelExtractor$TikaHSSFListener.processExtraText(ExcelExtractor.java:423)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.ExcelExtractor$TikaHSSFListener.processSheet(ExcelExtractor.java:522)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.ExcelExtractor$TikaHSSFListener.internalProcessRecord(ExcelExtractor.java:346)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.ExcelExtractor$TikaHSSFListener.processRecord(ExcelExtractor.java:297)&lt;br/&gt;
	at org.apache.poi.hssf.eventusermodel.FormatTrackingHSSFListener.processRecord(FormatTrackingHSSFListener.java:82)&lt;br/&gt;
	at org.apache.poi.hssf.eventusermodel.HSSFRequest.processRecord(HSSFRequest.java:112)&lt;br/&gt;
	at org.apache.poi.hssf.eventusermodel.HSSFEventFactory.genericProcessEvents(HSSFEventFactory.java:147)&lt;br/&gt;
	at org.apache.poi.hssf.eventusermodel.HSSFEventFactory.processEvents(HSSFEventFactory.java:106)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.ExcelExtractor$TikaHSSFListener.processFile(ExcelExtractor.java:276)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.ExcelExtractor.parse(ExcelExtractor.java:136)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.OfficeParser.parse(OfficeParser.java:206)&lt;br/&gt;
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)&lt;br/&gt;
	... 5 more&lt;/p&gt;

&lt;p&gt;Looking at the excel parser code, it seems that we're not doing anything wrong, so I think the issue is with the SAX stuff used by the CLI&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ExcelExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 2 Jun 2011 11:55:09 +0000" id="756" opendate="Thu, 2 Jun 2011 11:52:46 +0000">
		<buginformation>
			<summary>Better handling of XML parse errors</summary>
			<description>&lt;p&gt;Currently an XML parse error will cause an untagged SAXException to be thrown and then reported as a illegal (a SAXException not thrown by the given ContentHandler) by the CompositeParser class.&lt;/p&gt;

&lt;p&gt;To better handle this, the XMLParser class should explicitly capture any such SAXExceptions and wrap them into TikaExceptions as described in the Parser.parse() contract.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/xml/XMLParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 30 Aug 2014 20:02:51 +0000" id="757" opendate="Fri, 3 Jun 2011 14:59:52 +0000">
		<buginformation>
			<summary>Support for FB2 (fiction book document) format</summary>
			<description>&lt;p&gt;Add support for FB2 format (&lt;a href=&quot;https://secure.wikimedia.org/wikipedia/en/wiki/FictionBook&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://secure.wikimedia.org/wikipedia/en/wiki/FictionBook&lt;/a&gt;)&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 15 Jul 2011 14:16:27 +0000" id="758" opendate="Mon, 20 Jun 2011 13:06:46 +0000">
		<buginformation>
			<summary>Unexpected RuntimeException from org.apache.tika.parser.mail.RFC822Parser</summary>
			<description>&lt;p&gt;I get this exception from the tika-app with the command line:&lt;/p&gt;

&lt;p&gt;java -jar tika-app-0.9.jar -t test.txt &lt;br/&gt;
Exception in thread &quot;main&quot; org.apache.tika.exception.TikaException: Unexpected RuntimeException from org.apache.tika.parser.mail.RFC822Parser@558041e0&lt;br/&gt;
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:199)&lt;br/&gt;
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:197)&lt;br/&gt;
	at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:135)&lt;br/&gt;
	at org.apache.tika.cli.TikaCLI$OutputType.process(TikaCLI.java:107)&lt;br/&gt;
	at org.apache.tika.cli.TikaCLI.process(TikaCLI.java:302)&lt;br/&gt;
	at org.apache.tika.cli.TikaCLI.main(TikaCLI.java:91)&lt;br/&gt;
Caused by: java.lang.NullPointerException&lt;br/&gt;
	at org.apache.tika.parser.mail.MailContentHandler.field(MailContentHandler.java:130)&lt;br/&gt;
	at org.apache.james.mime4j.parser.MimeStreamParser.parse(MimeStreamParser.java:121)&lt;br/&gt;
	at org.apache.tika.parser.mail.RFC822Parser.parse(RFC822Parser.java:60)&lt;br/&gt;
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:197)&lt;br/&gt;
	... 5 more&lt;/p&gt;


&lt;p&gt;It seems that what is really just a text file is identified as mail. I guess i'd expect this would fall back to a text parser if the mail processor can't handle it (the CC: can be optionally there).&lt;/p&gt;

&lt;p&gt;The content of test.txt (i'll attach it as well)&lt;br/&gt;
cat test.txt &lt;br/&gt;
From: xyz, abc&lt;br/&gt;
Sent: Monday, May 03, 2010 4:21 PM&lt;br/&gt;
To: abc, def&lt;br/&gt;
Cc: eft,hij; abc@gef.com&lt;br/&gt;
Subject: abcd&lt;/p&gt;

&lt;p&gt;foo:&lt;/p&gt;

&lt;p&gt;bar biz bat&lt;/p&gt;

			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/mail/RFC822ParserTest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 8 Nov 2011 18:17:57 +0000" id="759" opendate="Wed, 22 Jun 2011 16:44:16 +0000">
		<buginformation>
			<summary>Proposal for PRT Parser</summary>
			<description>&lt;p&gt;It would be nice if Tika had support for prt CAD files.&lt;br/&gt;
A preliminary prt text extractor has been created.&lt;br/&gt;
Any assistance further developing this code is appreciated.&lt;/p&gt;


&lt;div class=&quot;code panel&quot; style=&quot;border-style: solid;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeHeader panelHeader&quot; style=&quot;border-bottom-width: 1px;border-bottom-style: solid;&quot;&gt;&lt;b&gt;PRTParser.java&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt; org.apache.tika.parser.prt;

&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; java.io.BufferedInputStream;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; java.io.BufferedReader;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; java.io.IOException;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; java.io.InputStream;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; java.io.InputStreamReader;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; java.io.Reader;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; java.io.UnsupportedEncodingException;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; java.nio.charset.Charset;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; java.util.Collections;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; java.util.Set;

&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.poi.util.IOUtils;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.tika.exception.TikaException;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.tika.metadata.Metadata;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.tika.mime.MediaType;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.tika.parser.ParseContext;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.tika.parser.Parser;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.tika.sax.XHTMLContentHandler;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.xml.sax.ContentHandler;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.xml.sax.SAXException;

/**
 * Description: PRT (CAD Drawing) parser. This is a very basic parser.   
 * Searches &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; specific &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt; prefix, and outputs text from note entities.
 * Does not support special characters.
 */
 

&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;PRTParser &lt;span class=&quot;code-keyword&quot;&gt;implements&lt;/span&gt; Parser {

    &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; Set&amp;lt;MediaType&amp;gt; SUPPORTED_TYPES = Collections.singleton(MediaType.application(&lt;span class=&quot;code-quote&quot;&gt;&quot;prt&quot;&lt;/span&gt;));
    &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; PRT_MIME_TYPE = &lt;span class=&quot;code-quote&quot;&gt;&quot;application/prt&quot;&lt;/span&gt;;
        	
    &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; Set&amp;lt;MediaType&amp;gt; getSupportedTypes(ParseContext context) {
        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; SUPPORTED_TYPES;
        }
		
    &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void parse(
		InputStream stream, ContentHandler handler,
		Metadata metadata, ParseContext context)
		&lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException, SAXException, TikaException {
		XHTMLContentHandler xhtml = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; XHTMLContentHandler(handler, metadata);
		
		&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;[] prefix = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;[] {227, 63};  				&lt;span class=&quot;code-comment&quot;&gt;//Looking &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; a prefix set of bytes {E3, 3F} 
&lt;/span&gt;		&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; pos = 0;										&lt;span class=&quot;code-comment&quot;&gt;//position inside the prefix
&lt;/span&gt;		&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; read;
		&lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt;( (read = stream.read()) &amp;gt; -1) {					&lt;span class=&quot;code-comment&quot;&gt;// stream.read() moves to the next &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt;, and returns an integer value of the &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt;.  a value of -1 signals the EOF
&lt;/span&gt;			&lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt;(read == prefix[pos]) {								&lt;span class=&quot;code-comment&quot;&gt;// is the last &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt; read the same as the first &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt; in the prefix?
&lt;/span&gt;				pos++;													
					&lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt;(pos == prefix.length) {								&lt;span class=&quot;code-comment&quot;&gt;//Are we at the last position of the prefix?
&lt;/span&gt;						stream.skip(11);										&lt;span class=&quot;code-comment&quot;&gt;// skip the 11 bytes of the prefix which can vary.
&lt;/span&gt;						&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; lengthbyte = stream.read();							&lt;span class=&quot;code-comment&quot;&gt;// Set the next &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt; equal to the length of text in the user input field, see PRT schema
&lt;/span&gt;						stream.skip(1);											
						&lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt;[] text = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt;[lengthbyte];						&lt;span class=&quot;code-comment&quot;&gt;// a &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt; array called text is created.  It should contain an array of integer values of the user inputted text.
&lt;/span&gt;						IOUtils.readFully(stream, text);						
						&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; str = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;(text, 0, text.length, &lt;span class=&quot;code-quote&quot;&gt;&quot;Cp437&quot;&lt;/span&gt;);	&lt;span class=&quot;code-comment&quot;&gt;// Cp437 turn it into a string, but does not remove &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; termination, assumes it's found to be MS-DOS Encoding
&lt;/span&gt;						str = str.replace(&lt;span class=&quot;code-quote&quot;&gt;&quot;\u03C6&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;\u00D8&quot;&lt;/span&gt;);					&lt;span class=&quot;code-comment&quot;&gt;// Note: Substitute CP437's lowercase &lt;span class=&quot;code-quote&quot;&gt;&quot;phi&quot;&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; Nordic &lt;span class=&quot;code-quote&quot;&gt;&quot;O with slash&quot;&lt;/span&gt; to represent diameter symbol. 
&lt;/span&gt;						metadata.add(&lt;span class=&quot;code-quote&quot;&gt;&quot;Content&quot;&lt;/span&gt;,str);
						xhtml.startElement(&lt;span class=&quot;code-quote&quot;&gt;&quot;p&quot;&lt;/span&gt;);	
						xhtml.characters(str);
						xhtml.endElement(&lt;span class=&quot;code-quote&quot;&gt;&quot;p&quot;&lt;/span&gt;);
						pos = 0; 							
					}
			} 
			&lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
				&lt;span class=&quot;code-comment&quot;&gt;//Did not find the prefix. Reset the position counter.
&lt;/span&gt;				pos = 0;
			}
		}
	&lt;span class=&quot;code-comment&quot;&gt;//Reached the end of file
&lt;/span&gt;	&lt;span class=&quot;code-comment&quot;&gt;//&lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.out.println(&lt;span class=&quot;code-quote&quot;&gt;&quot;Finished searching the file&quot;&lt;/span&gt;);	
&lt;/span&gt;	}


		
	/**
    * @deprecated This method will be removed in Apache Tika 1.0.
    */
    &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void parse(
                   InputStream stream, ContentHandler handler, Metadata metadata)
                   &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; IOException, SAXException, TikaException {
                parse(stream, handler, metadata, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ParseContext());
    }
}&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;   






			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/prt/PRTParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 15 Sep 2011 16:43:04 +0000" id="760" opendate="Fri, 15 Jul 2011 14:22:34 +0000">
		<buginformation>
			<summary>RTF Parser issues with non european characters</summary>
			<description>&lt;p&gt;As reported on user@ in &quot;non-West European languages support&quot;:&lt;br/&gt;
  &lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/tika-user/201107.mbox/%3COF0C0A3275.DA7810E9-ONC22578CC.0051EEDE-C22578CC.0052548B@il.ibm.com%3E&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://mail-archives.apache.org/mod_mbox/tika-user/201107.mbox/%3COF0C0A3275.DA7810E9-ONC22578CC.0051EEDE-C22578CC.0052548B@il.ibm.com%3E&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The RTF Parser seems to be doubling up some non-european characters&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/TestParsers.java</file>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/rtf/RTFParserTest.java</file>
			<file>/tika-core/src/test/java/org/apache/tika/TikaDetectionTest.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/rtf/RTFParser.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 17 Sep 2011 11:51:48 +0000" id="761" opendate="Tue, 9 Aug 2011 18:56:10 +0000">
		<buginformation>
			<summary>Enhance content-type detector to recognize almost plain text</summary>
			<description>&lt;p&gt;I am using TIKA to convert a collection of documents that includes files named something.txt.  I use the Tika#parse(InputStream) interface to get a parser that auto detects content.  The files are almost plain text &amp;#8211; the documents have a scattering of control characters in them.  On these text files the reader given to me by the Tika#parse() method immediately returns null.  After some experimentation I found that a single control K character early in the file will cause the mime type detector to give up and label it application/octet-stream.  Please consider adding a recognizer because it would be great if Tika could clean up these files by dropping text characters.  I note that if I drop this file into the Tika GUI, or if I invoke Tika on the command line it does well, and I think this behavior is obtained by using the file name as a hint.  I probably should be using a different Tika method, trying to figure that out next.  Thanks for listening.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/detect/TextDetector.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 17 Sep 2011 09:23:19 +0000" id="762" opendate="Sat, 20 Aug 2011 15:37:33 +0000">
		<buginformation>
			<summary>TikaCLI -x or -h on a Word doc sometimes adds newline after &lt;/b&gt; tag</summary>
			<description>
&lt;p&gt;[Note: spinoff from the tika-dev thread &quot;Issue in text extraction in&lt;br/&gt;
Solr / Tika&quot; on Aug 19 2011, by nirnaydewan]&lt;/p&gt;

&lt;p&gt;When parsing a Word doc where some contiguous text is bolded, due to&lt;br/&gt;
differences in how the user had bolded different parts of the text&lt;br/&gt;
with Word, TikaCLI -x or -h will sometimes generate output like this:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;&amp;lt;p&amp;gt;F&amp;lt;b&amp;gt;oob&amp;lt;/b&amp;gt;a&amp;lt;b&amp;gt;r&amp;lt;/b&amp;gt;
&amp;lt;/p&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and other times like this (extra newline &amp;amp; 2 adjacent bold sections):&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;&amp;lt;p&amp;gt;F&amp;lt;b&amp;gt;oo&amp;lt;/b&amp;gt;
&amp;lt;b&amp;gt;b&amp;lt;/b&amp;gt;a&amp;lt;b&amp;gt;r&amp;lt;/b&amp;gt;
&amp;lt;/p&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The extra newline in the second example causes browsers (I tried&lt;br/&gt;
Firefox, Safari, Chrome), JTidy and Tika itself to (incorrectly)&lt;br/&gt;
insert a space when rending/extracting text, breaking up the word.&lt;/p&gt;

&lt;p&gt;While this might be technically correct/OK (ie, XML white space rules&lt;br/&gt;
might allow for non-significant space after the &amp;lt;/b&amp;gt; within a &amp;lt;p&amp;gt;&lt;br/&gt;
should be ignored), I think we should still fix Tika to not insert&lt;br/&gt;
newlines, if we can.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/WordExtractor.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/sax/XHTMLContentHandler.java</file>
			<file>/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/XWPFWordExtractorDecorator.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 22 Aug 2011 11:43:35 +0000" id="763" opendate="Mon, 22 Aug 2011 11:39:04 +0000">
		<buginformation>
			<summary>Incorrect mime-type for .pptm, .ppsm and .ppsx in OOXMLParser</summary>
			<description>&lt;p&gt;Current parser set mime type &quot;application/vnd.openxmlformats-officedocument.presentationml.presentation&quot; for all PowerPoint XML formats, but pptm, ppsm and ppsx has it own mime types&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/XSLFPowerPointExtractorDecorator.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 2 Sep 2011 17:45:39 +0000" id="764" opendate="Fri, 26 Aug 2011 08:01:47 +0000">
		<buginformation>
			<summary>&quot;Invalid UTF-16 surrogate detected:&quot; parsing PowerPoint 97-2003</summary>
			<description>&lt;p&gt;Exception when parsing this MS PowerPoint file :  &lt;a href=&quot;http://jeanferrette.free.fr/MS8.ppt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://jeanferrette.free.fr/MS8.ppt&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;java.io.IOException: Substitut UTF-16 non valide détecté : db00 bfff ?&lt;br/&gt;
                at com.sun.org.apache.xml.internal.serializer.ToStream.endElement(ToStream.java:2060)&lt;br/&gt;
                at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerHandlerImpl.endElement(TransformerHandlerImpl.java:273)&lt;br/&gt;
                at org.apache.tika.sax.TeeContentHandler.endElement(TeeContentHandler.java:94)&lt;br/&gt;
                at org.apache.tika.sax.ContentHandlerDecorator.endElement(ContentHandlerDecorator.java:136)&lt;br/&gt;
                at org.apache.tika.sax.SecureContentHandler.endElement(SecureContentHandler.java:215)&lt;br/&gt;
                at org.apache.tika.sax.ContentHandlerDecorator.endElement(ContentHandlerDecorator.java:136)&lt;br/&gt;
                at org.apache.tika.sax.ContentHandlerDecorator.endElement(ContentHandlerDecorator.java:136)&lt;br/&gt;
                at org.apache.tika.sax.ContentHandlerDecorator.endElement(ContentHandlerDecorator.java:136)&lt;br/&gt;
                at org.apache.tika.sax.XHTMLContentHandler.lazyEndHead(XHTMLContentHandler.java:169)&lt;br/&gt;
                at org.apache.tika.sax.XHTMLContentHandler.startElement(XHTMLContentHandler.java:234)&lt;br/&gt;
                at org.apache.tika.sax.XHTMLContentHandler.startElement(XHTMLContentHandler.java:271)&lt;br/&gt;
                at org.apache.tika.sax.XHTMLContentHandler.element(XHTMLContentHandler.java:308)&lt;br/&gt;
                at org.apache.tika.parser.microsoft.HSLFExtractor.parse(HSLFExtractor.java:41)&lt;br/&gt;
                at org.apache.tika.parser.microsoft.OfficeParser.parse(OfficeParser.java:201)&lt;br/&gt;
                at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)&lt;br/&gt;
                at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)&lt;br/&gt;
                at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:129)&lt;/p&gt;

&lt;p&gt;                &lt;span class=&quot;error&quot;&gt;&amp;#91;...&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Parsing this file works fine with tika 0.4.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/sax/SafeContentHandler.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 1 Sep 2011 15:31:00 +0000" id="765" opendate="Wed, 31 Aug 2011 13:02:17 +0000">
		<buginformation>
			<summary>Fix problems with TemporaryFiles</summary>
			<description>&lt;p&gt;As discussed on dev@, there are still some design issues with the TemporaryFiles class that we introduced in &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-567&quot; title=&quot;Temporary file leak in TikaInputStream&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-567&quot;&gt;&lt;del&gt;TIKA-567&lt;/del&gt;&lt;/a&gt;. They should be solved in time for Tika 1.0.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/parser/external/ExternalParser.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/Tika.java</file>
			<file>/tika-core/src/test/java/org/apache/tika/language/LanguageProfilerBuilderTest.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/extractor/ParserContainerExtractor.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/io/TikaInputStream.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pkg/ZipContainerDetector.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 2 Sep 2011 16:41:15 +0000" id="766" opendate="Thu, 1 Sep 2011 10:11:19 +0000">
		<buginformation>
			<summary>Cannot compile Tika with Java 7 (ImageMetadataExtractor.java)</summary>
			<description>&lt;p&gt;Spinoff from user thread &quot;Closing streams (Was: Tika leaves files open)&quot; started by Jukka on 8/30/2011 (&lt;a href=&quot;http://markmail.org/message/6iq4tapiwzpmanhw&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://markmail.org/message/6iq4tapiwzpmanhw&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;ImageMetadatExtractor is (indirectly) using a retired Java API, resulting in compilation error under Java 7:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;tika-parsers/src/main/java/org/apache/tika/parser/image/ImageMetadataExtractor.java:[89,34] error: cannot access JPEGDecodeParam
[ERROR]   class file for com.sun.image.codec.jpeg.JPEGDecodeParam not found
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It's possible upgrading to the latest metadata-extractor might fix this... (we use 2.4.0-beta-1 but there's now a 2.5.0-RC2: &lt;a href=&quot;http://code.google.com/p/metadata-extractor&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://code.google.com/p/metadata-extractor&lt;/a&gt;).&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/image/ImageMetadataExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 2 Sep 2011 15:17:32 +0000" id="767" opendate="Thu, 1 Sep 2011 17:34:04 +0000">
		<buginformation>
			<summary>PDF and Outlook docs embedded in MS Word documents not parsed</summary>
			<description>&lt;p&gt;Currently there appear to be issues with embedded pdf's and outlook Msg files contained in MS Word documents. I'll attach a sample for each and my recursive parser (incase the problem lies in there).&lt;/p&gt;

&lt;p&gt;From what I see, when these embedded objects are parsed, they're initially identified as vnd.openxmlformats-officedocument.oleObject in the metadata's Content-Type field. After a call to the RecurciveParsers super parse class the Content-Types update to the following:&lt;/p&gt;

&lt;p&gt;PDF's: application/vnd.ms-works&lt;br/&gt;
.MSG: application/x-tika-msoffice&lt;/p&gt;

&lt;p&gt;The internal AutoDetectParser is unable to properly identify these PDF's and therfore does not call the PDFParser on them.&lt;/p&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/POIFSContainerDetector.java</file>
			<file>/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java</file>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/microsoft/ooxml/OOXMLContainerExtractionTest.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/AbstractOOXMLExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 20 Oct 2011 12:43:19 +0000" id="768" opendate="Mon, 5 Sep 2011 14:12:37 +0000">
		<buginformation>
			<summary>Valid OOXML PPT file hits InvalidFormatException thrown in POI</summary>
			<description>&lt;p&gt;I took the &quot;testRTFVarious.rtf&quot; test case from &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-683&quot; title=&quot;RTF Parser issues with non european characters&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-683&quot;&gt;&lt;del&gt;TIKA-683&lt;/del&gt;&lt;/a&gt;, and saved it as various other doc types, to generate more test cases.&lt;/p&gt;

&lt;p&gt;But when I did this for PPTX, the resulting file hits this exception:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;
Exception in thread &quot;main&quot; org.apache.tika.exception.TikaException: Broken OOXML file
	at org.apache.tika.parser.microsoft.ooxml.AbstractOOXMLExtractor.handleEmbeddedParts(AbstractOOXMLExtractor.java:141)
	at org.apache.tika.parser.microsoft.ooxml.AbstractOOXMLExtractor.getXHTML(AbstractOOXMLExtractor.java:112)
	at org.apache.tika.parser.microsoft.ooxml.OOXMLExtractorFactory.parse(OOXMLExtractorFactory.java:95)
	at org.apache.tika.parser.microsoft.ooxml.OOXMLParser.parse(OOXMLParser.java:70)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:242)
	at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:129)
	at org.apache.tika.cli.TikaCLI$OutputType.process(TikaCLI.java:126)
	at org.apache.tika.cli.TikaCLI.process(TikaCLI.java:363)
	at org.apache.tika.cli.TikaCLI.main(TikaCLI.java:97)
Caused by: org.apache.poi.openxml4j.exceptions.InvalidFormatException: A segment shall not hold any characters other than pchar characters. [M1.6]
	at org.apache.poi.openxml4j.opc.PackagePartName.checkPCharCompliance(PackagePartName.java:370)
	at org.apache.poi.openxml4j.opc.PackagePartName.throwExceptionIfPartNameHaveInvalidSegments(PackagePartName.java:270)
	at org.apache.poi.openxml4j.opc.PackagePartName.throwExceptionIfInvalidPartUri(PackagePartName.java:185)
	at org.apache.poi.openxml4j.opc.PackagePartName.&amp;lt;init&amp;gt;(PackagePartName.java:83)
	at org.apache.poi.openxml4j.opc.PackagingURIHelper.createPartName(PackagingURIHelper.java:490)
	at org.apache.tika.parser.microsoft.ooxml.AbstractOOXMLExtractor.handleEmbeddedParts(AbstractOOXMLExtractor.java:124)
	... 9 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;All I did was open Office 2007, copy/paste over the text from the Word doc, and save it.  Ie, it should be a valid OOXML file, unless Office 2007 is buggy?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/microsoft/ooxml/OOXMLParserTest.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/AbstractOOXMLExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 2 Aug 2012 09:32:51 +0000" id="769" opendate="Wed, 7 Sep 2011 16:02:39 +0000">
		<buginformation>
			<summary>Tika network server does not print anything in response to, for example, Word documents</summary>
			<description>&lt;p&gt;When trying to use Tika Server (java -jar tika-app-0.9.jar -t -p PORT) to parse M$Word DOC/DOCX files, tika server reads the file and then doesn't do anything more, it simply hangs, probably blocked on a socket read. This does not happend with, for example, HTML documents. I don't know the mechanics of this bug, but the following change definitely fixes the issue:&lt;/p&gt;

&lt;p&gt;Change&lt;br/&gt;
type.process(socket.getInputStream(), output);&lt;br/&gt;
to&lt;br/&gt;
type.process(new CloseShieldInputStream(socket.getInputStream()), output);&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/feed/FeedParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 9 Sep 2011 09:17:41 +0000" id="770" opendate="Fri, 9 Sep 2011 08:44:48 +0000">
		<buginformation>
			<summary>Expose the Parser and Detector instances within the Tika facade</summary>
			<description>&lt;p&gt;To simplify the work of users transitioning from the Tika facade to the more powerful underlying Parser and Detector interfaces, it would be useful if the Tika facade class directly implemented these interfaces in addition to providing various utility methods.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/Tika.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 20 Sep 2011 23:03:57 +0000" id="771" opendate="Fri, 16 Sep 2011 15:20:25 +0000">
		<buginformation>
			<summary>Upgrade apache-Mime4J to Version 0.7</summary>
			<description>&lt;p&gt;A newer version of mim4j was released in July 2011. &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://james.apache.org/newsarchive.html#a25072011&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://james.apache.org/newsarchive.html#a25072011&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It appears it has some imporved functionality for handling various malformed RFC822 messages.  Not sure how seemlessly it will intergrate with Tika and other mods.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-bundle/pom.xml</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 1 Oct 2011 16:24:43 +0000" id="772" opendate="Mon, 19 Sep 2011 16:29:07 +0000">
		<buginformation>
			<summary>EBCDIC encoding not detected</summary>
			<description>&lt;p&gt;I have a test file encoded in EBCDIC, but Tika fails to detect it.&lt;/p&gt;

&lt;p&gt;Not sure we can realistically fix this; I have no idea how (and,&lt;br/&gt;
realistically, one really ought to convert out of EBCDIC on export&lt;br/&gt;
from a mainframe...).&lt;/p&gt;

&lt;p&gt;Here's what Tika detects:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Shift_JIS: confidence=51
Big5: confidence=40
GB18030: confidence=10
KOI8-R: confidence=5
windows-1252: confidence=5
windows-1253: confidence=2
IBM866: confidence=1
windows-1251: confidence=1
windows-1250: confidence=1
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The test file decodes fine as cp500; eg in Python just run this:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;import codecs
codecs.getdecoder('cp500')(open('English_EBCDIC.txt').read())
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/txt/CharsetMatch.java</file>
			<file>/tika-parsers/src/test/resources/test-documents/english.cp500.txt</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/txt/CharsetDetector.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 19 Dec 2012 02:59:36 +0000" id="773" opendate="Tue, 20 Sep 2011 09:20:58 +0000">
		<buginformation>
			<summary>Empty title element makes Tika-generated HTML documents not open in Chromium</summary>
			<description>&lt;p&gt;Currently when converting Excel sheets (both XLS and XLSX), Tika generates an empty title element as &amp;lt;title/&amp;gt; into the document HEAD section. This causes Chromium not to display the document contents.&lt;/p&gt;

&lt;p&gt;Switching it to &amp;lt;title&amp;gt;&amp;lt;/title&amp;gt; fixes this.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/sax/ExpandedTitleContentHandler.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/sax/XHTMLContentHandler.java</file>
			<file>/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 21 Sep 2011 09:40:32 +0000" id="774" opendate="Wed, 21 Sep 2011 09:25:03 +0000">
		<buginformation>
			<summary>Provide a way to distinguish generic parse error and parse error due to unknown/wrong decryption key</summary>
			<description>&lt;p&gt;Currently there is no way to distinguish generic parse failure (i.e. error in parser) and situation when extraction can be done due to unknown or wrong password.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/exception/EncryptedDocumentException.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/parser/CryptoParser.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ExcelExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 17 Aug 2015 04:11:48 +0000" id="775" opendate="Wed, 29 Jul 2015 02:03:05 +0000">
		<buginformation>
			<summary>Integrate the GROBID PDF extractor in Tika</summary>
			<description>&lt;p&gt;GROBID (&lt;a href=&quot;http://grobid.readthedocs.org/en/latest/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://grobid.readthedocs.org/en/latest/&lt;/a&gt;) is a machine learning library for extracting, parsing and re-structuring raw documents such as PDF into structured TEI-encoded documents with a particular focus on technical and scientific publications.&lt;br/&gt;
It has a java api which can be used to augment PDF parsing for journals and help extract extra metadata about the paper like authors, publication, citations, etc. &lt;/p&gt;

&lt;p&gt;It would be nice to have this integrated into Tika, I have tried it on my local, will issue a pull request soon.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-bundle/pom.xml</file>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 1 Aug 2015 22:11:36 +0000" id="776" opendate="Thu, 30 Jul 2015 12:54:29 +0000">
		<buginformation>
			<summary>tika-config.xml does not provide a way to set ServiceLoader to dynamic</summary>
			<description>&lt;p&gt;Currently if you create a TikaConfig from a file (ex tika-config.xml).  There is no way to specify that you want to use a ServiceLoader with dynamic set.  Prior to tika 1.7 this was not an issue since the during the tika-config.xml parse Tika would instantiate parsers using the default constructor which in turn would instantiate a new ServiceLoader.  The default ServiceLoader constructor sets dynamic to true which allows dynamic loading of parsers.  &lt;/p&gt;

&lt;p&gt;Changes to TikaConfig now cause the tika-config.xml parse to call a constructor which passes the ServiceLoader to be passed as a parameter.  This ServiceLoader is always constructed with a Classloader which will cause dynamic to always be set to false.  This breaks Tika in OSGi environments which depend on dynamic being set to true (for example Apache Sling). &lt;/p&gt;

&lt;p&gt;I'm proposing adding an xml attribute to the parser element to instantiate the parser with dynamic set to true.  This allows users of tika-config.xml to determine how they want parsers loaded. &lt;/p&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/config/ServiceLoader.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 17 Aug 2015 18:10:13 +0000" id="777" opendate="Fri, 14 Aug 2015 17:00:06 +0000">
		<buginformation>
			<summary>Detectors loaded from configuration files into CompositeDetector fail</summary>
			<description>&lt;p&gt;Loading individual Detectors from a configuration file, e.g.&lt;/p&gt;

&lt;p&gt;  &amp;lt;detector class=&quot;org.apache.tika.parser.microsoft.POIFSContainerDetector&quot;/&amp;gt;&lt;br/&gt;
  &amp;lt;detector class=&quot;org.apache.tika.mime.MimeTypes&quot;/&amp;gt;&lt;/p&gt;

&lt;p&gt;will cause them to be added to a CompositeDetector which does not detect, e.g. PST files.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/config/TikaConfig.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 20 Aug 2015 21:47:10 +0000" id="778" opendate="Sat, 15 Aug 2015 22:35:04 +0000">
		<buginformation>
			<summary>Replace usages of classes in org.apache.tika.io with current alternatives</summary>
			<description>&lt;p&gt;Many of the classes in org.apache.tika.io were inlined from commons-io in &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-249&quot; title=&quot;Inline key commons-io classes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-249&quot;&gt;&lt;del&gt;TIKA-249&lt;/del&gt;&lt;/a&gt;, but these days most components use commons-io anyway, so in order to clean the dependencies on org.apache.tika.io in preparation of adding commons-io to tika-core, the following can be done:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Replace usages of classes in org.apache.tika.io within non-core components with the corresponding classes in commons-io&lt;/li&gt;
	&lt;li&gt;Replace usages of org.apache.tika.io.IOUtils.UTF_8 with java.nio.charset.StandardCharsets.UTF_8 (in all components, including tika-core)&lt;/li&gt;
	&lt;li&gt;Replace other uses of String encoding names of standard charsets with their corresponding Charsets instances from StandardCharsets (this is logically related to IOUtils as these constants should have been there as UTF_8 was before Java 7)&lt;/li&gt;
&lt;/ul&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-batch/pom.xml</file>
			<file>/tika-core/src/main/java/org/apache/tika/config/ServiceLoader.java</file>
			<file>/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java</file>
			<file>/tika-parsers/pom.xml</file>
			<file>/tika-core/src/test/java/org/apache/tika/detect/MagicDetectorTest.java</file>
			<file>/tika-bundle/src/test/java/org/apache/tika/bundle/BundleIT.java</file>
			<file>/tika-parsers/src/test/java/org/apache/tika/embedder/ExternalEmbedderTest.java</file>
			<file>/tika-translate/src/main/java/org/apache/tika/language/translate/GoogleTranslator.java</file>
			<file>/tika-server/src/main/java/org/apache/tika/server/HTMLHelper.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/audio/MidiParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 20 Aug 2015 17:08:36 +0000" id="779" opendate="Sun, 16 Aug 2015 09:01:25 +0000">
		<buginformation>
			<summary>Remove java6-activated profile from tika-bundle and move its plugins to default build</summary>
			<description>&lt;p&gt;Since the project now requires Java 7, there's no point in allowing Java 6+ since the build would fail.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-bundle/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 19 Aug 2015 15:58:20 +0000" id="780" opendate="Tue, 18 Aug 2015 06:55:41 +0000">
		<buginformation>
			<summary>Consider making default host for Tika Server 0.0.0.0 instead of localhost</summary>
			<description>&lt;p&gt;I noticed in Tika-Python on Windows while fixing some bugs that by default Tika Server binds to localhost which means that the Tika Server running on Windows isn't available to external hosts trying to access it on &amp;lt;host name&amp;gt;:9998. I think the default behavior is that it &lt;b&gt;should&lt;/b&gt; be available externally, meaning, we should probably bind to the special address, 0.0.0,0 which binds to all interfaces. &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-server/src/main/java/org/apache/tika/server/TikaServerCli.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 28 Aug 2015 13:00:06 +0000" id="781" opendate="Tue, 18 Aug 2015 16:15:54 +0000">
		<buginformation>
			<summary>Tika Server's recursive JSON output from /rmeta different than tika-app -J output</summary>
			<description>&lt;p&gt;Over in Tika Python, we've received a request for exposing the XHTML output that Tika provides. I noticed that in &lt;a href=&quot;http://wiki.apache.org/tika/TikaJAXRS&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;TikaJAXRS&lt;/a&gt; that the JSON output from /rmeta which Tika Python uses is different from tika-app's -J command. For example, see &lt;a href=&quot;http://wiki.apache.org/tika/GrobidJournalParser&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;GrobidJournalParser&lt;/a&gt;. I'm not sure they should be different. Maybe they should. But it would be nice to at least provide maybe X:TIKA:XHTMLContent or something like that in /rmeta the same way that Tika-app -J provides.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 20 Aug 2015 21:15:17 +0000" id="782" opendate="Thu, 20 Aug 2015 18:07:07 +0000">
		<buginformation>
			<summary>Upgrade to Commons Compress 1.10</summary>
			<description>&lt;p&gt;Apache Commons Compress 1.10 is out, we should upgrade from our current version of 1.9&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-batch/pom.xml</file>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 31 Aug 2015 04:30:02 +0000" id="783" opendate="Thu, 20 Aug 2015 19:48:57 +0000">
		<buginformation>
			<summary>Utilize try-with-resources where it is trivial</summary>
			<description>&lt;p&gt;The following type of resource usages:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    AutoCloseable resource = ...;
    &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
        &lt;span class=&quot;code-comment&quot;&gt;// &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; something with resource
&lt;/span&gt;    } &lt;span class=&quot;code-keyword&quot;&gt;finally&lt;/span&gt; {
        resource.close();
    }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    AutoCloseable resource = &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;;
    &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
        resource = ...;
        &lt;span class=&quot;code-comment&quot;&gt;// &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; something with resource
&lt;/span&gt;    } &lt;span class=&quot;code-keyword&quot;&gt;finally&lt;/span&gt; {
        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (resource != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
            resource.close();
        }
    }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and similar constructs can be trivially replaced with Java 7's try-with-resource statement:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; (AutoCloseable resource = ...) {
        &lt;span class=&quot;code-comment&quot;&gt;// &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; something with resource
&lt;/span&gt;    }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This brings more concise code with less chance of causing resource leaks.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 27 Aug 2015 13:29:37 +0000" id="784" opendate="Wed, 26 Aug 2015 10:11:54 +0000">
		<buginformation>
			<summary>Collect multiple exceptions in TemporaryResources.close() using Throwable.addSuppressed()</summary>
			<description>&lt;p&gt;TemporaryResource.close() currently collects exceptions throw by trying to close its resources in a list.&lt;br/&gt;
When the time to propagate an exception comes, information is lost - the thrown exception contains a message with the string descriptions of all exceptions, and the first exception as the cause - there is no stack trace describing what went wrong closing a resource.&lt;br/&gt;
In addition, the thrown exception is IOExceptionWithCause, copied from commons-io, which is redundant since Java 6.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/io/TemporaryResources.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 27 Aug 2015 09:32:17 +0000" id="785" opendate="Wed, 26 Aug 2015 10:14:24 +0000">
		<buginformation>
			<summary>Replace IOExceptionWithCause in ForkClient</summary>
			<description>&lt;p&gt;IOExceptionWithCause (copied from commons-io) is redundant since Java 6.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/fork/ForkClient.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 27 Aug 2015 09:25:07 +0000" id="786" opendate="Thu, 27 Aug 2015 00:16:46 +0000">
		<buginformation>
			<summary>Tika methods that accept a File needlessly convert it to a URL</summary>
			<description>&lt;p&gt;The following methods:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Tika.detect(File)&lt;/li&gt;
	&lt;li&gt;Tika.parse(File)&lt;/li&gt;
	&lt;li&gt;Tika.parseToString(File)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Convert the given File to a URL and use the corresponding overloaded method that accepts a URL.&lt;br/&gt;
This seems like a shortcut, but essentially does the following:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Converts the file to a URI&lt;/li&gt;
	&lt;li&gt;Converts the URI to a URL&lt;/li&gt;
	&lt;li&gt;Calls TikaInputStream.get(URL, Metadata), which then performs the following special handling:&lt;/li&gt;
	&lt;li&gt;Checks if the protocol is &quot;file&quot;&lt;/li&gt;
	&lt;li&gt;Tries to convert the URL (back) to a URI&lt;/li&gt;
	&lt;li&gt;Creates a File around the URI&lt;/li&gt;
	&lt;li&gt;Checks if file.isFile()&lt;/li&gt;
	&lt;li&gt;Calls TikaInputStream.get(File, Metadata)&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;The special handling in TikaInputStream.get(URL/URI) is a good optimization for in-the-wild file resources, but for internal uses it can be skipped - making Tika call TikaInputStream.get(File, Metadata) directly.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/Tika.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 19 Oct 2015 05:36:44 +0000" id="787" opendate="Thu, 3 Sep 2015 08:55:09 +0000">
		<buginformation>
			<summary>Detection is not working properly for detecting HWP 5.0 file</summary>
			<description>&lt;p&gt;HWP file has two formats which are HWP 3.0 and HWP 5.0.&lt;br/&gt;
'tika-app-1.10.jar' detects HWP 3.0 format's file correctly.&lt;br/&gt;
But, not for HWP 5.0.&lt;br/&gt;
Used commands and returned results are addresses below.&lt;/p&gt;

&lt;p&gt;&amp;gt; java -jar tika-app-1.10.jar --detect test_3.0.hwp&lt;br/&gt;
&amp;gt; application/x-hwp&lt;/p&gt;

&lt;p&gt;&amp;gt; java -jar tika-app-1.10.jar --detect test_5.0.hwp&lt;br/&gt;
&amp;gt; application/x-tika-msoffice&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 22 Sep 2015 13:32:38 +0000" id="788" opendate="Wed, 16 Sep 2015 09:06:08 +0000">
		<buginformation>
			<summary>Use java.nio.file.Path in TemporaryResources</summary>
			<description>&lt;p&gt;This will provide support for the new api for uses who need it, and provide better information in I/O operations, e.g. detailed exception if temporary file deletion fails.&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;used Path and methods in java.nio.file.Files internally&lt;/li&gt;
	&lt;li&gt;add setTemporaryFileDirectory(Path) method&lt;/li&gt;
	&lt;li&gt;add createTempFile() method (mimicking Files.createTempFile)&lt;/li&gt;
	&lt;li&gt;add unit test for proper deletion of temp files&lt;/li&gt;
&lt;/ul&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/io/TemporaryResources.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 8 Oct 2015 23:34:44 +0000" id="789" opendate="Fri, 18 Sep 2015 11:07:19 +0000">
		<buginformation>
			<summary>Bouncy Castle version binary incompatibility</summary>
			<description>&lt;p&gt;One file in our Common Crawl stash demonstrates a Bouncy Castle version conflict...incompatible binaries with Jackcess and our current version of Bouncy Castle.&lt;/p&gt;

&lt;p&gt;java.lang.NoSuchMethodError: org.bouncycastle.crypto.StreamCipher.processBytes([BII[BI)V&lt;br/&gt;
 at com.healthmarketscience.jackcess.impl.BaseCryptCodecHandler.streamDecrypt(BaseCryptCodecHandler.java:91)&lt;br/&gt;
 at com.healthmarketscience.jackcess.impl.BaseJetCryptCodecHandler.decodePage(BaseJetCryptCodecHandler.java:62)&lt;br/&gt;
 at com.healthmarketscience.jackcess.impl.PageChannel.readPage(PageChannel.java:224)&lt;br/&gt;
 at com.healthmarketscience.jackcess.impl.UsageMap.read(UsageMap.java:130)&lt;br/&gt;
 at com.healthmarketscience.jackcess.impl.PageChannel.initialize(PageChannel.java:117)&lt;br/&gt;
 at com.healthmarketscience.jackcess.impl.DatabaseImpl.&amp;lt;init&amp;gt;(DatabaseImpl.java:516)&lt;br/&gt;
 at com.healthmarketscience.jackcess.impl.DatabaseImpl.open(DatabaseImpl.java:389)&lt;br/&gt;
 at com.healthmarketscience.jackcess.DatabaseBuilder.open(DatabaseBuilder.java:248)&lt;br/&gt;
 at TestIt.testIt(TestIt.java:19)&lt;/p&gt;

&lt;p&gt;A full description and test file are attached &lt;a href=&quot;https://sourceforge.net/p/jackcessencrypt/feature-requests/2/#b65d&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There was an API change in 1.51 that causes this problem.  1.50 works with the one test file, and 1.51 does not work.  We're currently using 1.52.&lt;/p&gt;

&lt;p&gt;It looks like POI is using 1.51 in trunk, now. According to PDFBox trunk's build.xml, they're using 1.50, but their pom.xml has 1.51.&lt;/p&gt;

&lt;p&gt;Two options that I see:&lt;br/&gt;
1) close our eyes and hope it doesn't affect too many people before Jackcess Encrypt upgrades... perhaps add a try/catch for this one version conflict?  Is there any shade magic we can do on our end ... or (I'm assuming) would that have to be done by Jackcess (or an upgrade, of course)?&lt;br/&gt;
2) downgrade our bc-prov to 1.50 (from 1.52).&lt;/p&gt;

&lt;p&gt;Other options?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 7 Oct 2015 19:20:12 +0000" id="790" opendate="Tue, 22 Sep 2015 17:12:33 +0000">
		<buginformation>
			<summary>Include CTAKESConfig.properties within tika-parsers resources by default</summary>
			<description>&lt;p&gt;It is a PITA to have to grab the example CTAKESConfig.properties file graciously provided by &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gostep&quot; class=&quot;user-hover&quot; rel=&quot;gostep&quot;&gt;Giuseppe Totaro&lt;/a&gt; as the &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1645&quot; title=&quot;Extraction of biomedical information using CTAKESParser&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1645&quot;&gt;&lt;del&gt;TIKA-1645&lt;/del&gt;&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12737116/CTAKESConfig.properties&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;attachment&lt;/a&gt;.&lt;br/&gt;
I propose to have it added to &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;tika-parsers/src/main/resources/.../ctakes/CTAKESConfig.properties&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 30 Sep 2015 17:05:25 +0000" id="791" opendate="Tue, 22 Sep 2015 21:19:05 +0000">
		<buginformation>
			<summary>StackOverflowError parsing a PDF with ExtractInlineImages=true</summary>
			<description>&lt;p&gt;Here's the file:&lt;br/&gt;
&lt;a href=&quot;http://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Code to repro (&lt;tt&gt;ExtractInlineImages&lt;/tt&gt; must be true):&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;    Parser parser = new PDFParser();
    Metadata metadata = new Metadata();
    ParseContext context = new ParseContext();
    PDFParserConfig config = new PDFParserConfig();
    ContentHandler handler = new DefaultHandler();

    config.setExtractInlineImages(true);
    config.setExtractUniqueInlineImagesOnly(false);

    context.set(PDFParserConfig.class, config);
    context.set(Parser.class, parser);

    InputStream is = new BufferedInputStream(new FileInputStream(args[0]));
    try {
      parser.parse(is, handler, metadata, context);
    } finally {
      is.close();
    }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Error (infinite recursion in &lt;tt&gt;extractImages&lt;/tt&gt;):&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Exception in thread &quot;main&quot; java.lang.StackOverflowError
	at java.util.LinkedHashMap$Entry.addBefore(LinkedHashMap.java:340)
	at java.util.LinkedHashMap$Entry.access$600(LinkedHashMap.java:320)
	at java.util.LinkedHashMap.createEntry(LinkedHashMap.java:444)
	at java.util.HashMap.addEntry(HashMap.java:888)
	at java.util.LinkedHashMap.addEntry(LinkedHashMap.java:427)
	at java.util.HashMap.put(HashMap.java:509)
	at org.apache.pdfbox.cos.COSDictionary.setItem(COSDictionary.java:246)
	at org.apache.pdfbox.pdmodel.common.COSDictionaryMap.convert(COSDictionaryMap.java:206)
	at org.apache.pdfbox.pdmodel.PDResources.setXObjects(PDResources.java:331)
	at org.apache.pdfbox.pdmodel.PDResources.getXObjects(PDResources.java:269)
	at org.apache.tika.parser.pdf.PDF2XHTML.extractImages(PDF2XHTML.java:310)
	at org.apache.tika.parser.pdf.PDF2XHTML.extractImages(PDF2XHTML.java:319)
	at org.apache.tika.parser.pdf.PDF2XHTML.extractImages(PDF2XHTML.java:319)
	at org.apache.tika.parser.pdf.PDF2XHTML.extractImages(PDF2XHTML.java:319)
	at org.apache.tika.parser.pdf.PDF2XHTML.extractImages(PDF2XHTML.java:319)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 30 Sep 2015 15:29:26 +0000" id="792" opendate="Tue, 22 Sep 2015 22:13:09 +0000">
		<buginformation>
			<summary>Use java.nio.file.Path in TikaInputStream</summary>
			<description>&lt;p&gt;This will provide support for the new api for users who need it, and provide better information in I/O operations, e.g. detailed exception if file cannot be read.&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;used Path and methods in java.nio.file.Files internally&lt;/li&gt;
	&lt;li&gt;add getPath() method as the counterpart to getFile()&lt;/li&gt;
	&lt;li&gt;modified test to use&lt;/li&gt;
&lt;/ul&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/io/TikaInputStream.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 30 Sep 2015 16:00:10 +0000" id="793" opendate="Wed, 23 Sep 2015 12:12:49 +0000">
		<buginformation>
			<summary>Change file-&gt;path in tika-batch throughout</summary>
			<description>&lt;p&gt;Add Path equivalents for File and deprecate File usage in tika-batch.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 30 Sep 2015 17:08:20 +0000" id="794" opendate="Thu, 24 Sep 2015 14:04:37 +0000">
		<buginformation>
			<summary>Use java.nio.file.Path in org.apache.tika.detect</summary>
			<description>&lt;p&gt;Add constructors and methods accepting java.nio.file.Path to TrainedModelDetector &amp;amp; Son.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/detect/NNExampleModelDetector.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 8 Oct 2015 02:24:03 +0000" id="795" opendate="Wed, 30 Sep 2015 18:52:50 +0000">
		<buginformation>
			<summary>Make ppt and pptx paragraph/div breaks more consistent</summary>
			<description>&lt;p&gt;In working on &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=kiwiwings&quot; class=&quot;user-hover&quot; rel=&quot;kiwiwings&quot;&gt;Andreas Beeker&lt;/a&gt;'s patch for the new handling of PPT/X, I found that our PPT/PPTX parsers behave very differently with &amp;lt;p&amp;gt; and &amp;lt;div&amp;gt; breaks, especially now that we've applied the upgrades from &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1707&quot; title=&quot;Upgrade to Apache POI 3.13 Beta 2&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1707&quot;&gt;&lt;del&gt;TIKA-1707&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I propose adding quite a few more &amp;lt;p&amp;gt; to capture the sentence/bullet level breaks in PPTX as we're now doing for PPT.&lt;/p&gt;

&lt;p&gt;There are a handful of other things that we could clean up (table handling) as well.&lt;/p&gt;

&lt;p&gt;Some of these changes may be relevant to this &lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/tika-dev/201306.mbox/%3CCAL8PwkY96_GKJmps6ZXuoe7H7-byvpxJbkTBuy1goKU3sKZMtQ@mail.gmail.com%3E&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;discussion&lt;/a&gt;.  &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=shaie&quot; class=&quot;user-hover&quot; rel=&quot;shaie&quot;&gt;Shai Erera&lt;/a&gt;, any input?&lt;/p&gt;

&lt;p&gt;Patch and example output to follow.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 1 Oct 2015 13:26:22 +0000" id="796" opendate="Wed, 30 Sep 2015 21:02:46 +0000">
		<buginformation>
			<summary>Update forbiddenapis to v2.0</summary>
			<description>&lt;p&gt;Forbiddenapis 2.0 was released a few hours ago. Apache POI and Lucene already updated, Tika should do this, too.&lt;/p&gt;

&lt;p&gt;Attached is a patch.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The main new feature is native support for the Gradle build system (minimum requirement is Gradle 2.3). But also Apache Ant and Apache Maven build systems got improved support: Ant can now load signatures from arbitrary resources by using a new XML element &amp;lt;signatures&amp;gt;&amp;lt;/signatures&amp;gt; that may contain any valid ANT resource, e.g., ivy's cache-filesets or plain URLs. Apache Maven now supports to load signatures files as artifacts from your repository or Maven Central (new signaturesArtifacts Mojo property).&lt;/p&gt;

&lt;p&gt;Breaking changes:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Update to Java 6 as minimum requirement.&lt;/li&gt;
	&lt;li&gt;Switch default Maven lifecycle phase to verify.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Bug fixes:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Add automatic plugin execution override for M2E. It is no longer needed to add a lifecycle mapping to exclude forbiddenapis to execute inside Eclipse's M2E&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;The M2E change is nice, because you no longer need the M2E workaround to disable running the plugin in Eclipse manually.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parent/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 1 Oct 2015 00:25:51 +0000" id="797" opendate="Wed, 30 Sep 2015 21:14:02 +0000">
		<buginformation>
			<summary>tika-batch tests fail on systems with whitespace or special chars in folder name</summary>
			<description>&lt;p&gt;This is one problem that forbiddenapis des not catch, because the method affected has valid use cases: &lt;tt&gt;URL#getFile()&lt;/tt&gt; or &lt;tt&gt;URL#getPath()&lt;/tt&gt; both return the URL path, which should never be treated as a file system path (for file: URLs). This is breaks asap, if the path contains special characters which may not be part of URL. getFile() and getPath() return the encoded path.&lt;/p&gt;

&lt;p&gt;The correct way to transform a file URL to a file is: &lt;tt&gt;new File(url.toURI())&lt;/tt&gt;. See also the list of &quot;bad stuff&quot; as listed by the Maven community for Mojos/Plugins.&lt;/p&gt;

&lt;p&gt;In fact the affected test should not use a file at all. Instead it should use &lt;tt&gt;Class#getResourceAsStream()&lt;/tt&gt;.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/test/java/org/apache/tika/cli/TikaCLIBatchCommandLineTest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 15 Oct 2015 02:51:04 +0000" id="798" opendate="Sun, 4 Oct 2015 01:42:40 +0000">
		<buginformation>
			<summary>Create Executor Service from TikaConfig</summary>
			<description>&lt;p&gt;Create a configurable executor service that is configurable from the TikaConfig.&lt;/p&gt;

&lt;p&gt; Konstantin Gribov added a comment - 23/Sep/15 09:55&lt;/p&gt;

&lt;p&gt;Bob Paulin, I have two ideas on the issue:&lt;/p&gt;

&lt;p&gt;    by default use common thread pool, configured via and contained in TikaConfig as Tyler Palsulich suggested,&lt;br/&gt;
    you can pass thread pool for parser invocation via ParserContext with fallback to default if now thread pool/executor service in context.&lt;/p&gt;

&lt;p&gt;Also o.a.tika.Tika#parse(InputStream, Metadate) produces o.a.tika.parser.ParsingReader and anonymous Executor with unbounded daemon thread creation.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/concurrent/ConfigurableThreadPoolExecutor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 8 Oct 2015 01:31:48 +0000" id="799" opendate="Wed, 7 Oct 2015 13:45:00 +0000">
		<buginformation>
			<summary>Some doc and docx store multiple authors as semi-colon delimited list</summary>
			<description>&lt;p&gt;It looks like doc and docx are storing multiple authors in a single author field delimited by semi-colons.  We should parse this value and add multiple authors where appropriate.&lt;/p&gt;

&lt;p&gt;Notes: when I tried to add an author with a semicolon in the name, the result was two authors...doesn't look like there is any escaping going on.&lt;/p&gt;

&lt;p&gt;We should check to see what's going on in the other MS formats and with other metadata items that are allowed to be multivalued in Dublin Core.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 16 Oct 2015 13:46:05 +0000" id="800" opendate="Fri, 16 Oct 2015 06:42:27 +0000">
		<buginformation>
			<summary>Mimetype of VTT files</summary>
			<description>&lt;p&gt;Files with extension &quot;vtt&quot; are &quot;WebVTT: The Web Video Text Tracks Format&quot; files.&lt;/p&gt;

&lt;p&gt;The mimetype resolved by tika is currently text/plain.&lt;/p&gt;

&lt;p&gt;The correct mimetype should be text/vtt.&lt;/p&gt;

&lt;p&gt;see: &lt;a href=&quot;https://w3c.github.io/webvtt/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://w3c.github.io/webvtt/&lt;/a&gt;&lt;/p&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/resources/test-documents/testWebVTT_header.vtt</file>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
			<file>/tika-parsers/src/test/java/org/apache/tika/mime/TestMimeTypes.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 21 Oct 2015 14:15:39 +0000" id="801" opendate="Wed, 21 Oct 2015 14:11:20 +0000">
		<buginformation>
			<summary>Regression in spacing around differently formatted runs in PPT</summary>
			<description>&lt;p&gt;There's a regression in spacing between runs that have different font formats within ppt files.  Can't tell if we can fix this at the Tika level or if we have to go into the POI level. &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 11 Mar 2016 01:38:32 +0000" id="802" opendate="Thu, 10 Mar 2016 22:05:04 +0000">
		<buginformation>
			<summary>NullPointerException in ImageMetadataExtractor$ExifHandler</summary>
			<description>&lt;p&gt;Null pointer exception in ImageMetadataExtractor$ExifHandler&lt;/p&gt; &lt;p&gt;=====================================================&lt;br/&gt;
org.apache.tika.exception.TikaException: Unexpected RuntimeException from org.apache.tika.parser.jpeg.JpegParser@9597028&lt;br/&gt;
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:282)&lt;br/&gt;
Caused by: java.lang.NullPointerException&lt;br/&gt;
	at org.apache.tika.parser.image.ImageMetadataExtractor$ExifHandler.handlePhotoTags(ImageMetadataExtractor.java:383)&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/image/ImageMetadataExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 13 Mar 2016 01:16:18 +0000" id="803" opendate="Sun, 13 Mar 2016 00:32:38 +0000">
		<buginformation>
			<summary>Java 9 ThreadPoolExecutor Requires Max to be set first </summary>
			<description>&lt;p&gt;In Java 9 the setCorePoolSize checks the max to ensure the core &amp;lt; max.  Otherwise it throws an IllegalArgumentException.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/config/TikaConfig.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 29 Jun 2015 09:02:06 +0000" id="804" opendate="Wed, 22 Oct 2014 19:01:07 +0000">
		<buginformation>
			<summary>Extracting as HTML loses links in xlsx, ppt, and pptx files</summary>
			<description>&lt;p&gt;I am trying to convert documents to HTML, then looking through the HTML for anchor tags to find links to external URLs.  This works fine when looking at some document types, including PDFs, Open Document formats, Microsoft Word formats .doc and .docx, and the older Microsoft Excel .xls format, but it does not work for any Microsoft Powerpoint formats (.ppt or .pptx) and it does not work for the newer Excel .xlsx format.  For the .ppt, .pptx, and .xlsx formats, the text is extracted properly and formatted into HTML, but the link is not converted to an anchor tag.&lt;/p&gt;

&lt;p&gt;I am running tika in --server --html mode.&lt;/p&gt;

&lt;p&gt;I included samples of .xlsx, .ppt, and .pptx files that do not properly extract links, and also included samples of .ods and .odp files that do extract links properly.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/HSLFExtractor.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 2 Mar 2016 14:49:26 +0000" id="805" opendate="Mon, 15 Jun 2015 16:17:22 +0000">
		<buginformation>
			<summary>Allow easier XML serialization of TikaConfig</summary>
			<description>&lt;p&gt;In &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1418&quot; title=&quot;Add TikaConfigDumperExample to example package&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1418&quot;&gt;&lt;del&gt;TIKA-1418&lt;/del&gt;&lt;/a&gt;, we added an example for how to dump the config file so that users could easily modify it.  I think we should go further and make this an option at the tika-core level with hooks for tika-app and tika-server.  I propose adding a main() to TikaConfig that will print the xml config file that Tika is currently using to stdout.&lt;/p&gt;

&lt;p&gt;I'd like to put this into core so that e.g. Solr's DIH users can get by without having to download tika-app separately.  &lt;/p&gt;

&lt;p&gt;There's every chance that I've not accounted for issues with dynamic loading etc.  Also, I'd be ok with only having this available in tika-app and tika-server if there are good reasons.&lt;/p&gt;

&lt;p&gt;Feedback?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/language/translate/DefaultTranslator.java</file>
			<file>/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 26 Jan 2016 16:35:06 +0000" id="806" opendate="Tue, 5 Jan 2016 23:32:17 +0000">
		<buginformation>
			<summary>Support detecting DWF format</summary>
			<description>&lt;p&gt;Tika currently detects dwf files as application/octect-stream.&lt;br/&gt;
To make Tika mime magic detector correctly recognize dwf files it should be added this code fragment in &lt;em&gt;tika-mimetypes.xml&lt;/em&gt; registry:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-xml&quot;&gt;&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;mime-type type=&lt;span class=&quot;code-quote&quot;&gt;&quot;model/vnd.dwf&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;
	&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;acronym&amp;gt;&lt;/span&gt;dwf&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/acronym&amp;gt;&lt;/span&gt;
	&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;_comment&amp;gt;&lt;/span&gt;Design Web Format&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/_comment&amp;gt;&lt;/span&gt;
	&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;magic priority=&lt;span class=&quot;code-quote&quot;&gt;&quot;50&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;
		&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;match type=&lt;span class=&quot;code-quote&quot;&gt;&quot;string&quot;&lt;/span&gt; offset=&lt;span class=&quot;code-quote&quot;&gt;&quot;0&quot;&lt;/span&gt; value=&lt;span class=&quot;code-quote&quot;&gt;&quot;(DWF V&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;
			&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;match type=&lt;span class=&quot;code-quote&quot;&gt;&quot;string&quot;&lt;/span&gt; offset=&lt;span class=&quot;code-quote&quot;&gt;&quot;8&quot;&lt;/span&gt; value=&lt;span class=&quot;code-quote&quot;&gt;&quot;.&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;
				&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;match type=&lt;span class=&quot;code-quote&quot;&gt;&quot;string&quot;&lt;/span&gt; offset=&lt;span class=&quot;code-quote&quot;&gt;&quot;11&quot;&lt;/span&gt; value=&lt;span class=&quot;code-quote&quot;&gt;&quot;)&quot;&lt;/span&gt; /&amp;gt;&lt;/span&gt;
			&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/match&amp;gt;&lt;/span&gt;
		&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/match&amp;gt;&lt;/span&gt;
	&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/magic&amp;gt;&lt;/span&gt;
	&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;glob pattern=&lt;span class=&quot;code-quote&quot;&gt;&quot;*.dwf&quot;&lt;/span&gt; /&amp;gt;&lt;/span&gt;
&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/mime-type&amp;gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;br class=&quot;atl-forced-newline&quot; /&gt;
In current version (DWF 6.0), dwf file is a ZIP-compressed container for vector-based CAD drawings. It is basically a ZIP archive with the &lt;em&gt;(DWF V06.00)&lt;/em&gt; signature added before the regular ZIP magic number. For this reason, the match value to detect dwf files should be: &lt;tt&gt;(DWF V06.00)PK&lt;/tt&gt;.&lt;br/&gt;
In the previous versions, the dwf data transport isn't a ZIP file format, so the magic number is only the &lt;em&gt;(DWF V00.55)&lt;/em&gt; signature in the file header.&lt;br/&gt;
To make Tika detect dwf files with this version too I propose the match value in the code above.&lt;/p&gt;

&lt;p&gt;Thanks,&lt;/p&gt;

&lt;p&gt;Luca&lt;/p&gt;

&lt;p&gt;&lt;br class=&quot;atl-forced-newline&quot; /&gt;
P.S.: The DWF format specification is included in the DWF Toolkit. The DWF Toolkit is available for free at &lt;a href=&quot;http://www.autodesk.com/dwftoolkit&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.autodesk.com/dwftoolkit&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
			<file>/tika-parsers/src/test/resources/test-documents/testDWF2010.dwf</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 2 Feb 2016 19:46:06 +0000" id="807" opendate="Tue, 12 Jan 2016 20:41:12 +0000">
		<buginformation>
			<summary>Upgrade to PDFBox 1.8.11 when available</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/pom.xml</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 17 Feb 2016 18:52:46 +0000" id="808" opendate="Tue, 16 Feb 2016 02:05:44 +0000">
		<buginformation>
			<summary>Error while parsing an ogg file</summary>
			<description>&lt;p&gt;Unable to detect a malformed ogg file. The error thrown was &lt;br/&gt;
Exception in thread &quot;main&quot; java.io.IOException: Asked to read 4335 bytes&lt;br/&gt;
from 0 but hit EoF at 780&lt;br/&gt;
        at org.gagravarr.ogg.IOUtils.readFully(IOUtils.java:39)&lt;br/&gt;
        at org.gagravarr.ogg.IOUtils.readFully(IOUtils.java:31)&lt;br/&gt;
        at org.gagravarr.ogg.OggPage.&amp;lt;init&amp;gt;(OggPage.java:82)&lt;br/&gt;
        at&lt;br/&gt;
org.gagravarr.ogg.OggPacketReader.getNextPacket(OggPacketReader.java:116)&lt;br/&gt;
        at org.gagravarr.tika.OggDetector.detect(OggDetector.java:97)&lt;br/&gt;
        at&lt;br/&gt;
org.apache.tika.detect.CompositeDetector.detect(CompositeDetector.java:61)&lt;br/&gt;
        at org.apache.tika.cli.TikaCLI$10.process(TikaCLI.java:291)&lt;br/&gt;
        at org.apache.tika.cli.TikaCLI.process(TikaCLI.java:477)&lt;br/&gt;
        at org.apache.tika.cli.TikaCLI.main(TikaCLI.java:134)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;xdatadeploy@xdata upload&amp;#93;&lt;/span&gt;$&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 2 Mar 2016 02:25:39 +0000" id="809" opendate="Tue, 16 Feb 2016 03:55:43 +0000">
		<buginformation>
			<summary>Enhance PDFParser to extract text from XFA forms</summary>
			<description>&lt;p&gt;Extract text from PDF Forms (XFA).  Information about XFA: &lt;a href=&quot;https://en.wikipedia.org/wiki/XFA&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://en.wikipedia.org/wiki/XFA&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/XFAExtractor.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDF2XHTML.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 24 Feb 2016 15:03:59 +0000" id="810" opendate="Wed, 24 Feb 2016 12:37:19 +0000">
		<buginformation>
			<summary>Jackson update to latest version</summary>
			<description>&lt;p&gt;Linked to &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1868&quot; title=&quot;create clean tika-server jar and shaded classifier jar&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1868&quot;&gt;&lt;del&gt;TIKA-1868&lt;/del&gt;&lt;/a&gt; this is to update the version of Jackson used from 2.4.0 to 2.7.1.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-translate/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 25 Feb 2016 18:20:57 +0000" id="811" opendate="Wed, 24 Feb 2016 14:46:57 +0000">
		<buginformation>
			<summary>Relocating RichTextContentHandler into tika-core from tika-server</summary>
			<description>&lt;p&gt;linked to &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1868&quot; title=&quot;create clean tika-server jar and shaded classifier jar&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1868&quot;&gt;&lt;del&gt;TIKA-1868&lt;/del&gt;&lt;/a&gt;, different solution by refactoring class into tika-core so don't need to depend upon tika-server and changing other classes used to custom ones or other alternatives.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/sax/RichTextContentHandler.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 6 Mar 2016 12:47:56 +0000" id="812" opendate="Sun, 28 Feb 2016 04:11:56 +0000">
		<buginformation>
			<summary>Upgrade Apache SIS 0.6</summary>
			<description>&lt;p&gt;Pull request here: &lt;a href=&quot;https://github.com/apache/tika/pull/79&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/tika/pull/79&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/pom.xml</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 27 Apr 2016 00:20:12 +0000" id="813" opendate="Mon, 7 Mar 2016 14:56:40 +0000">
		<buginformation>
			<summary>Add XMPMM metadata extraction to JempboxExtractor</summary>
			<description>&lt;p&gt;The XMP Media Management (XMPMM) section of xmp carries some useful information.  We currently have keys for many of the important attributes in tika-core's o.a.t.metadata.XMPMM, and JempBox extracts the XMPMM schema, but the wiring between the two has not yet been installed. &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/image/xmp/JempboxExtractor.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 18 Apr 2016 16:50:03 +0000" id="814" opendate="Tue, 8 Mar 2016 14:27:44 +0000">
		<buginformation>
			<summary>Invalid closing script tag not handled gracefully by HtmlParser</summary>
			<description>&lt;p&gt;When an HTML file contains an invalid closing script tag, all content after that tag is interpreted as script data and therefore ignored.&lt;/p&gt;

&lt;p&gt;Reduced test case file attached.&lt;/p&gt;

&lt;p&gt;To reproduce:&lt;/p&gt;

&lt;p&gt;1) create a file with the following HTML&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-html&quot;&gt;&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;!DOCTYPE HTML PUBLIC &lt;span class=&quot;code-quote&quot;&gt;&quot;-//W3C//DTD HTML 4.01 Transitional//EN&quot;&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&quot;http://www.w3.org/TR/html4/loose.dtd&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;html&amp;gt;&lt;/span&gt;
	&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;head&amp;gt;&lt;/span&gt;
		&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;script lang=&lt;span class=&quot;code-quote&quot;&gt;&quot;javascript&quot;&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/script language&amp;gt;&lt;/span&gt;
	&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/head&amp;gt;&lt;/span&gt;
	&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;body&amp;gt;&lt;/span&gt;
		&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;p&amp;gt;&lt;/span&gt;This is a test.&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/p&amp;gt;&lt;/span&gt;
	&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/body&amp;gt;&lt;/span&gt;
&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/html&amp;gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;2) &lt;tt&gt;java -jar tika-app-1.12.jar -t test.html&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;Expected result:&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;This is a test.&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;What is actually returned:&lt;/p&gt;

&lt;p&gt;Nothing.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/html/HtmlParserTest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 4 Apr 2016 19:19:01 +0000" id="815" opendate="Thu, 31 Mar 2016 02:38:10 +0000">
		<buginformation>
			<summary>ExecutableParser doesn't call start document</summary>
			<description>&lt;p&gt;The ExecutableParser doesn't call start document which causes errors when producing XHTML &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/executable/ExecutableParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 4 Apr 2016 12:10:47 +0000" id="816" opendate="Thu, 31 Mar 2016 03:01:29 +0000">
		<buginformation>
			<summary>NPE in OpenDocumentParser</summary>
			<description>&lt;p&gt;NPE in OpenDocumentParser when no &quot;meta.xml&quot; file exists&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/odf/OpenDocumentParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 4 Apr 2016 17:53:19 +0000" id="817" opendate="Mon, 4 Apr 2016 00:06:56 +0000">
		<buginformation>
			<summary>NPE in JDBCTableReader</summary>
			<description>&lt;p&gt;NPE when there is a null String in a SQLite DB.&lt;/p&gt;

&lt;p&gt;Caused by: java.lang.NullPointerException&lt;br/&gt;
	at org.apache.tika.parser.jdbc.JDBCTableReader.addAllCharacters(JDBCTableReader.java:252)&lt;br/&gt;
	at org.apache.tika.parser.jdbc.JDBCTableReader.handleCell(JDBCTableReader.java:135)&lt;br/&gt;
	at org.apache.tika.parser.jdbc.JDBCTableReader.nextRow(JDBCTableReader.java:95)&lt;br/&gt;
	at org.apache.tika.parser.jdbc.AbstractDBParser.parse(AbstractDBParser.java:90)&lt;br/&gt;
	at org.apache.tika.parser.jdbc.SQLite3Parser.parse(SQLite3Parser.java:78)&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/jdbc/JDBCTableReader.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 16 May 2016 00:12:06 +0000" id="818" opendate="Mon, 4 Apr 2016 11:33:55 +0000">
		<buginformation>
			<summary>Filename detection misses when a # is in a filename</summary>
			<description>&lt;p&gt;If there is a pound character in a filename it will be detected as application/octet-stream instead of the proper type that is detected without the filename containing the pound.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Metadata metadata = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Metadata();
Tika tika = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Tika();
metadata.add(Metadata.RESOURCE_NAME_KEY, &lt;span class=&quot;code-quote&quot;&gt;&quot;test#.pdf&quot;&lt;/span&gt;);
&lt;span class=&quot;code-comment&quot;&gt;// tika uses NameDetector &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; first parameter == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;
&lt;/span&gt;&lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.out.println(tika.detect(&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;, metadata));
&lt;span class=&quot;code-comment&quot;&gt;// prints application/octet-stream instead of application/pdf&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Tested for application/pdf and application/xml.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/detect/NameDetector.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 4 Apr 2016 17:58:49 +0000" id="819" opendate="Mon, 4 Apr 2016 14:02:20 +0000">
		<buginformation>
			<summary>Need to close resources on exception in sqlite parser</summary>
			<description>&lt;p&gt;If there's an exception during parsing of a SQLite file, we aren't guaranteeing that the temp file is deleted.&lt;br/&gt;
If a TikaInputStream is used, we assume the calling code will close the stream and thereby delete the temp file.  However, if another type of InputStream is used, we copy that to a temp file, and we need to ensure that we delete that temp file if there's an exception during the parse.&lt;/p&gt;

&lt;p&gt;While we're at it, we should also clean up test code to close streams correctly.&lt;/p&gt;

&lt;p&gt;Unrelated to this issue... I noticed that xerial's SQLite code is still leaving behind a copy of the native dll in the temp folder on Windows the first time the SQLite parser is called.  See &lt;a href=&quot;https://github.com/xerial/sqlite-jdbc/issues/80&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/xerial/sqlite-jdbc/issues/80&lt;/a&gt;.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/jdbc/SQLite3ParserTest.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/jdbc/SQLite3DBParser.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/jdbc/AbstractDBParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 22 Apr 2016 14:21:26 +0000" id="820" opendate="Wed, 6 Apr 2016 11:17:57 +0000">
		<buginformation>
			<summary>Revert mp4 parser version because of new permanent hangs with 1.1.18</summary>
			<description>&lt;p&gt;In pre-pre-release-1-13 corpus testing of trunk, I found that the upgraded mp4parser is hitting permanent hangs with three files.  In the older version, it was able to parse 2 with no problem and threw an exception on one.&lt;/p&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/pom.xml</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 7 Apr 2016 23:26:36 +0000" id="821" opendate="Wed, 6 Apr 2016 13:06:34 +0000">
		<buginformation>
			<summary>ForkParser leaves tmp jars behind on Windows (at least)</summary>
			<description>&lt;p&gt;During the build process, the ForkParser is leaving behind its temp jars.  &lt;/p&gt;

&lt;p&gt;I think the process is still holding onto the jar very briefly after we destroy() it.  &lt;/p&gt;

&lt;p&gt;Java thinks the process is done &amp;#8211; exitValue() returns 1 and then the jar fails to be deleted.&lt;/p&gt;

&lt;p&gt;If we add waitFor() or even a sleep(10), after we destroy(), the tmp jar is deleted.&lt;/p&gt;

&lt;p&gt;I'm always hesitant to add an unbounded waitFor() (which we'll be able to bound in Java 8).  Any preferences for a fix?&lt;/p&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/fork/ForkParserIntegrationTest.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/fork/ForkClient.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 18 Apr 2016 16:01:06 +0000" id="822" opendate="Thu, 7 Apr 2016 13:46:37 +0000">
		<buginformation>
			<summary>LinkContentHandler skips script tags</summary>
			<description>&lt;p&gt;Just like in &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1835&quot; title=&quot;LinkContentHandler skips iframe and rel tags&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1835&quot;&gt;&lt;del&gt;TIKA-1835&lt;/del&gt;&lt;/a&gt;, &amp;lt;script&amp;gt; tags are not collected by LinkContentHandler. The difference between &amp;lt;script&amp;gt; and the other tags is that &amp;lt;script&amp;gt; tags that do not contain a &quot;src&quot; attribute are not links.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 10 Apr 2016 09:36:49 +0000" id="823" opendate="Sat, 9 Apr 2016 21:55:05 +0000">
		<buginformation>
			<summary>Add mime magic for apple single/double files</summary>
			<description>&lt;p&gt;Update tika-mimetypes.xml to identify Apple Single/Double files. &lt;a href=&quot;http://www.ietf.org/rfc/rfc1740.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.ietf.org/rfc/rfc1740.txt&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;lt;mime-type type=&quot;application/applefile&quot;&amp;gt;&lt;br/&gt;
&amp;lt;magic priority=&quot;50&quot;&amp;gt;&lt;br/&gt;
&amp;lt;match value=&quot;0x00051600&quot; type=&quot;string&quot; offset=&quot;0&quot;/&amp;gt;&lt;br/&gt;
&amp;lt;/magic&amp;gt;&lt;br/&gt;
&amp;lt;/mime-type&amp;gt;&lt;/p&gt;

&lt;p&gt;&amp;lt;mime-type type=&quot;multipart/appledouble&quot;&amp;gt;&lt;br/&gt;
&amp;lt;magic priority=&quot;50&quot;&amp;gt;&lt;br/&gt;
&amp;lt;match value=&quot;0x00051607&quot; type=&quot;string&quot; offset=&quot;0&quot;/&amp;gt;&lt;br/&gt;
&amp;lt;/magic&amp;gt;&lt;br/&gt;
&amp;lt;/mime-type&amp;gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 9 Jun 2017 01:48:46 +0000" id="824" opendate="Sun, 10 Apr 2016 00:37:33 +0000">
		<buginformation>
			<summary>Powerpoint parser doesn't extract text from diagrams</summary>
			<description>&lt;p&gt;Attached is an example org chart that Tika doesn't extract text from&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 12 Apr 2016 13:32:17 +0000" id="825" opendate="Tue, 12 Apr 2016 09:40:43 +0000">
		<buginformation>
			<summary>Upgrade to Commons Compress 1.11</summary>
			<description>&lt;p&gt;Commons Compress 1.11 is out, which includes a fix for the issue reported in &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1717&quot; title=&quot;Tika throws exception on detecting content-type of a zip file&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1717&quot;&gt;TIKA-1717&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Need to check that nothing breaks with the new version, then upgrade&lt;/p&gt;

&lt;p&gt;(Based on the changelog, it doesn't look like there's any new features to take advantage of, nor new formats, just fixes)&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 21 Apr 2016 14:10:47 +0000" id="826" opendate="Wed, 13 Apr 2016 01:05:13 +0000">
		<buginformation>
			<summary>Clean up jdom version conflict</summary>
			<description>&lt;p&gt;Not sure best way to fix this.  grib 4.5.5 uses jdom 2.0.4 and rometools uses 2.0.2.  Both are now getting packed into tika-app.  Should we exclude the earlier (jdom from rometools) and hope for the best?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 8 May 2016 21:19:17 +0000" id="827" opendate="Mon, 18 Apr 2016 16:15:44 +0000">
		<buginformation>
			<summary>MIME types updates and additions for Scientific Data based on TREC-DD-Polar</summary>
			<description>&lt;p&gt;We used &lt;a href=&quot;http://github.com/chrismattmann/trec-dd-polar/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://github.com/chrismattmann/trec-dd-polar/&lt;/a&gt; and submitted several PRs that update MIME type info and/or add it to better support scientific data files. I'll link all the PRs and relevant issues here.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/detect/ZeroSizeFileDetector.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 27 Apr 2016 01:18:26 +0000" id="828" opendate="Wed, 27 Apr 2016 01:11:18 +0000">
		<buginformation>
			<summary>Put legacy language detection code back into 1.x=trunk</summary>
			<description>&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=grossws&quot; class=&quot;user-hover&quot; rel=&quot;grossws&quot;&gt;Konstantin Gribov&lt;/a&gt; recently observed on the &lt;a href=&quot;https://mail-archives.apache.org/mod_mbox/tika-dev/201604.mbox/%3CCALvb29ytzFdOi%3Dx06NTc0DeiY3oN5uZNMNe%3DV9K3ME6r%2B4gzYg%40mail.gmail.com%3E&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;devlist&lt;/a&gt; that we had some recent API breaking changes to the language detection component in 1.x=trunk.&lt;/p&gt;

&lt;p&gt;We need to add the legacy code back before the 1.13 release.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/test/resources/org/apache/tika/language/langbuilder/welsh_corpus.txt</file>
			<file>/tika-core/src/main/java/org/apache/tika/language/LanguageIdentifier.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 30 Jan 2018 16:48:45 +0000" id="829" opendate="Fri, 29 Apr 2016 16:07:21 +0000">
		<buginformation>
			<summary>Clean up code -- FindBugs</summary>
			<description>&lt;p&gt;FindBugs found a few things that we might want to clean up.  There are some false positives, but a few very good finds.&lt;/p&gt;

&lt;p&gt;I found it easier to navigate through the results with the FindBugs plugin (in Intellij for me) than the static reports.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/mp4/DirectFileReadDataSource.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 9 Jun 2016 17:38:09 +0000" id="830" opendate="Wed, 4 May 2016 05:51:51 +0000">
		<buginformation>
			<summary>Issue in parsing iWorksDocument with Apache Tika</summary>
			<description>&lt;p&gt;I was trying to parse iWorksDoc with Apache Tika. But am not getting parsed content as it is instead getting some other output from the content handler. Code snippet that I've used is attached with this.&lt;/p&gt;

&lt;p&gt;Output :-&lt;/p&gt;

&lt;p&gt;Contents of the file :&lt;br/&gt;
Index/Document.iwa&lt;br/&gt;
Index/ViewState.iwa&lt;br/&gt;
Index/CalculationEngine.iwa&lt;br/&gt;
Index/Tables/HeaderStorageBucket-2.iwa&lt;br/&gt;
Index/Tables/Tile.iwa&lt;br/&gt;
Index/Metadata.iwa&lt;br/&gt;
Metadata/Properties.plist&lt;/p&gt;

&lt;p&gt;I'm able to detect the file type using Detector api correctly. But am not getting the useful content out of the document.&lt;/p&gt;

&lt;p&gt;I'm attaching the iWorks docs that I've tested with (made with latest version of iOS). I got it working when testing with older versions. Thanks&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/resources/test-documents/testKeynoteNew.key</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 8 Nov 2016 14:13:11 +0000" id="831" opendate="Tue, 8 Nov 2016 17:04:56 +0000">
		<buginformation>
			<summary>Add extractInlineImages to PDFParser to enable parameter setting via config</summary>
			<description>&lt;p&gt;We've been handling extractInlineImages via PDFParserConfig, but we should add a setter for that on PDFParser so that we can control the behavior via tika-config file.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDFParser.java</file>
			<file>/tika-core/src/test/java/org/apache/tika/TikaTest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 4 Nov 2015 04:05:41 +0000" id="832" opendate="Thu, 8 Jan 2015 18:07:47 +0000">
		<buginformation>
			<summary>Add uniformity to parser parameter configuration</summary>
			<description>&lt;p&gt;We can currently configure parsers by the following means:&lt;br/&gt;
1) programmatically by direct calls to the parsers or their config objects&lt;br/&gt;
2) sending in a config object through the ParseContext&lt;br/&gt;
3) modifying .properties files for specific parsers (e.g. PDFParser)&lt;/p&gt;

&lt;p&gt;Rather than scattering the landscape with .properties files for each parser, it would be great if we could specify parser parameters in the main config file, something along the lines of this:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;    &amp;lt;parser class=&quot;org.apache.tika.parser.audio.AudioParser&quot;&amp;gt;
      &amp;lt;params&amp;gt;
        &amp;lt;int name=&quot;someparam1&quot;&amp;gt;2&amp;lt;/int&amp;gt;
        &amp;lt;str name=&quot;someOtherParam2&quot;&amp;gt;something or other&amp;lt;/str&amp;gt;
      &amp;lt;/params&amp;gt;
      &amp;lt;mime&amp;gt;audio/basic&amp;lt;/mime&amp;gt;
      &amp;lt;mime&amp;gt;audio/x-aiff&amp;lt;/mime&amp;gt;
      &amp;lt;mime&amp;gt;audio/x-wav&amp;lt;/mime&amp;gt;
    &amp;lt;/parser&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/test/java/org/apache/tika/parser/ConfigurableParserTest.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDF2XHTML.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 4 Nov 2015 04:05:41 +0000" id="833" opendate="Thu, 8 Jan 2015 18:11:00 +0000">
		<buginformation>
			<summary>Create configurable strategies for composite parsers</summary>
			<description>&lt;p&gt;Several parsers can handle the same mime type, and we are currently ordering which parser is chosen (roughly) by the alphabetic order of the parser class name.&lt;/p&gt;

&lt;p&gt;Let's allow users to configure strategies for picking parsers.&lt;/p&gt;

&lt;p&gt;See and contribute to full discussion here: &lt;a href=&quot;http://wiki.apache.org/tika/CompositeParserDiscussion&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://wiki.apache.org/tika/CompositeParserDiscussion&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/parser/ParserDecorator.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 25 Feb 2016 17:35:22 +0000" id="834" opendate="Thu, 25 Feb 2016 17:29:36 +0000">
		<buginformation>
			<summary>Fix rare npe in XWPFListManager</summary>
			<description>&lt;p&gt;Many thanks to &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=centic&quot; class=&quot;user-hover&quot; rel=&quot;centic&quot;&gt;Dominik Stadler&lt;/a&gt;'s &lt;a href=&quot;https://github.com/centic9/CommonCrawlDocumentDownload&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;CommonCrawlDocumentDownload&lt;/a&gt;, I recently grabbed .docx files from the initial index that comes with that code.  I'll be adding these docs to our regular regression testing for &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1302&quot; title=&quot;Let&amp;#39;s run Tika against a large batch of docs nightly&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1302&quot;&gt;TIKA-1302&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;While running Tika on these ~166k docs, ~30 of those files had an NPE in XWPFListManager.  We need to add a null check.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/XWPFListManager.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 6 Mar 2016 14:34:16 +0000" id="835" opendate="Fri, 4 Mar 2016 01:12:12 +0000">
		<buginformation>
			<summary>Update mimetype for application/vnd.ms-cab-compressed</summary>
			<description>&lt;p&gt;Updating tika-mimetype.xml to identify *.cab file format.&lt;/p&gt;

&lt;p&gt;Updated mime-type:&lt;br/&gt;
 &amp;lt;mime-type type=&quot;application/vnd.ms-cab-compressed&quot;&amp;gt;&lt;br/&gt;
      &amp;lt;glob pattern=&quot;*.cab&quot;/&amp;gt;&lt;br/&gt;
     &amp;lt;magic priority=&quot;50&quot;&amp;gt;&lt;br/&gt;
       &amp;lt;match value=&quot;MSCF&quot; type=&quot;string&quot; offset=&quot;0&quot; /&amp;gt;&lt;br/&gt;
     &amp;lt;/magic&amp;gt;&lt;br/&gt;
    &amp;lt;/mime-type&amp;gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 20 Mar 2016 05:05:03 +0000" id="836" opendate="Sun, 20 Mar 2016 04:58:14 +0000">
		<buginformation>
			<summary>Fix JavaDoc Failures on Java 8</summary>
			<description>&lt;p&gt;JavaDoc fails on Java 8.  See &lt;a href=&quot;http://stackoverflow.com/questions/15886209/maven-is-not-working-in-java-8-when-javadoc-tags-are-incomplete&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://stackoverflow.com/questions/15886209/maven-is-not-working-in-java-8-when-javadoc-tags-are-incomplete&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parent/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 8 May 2016 01:16:46 +0000" id="837" opendate="Mon, 2 May 2016 17:12:12 +0000">
		<buginformation>
			<summary>Added types to Grobid quantities parser</summary>
			<description>&lt;p&gt;Grobid Quantities returns information about the measurement(&quot;type&quot;), one example could be : length&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/main/java/org/apache/tika/gui/TikaGUI.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 25 May 2016 15:46:52 +0000" id="838" opendate="Thu, 19 May 2016 22:52:10 +0000">
		<buginformation>
			<summary>Invocation of java.net.URL.equals(Object), which blocks to do domain name resolution, in org.apache.tika.parser.geo.topic.GeoParser.initialize(URL)</summary>
			<description>&lt;p&gt;Performance - The equals and hashCode methods of URL are blocking&lt;br/&gt;
The equals and hashCode method of URL perform domain name resolution, this can result in a big performance hit. See &lt;a href=&quot;http://michaelscharf.blogspot.com/2006/11/javaneturlequals-and-hashcode-make.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://michaelscharf.blogspot.com/2006/11/javaneturlequals-and-hashcode-make.html&lt;/a&gt; for more information. Consider using java.net.URI instead. &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/geo/topic/GeoParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 23 May 2016 12:49:19 +0000" id="839" opendate="Fri, 20 May 2016 12:17:38 +0000">
		<buginformation>
			<summary>Issue message when server mode has started</summary>
			<description>&lt;p&gt;I am starting Tika in server mode (--server) from within another application.&lt;/p&gt;

&lt;p&gt;Currently, the command does not emit any messages in this mode that indicate a successful startup, so my application has to delay a bit and hope that the startup was successful.&lt;/p&gt;

&lt;p&gt;If the server mode issued a simple informational message (and maybe in --verbose mode only), it would be easier to check.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/main/java/org/apache/tika/cli/TikaCLI.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 14 Aug 2016 18:48:37 +0000" id="840" opendate="Wed, 25 May 2016 21:02:05 +0000">
		<buginformation>
			<summary>support parser parameters with type (int, double, etc) in configuration XML file</summary>
			<description>&lt;p&gt;Tika Configuration should be enhanced to support for basic types like int, double, boolean, url, file.&lt;/p&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/config/Initializable.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/utils/AnnotationUtils.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/parser/ParseContext.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 14 Aug 2016 18:48:53 +0000" id="841" opendate="Thu, 2 Jun 2016 01:33:38 +0000">
		<buginformation>
			<summary>Image Recognition with Tika </summary>
			<description>&lt;p&gt;Create &quot;ImageRecognitionParser&quot; which can have pluggable implementation for core recognition logic.&lt;/p&gt;

&lt;p&gt;As the name suggests, this parser should detect objects in the images, and support many implementations + models (similar to what NamedEntityParser did for text).&lt;/p&gt;

&lt;p&gt;Supply a default implementation based on Tensorflow with the current state-of-the-art model [1].&lt;/p&gt;

&lt;p&gt;Links:&lt;br/&gt;
[1] &lt;a href=&quot;https://www.tensorflow.org/versions/r0.8/tutorials/image_recognition/index.html#usage-with-python-api&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://www.tensorflow.org/versions/r0.8/tutorials/image_recognition/index.html#usage-with-python-api&lt;/a&gt;&lt;/p&gt;



			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/resources/org/apache/tika/parser/recognition/tf/InceptionRestDockerfile</file>
			<file>/tika-core/src/main/java/org/apache/tika/config/TikaConfig.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 8 Jun 2016 15:55:47 +0000" id="842" opendate="Tue, 7 Jun 2016 14:44:16 +0000">
		<buginformation>
			<summary>org.apache.tika.sax.ToXMLContentHandler$ElementInfo.getPrefix(ToXMLContentHandler.java:58)</summary>
			<description>&lt;p&gt;When trying to read the following PDF document:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.arcadiz.com/content/assets/Artikel_CloudWorks_Vernieuwingen_zorg_vragen_om_veel_snellere_verbindingen.pdf&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.arcadiz.com/content/assets/Artikel_CloudWorks_Vernieuwingen_zorg_vragen_om_veel_snellere_verbindingen.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;TIKA crashes for me with a java.lang.StackOverflowError, caused by a large number of recursion in:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;    at org.apache.tika.sax.ToXMLContentHandler$ElementInfo.getPrefix(ToXMLContentHandler.java:58)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For some reason, the Tika App doesn't exhibit this behavior, but the following MWE exposes the issue for me:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;import java.io.ByteArrayOutputStream;
import java.io.File;
import java.io.FileInputStream;
import org.apache.tika.metadata.Metadata;
import org.apache.tika.parser.AutoDetectParser;
import org.apache.tika.parser.ParseContext;
import org.apache.tika.sax.ToHTMLContentHandler;

public class test
{
    public static void main(String [] args) throws Exception {
        String p = &quot;/home/eggie/faulty_pdf_document.pdf&quot;;
        
        FileInputStream input = new FileInputStream(new File(p));
        AutoDetectParser tk = new AutoDetectParser();
        ByteArrayOutputStream os = new ByteArrayOutputStream();
        ToHTMLContentHandler handler = new ToHTMLContentHandler(os, &quot;UTF-8&quot;);
        ParseContext pc = new ParseContext();
        System.out.println(&quot;Parsing&quot;);
        tk.parse(input, handler, new Metadata(), pc);
    }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/image/xmp/JempboxExtractor.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 22 Jul 2016 12:56:31 +0000" id="843" opendate="Mon, 27 Jun 2016 19:42:13 +0000">
		<buginformation>
			<summary>Extraction of long sequences of digits from Excel spreadsheets using Tika 1.13 doesn’t yield the expected results</summary>
			<description>&lt;p&gt;If an Excel spreadsheet contains a long sequence of digits, such as a credit card number, Tika 1.13 will emit the said sequence in scientific notation.&lt;/p&gt;

&lt;p&gt;For example, the credit card number “340229177292566” is extracted from the attached spreadsheet as 3.40229E+14, which clearly is not the desired output. &lt;br/&gt;
This works as expected in 1.12 and earlier. I suspect POI’s recent use of org.apache.poi.ss.usermodel.ExcelGeneralNumberFormat is to blame.&lt;/p&gt;

&lt;p&gt;I think the impact of this issue is significant. There’s plenty of information that can no longer be reliably extracted from spreadsheets. Think credit card numbers, telephone numbers and product identifiers to name a few.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/microsoft/ExcelParserTest.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 1 Jul 2016 18:31:41 +0000" id="844" opendate="Wed, 6 Jul 2016 16:24:03 +0000">
		<buginformation>
			<summary>Add link string to hrefs in PDF</summary>
			<description>&lt;p&gt;The PDFParser is not including any text in the &amp;lt;a href=&quot;somelink.com&quot;/&amp;gt; annotations.  It would be great if we could calculate what text should go in the content of the &amp;lt;a&amp;gt;&amp;lt;/a&amp;gt; elements.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/AbstractPDF2XHTML.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 8 Jul 2016 18:22:58 +0000" id="845" opendate="Thu, 7 Jul 2016 08:55:49 +0000">
		<buginformation>
			<summary>A space is suppressed when parsing Odt file</summary>
			<description>&lt;p&gt;I have an ODT sample file which contains:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;This is a sample text available in page 1
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When I extract its content with Tika, I'm getting:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;This isa sample text available in page 1
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note the missing space between &lt;tt&gt;is&lt;/tt&gt; and &lt;tt&gt;a&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;I'll link to an example ODT file which reproduces this issue.&lt;br/&gt;
Note that I generated this ODT file from MS Word. The original MS Word file is correctly parsed by Tika.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/resources/test-documents/testOpenOffice2.odt</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/odf/OpenDocumentContentParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 20 Jul 2016 17:21:06 +0000" id="846" opendate="Wed, 20 Jul 2016 15:42:06 +0000">
		<buginformation>
			<summary>Problems with email attachments</summary>
			<description>&lt;p&gt;I stumbled across a couple of problems while parsing and extracting attachments from .eml files from Thunderbird. Some of them are wrongly identified (as text/html, or application/xhtml+xml) and in a lot of them, the attachments are not detected. I tried to parse 20 random eml files with attachments (pdf,txt,html,etc), and at least 10 of them are either identified as html, or correctly identified as rfc822 but the attachments are not extracted. I tried the same files using TikaCLI -z option with the same result.&lt;/p&gt;

&lt;p&gt;What I did: I extended the class ParsingEmbeddedDocumentExtractor to extract and store the attachments somewhere else (exactly as shown in this example code &lt;a href=&quot;https://github.com/apache/tika/blob/master/tika-example/src/main/java/org/apache/tika/example/ExtractEmbeddedFiles.java&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/apache/tika/blob/master/tika-example/src/main/java/org/apache/tika/example/ExtractEmbeddedFiles.java&lt;/a&gt;). &lt;/p&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 27 Jul 2016 01:34:16 +0000" id="847" opendate="Mon, 25 Jul 2016 19:47:01 +0000">
		<buginformation>
			<summary>OOM when parsing a corrupted CHM</summary>
			<description>&lt;p&gt;When parsing the attached corrupted CHM, ChmParser leaks memory and causes the app to hang (collecting garbage) and throw OOM errors.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 26 Jul 2016 10:46:43 +0000" id="848" opendate="Tue, 26 Jul 2016 05:42:25 +0000">
		<buginformation>
			<summary>MBOX file detected wrongly as text/html</summary>
			<description>&lt;p&gt;MBOX file doesn't get recognized via &quot;magic detection&quot; mechanism as &quot;application/mbox&quot;, but wrongly as &quot;text/html&quot;.&lt;/p&gt;

&lt;p&gt;Workaround for this in Tika 1.13 is achieved by placing following in custom-mimetypes.xml, as suggested on mailing list (priority has to be larger than message/rfc822):&lt;br/&gt;
&amp;lt;mime-type type=&quot;application/mbox&quot;&amp;gt;&lt;br/&gt;
        &amp;lt;magic priority=&quot;70&quot;&amp;gt;&lt;br/&gt;
            &amp;lt;match value=&quot;From &quot; type=&quot;string&quot; offset=&quot;0&quot;/&amp;gt;&lt;br/&gt;
        &amp;lt;/magic&amp;gt;&lt;br/&gt;
        &amp;lt;glob pattern=&quot;*.mbox&quot;/&amp;gt;&lt;br/&gt;
&amp;lt;/mime-type&amp;gt;&lt;/p&gt;

&lt;p&gt;Sample MBOX file is attached.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 21 Sep 2016 23:44:30 +0000" id="849" opendate="Fri, 5 Aug 2016 12:11:08 +0000">
		<buginformation>
			<summary>TXTParser overwrites mime type/masks types that are subtype of text</summary>
			<description>&lt;p&gt;For vcal and other mime types that are subclasses of &lt;tt&gt;text/plain&lt;/tt&gt;, the TXTParser overwrites their mime type as &quot;text/plain&quot;.  We should check to see what mime has been sent in via the Metadata and add the charset to that, e.g. &quot;text/calendar; charset=ISO-8859-1&quot;...right?&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;            Charset charset = reader.getCharset();
            MediaType type = new MediaType(MediaType.TEXT_PLAIN, charset);
            metadata.set(Metadata.CONTENT_TYPE, type.toString());
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 5 Aug 2016 16:29:27 +0000" id="850" opendate="Fri, 5 Aug 2016 16:20:57 +0000">
		<buginformation>
			<summary>Add space for &lt;br/&gt; elements in MSWord 2003XML</summary>
			<description>&lt;p&gt;Not currently adding new line for &amp;lt;br/&amp;gt; elements. &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/xml/AbstractXML2003Parser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 15 Sep 2016 15:58:29 +0000" id="851" opendate="Tue, 16 Aug 2016 13:29:47 +0000">
		<buginformation>
			<summary>Exception on parsing .docx file</summary>
			<description>&lt;p&gt;Command: java -jar tika-app-1.13.jar input.docx&lt;br/&gt;
Exception in thread &quot;main&quot; org.apache.tika.exception.TikaException: Error creating OOXML extractor&lt;br/&gt;
        at org.apache.tika.parser.microsoft.ooxml.OOXMLExtractorFactory.parse(OOXMLExtractorFactory.java:120)&lt;br/&gt;
        at org.apache.tika.parser.microsoft.ooxml.OOXMLParser.parse(OOXMLParser.java:87)&lt;br/&gt;
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:280)&lt;br/&gt;
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:280)&lt;br/&gt;
        at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)&lt;br/&gt;
        at org.apache.tika.cli.TikaCLI$OutputType.process(TikaCLI.java:191)&lt;br/&gt;
        at org.apache.tika.cli.TikaCLI.process(TikaCLI.java:480)&lt;br/&gt;
        at org.apache.tika.cli.TikaCLI.main(TikaCLI.java:145)&lt;br/&gt;
Caused by: org.apache.xmlbeans.impl.values.XmlValueOutOfRangeException: Invalid int value: 4294967295&lt;br/&gt;
        at org.apache.xmlbeans.impl.values.JavaIntHolder.set_text(JavaIntHolder.java:43)&lt;br/&gt;
        at org.apache.xmlbeans.impl.values.XmlObjectBase.update_from_wscanon_text(XmlObjectBase.java:1180)&lt;br/&gt;
        at org.apache.xmlbeans.impl.values.XmlObjectBase.check_dated(XmlObjectBase.java:1319)&lt;br/&gt;
        at org.apache.xmlbeans.impl.values.JavaIntHolder.getIntValue(JavaIntHolder.java:53)&lt;br/&gt;
        at org.openxmlformats.schemas.officeDocument.x2006.extendedProperties.impl.CTPropertiesImpl.getTotalTime(Unknown Source)&lt;br/&gt;
        at org.apache.tika.parser.microsoft.ooxml.MetadataExtractor.extractMetadata(MetadataExtractor.java:124)&lt;br/&gt;
        at org.apache.tika.parser.microsoft.ooxml.MetadataExtractor.extract(MetadataExtractor.java:62)&lt;br/&gt;
        at org.apache.tika.parser.microsoft.ooxml.OOXMLExtractorFactory.parse(OOXMLExtractorFactory.java:109)&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/MetadataExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 26 Sep 2016 16:56:25 +0000" id="852" opendate="Thu, 18 Aug 2016 01:33:11 +0000">
		<buginformation>
			<summary>Extract PDF DocInfo fields into separate metadata fields</summary>
			<description>&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I have a PDF in which title has been set twice &amp;#8211; once as Dublin core metadata: &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&amp;lt;dc:title&amp;gt;
  &amp;lt;rdf:Alt&amp;gt;
    &amp;lt;rdf:li xml:lang=&lt;span class=&quot;code-quote&quot;&gt;&quot;x-&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;&quot;&lt;/span&gt;&amp;gt;
      Consumer credit cards - conditions of use
    &amp;lt;/rdf:li&amp;gt;
  &amp;lt;/rdf:Alt&amp;gt;
&amp;lt;/dc:title&amp;gt;&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and again in the PDF DocInfo section: &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;/Title(Consumer Credit Card - Conditions of Use)&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When I use Tika to transform the PDF into HTML &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;java -jar tika-app-1.13.jar int_Consumer_Conditions_of_use.pdf&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt; it outputs this metadata: &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;dc:title&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;Consumer credit cards - conditions of use&quot;&lt;/span&gt;/&amp;gt;&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt; and this &amp;lt;title&amp;gt; tag: &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&amp;lt;title&amp;gt;Consumer credit cards - conditions of use&amp;lt;/title&amp;gt;&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt; meaning we no longer have access to the DocInfo title.&lt;/p&gt;

&lt;p&gt;Is there some way you could adapt Tika to copy this PDF DocInfo forward during a conversion under a new type of metadata, e.g. &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&amp;lt;meta name=&lt;span class=&quot;code-quote&quot;&gt;&quot;docinfo:title&quot;&lt;/span&gt; content=&lt;span class=&quot;code-quote&quot;&gt;&quot;Consumer Credit Card - Conditions of Use&quot;&lt;/span&gt;/&amp;gt;&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

			</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 29 Aug 2016 13:51:02 +0000" id="853" opendate="Mon, 29 Aug 2016 03:39:31 +0000">
		<buginformation>
			<summary>Document type detected incorrectly for Stata datasets (.dta extension)</summary>
			<description>&lt;p&gt;The content type of Stata datasets (created using &lt;a href=&quot;http://www.stata.com&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.stata.com&lt;/a&gt; software) is incorrectly detected as `text/html` by Tika. I have tested this using the latest release of Tika, v1.13:&lt;/p&gt;

&lt;p&gt;```&lt;br/&gt;
$ curl -O &lt;a href=&quot;http://www.stata-press.com/data/r14/auto.dta&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.stata-press.com/data/r14/auto.dta&lt;/a&gt;&lt;br/&gt;
$ java -jar tika-app-1.13.jar --detect auto.dta&lt;br/&gt;
text/html&lt;br/&gt;
```&lt;/p&gt;

&lt;p&gt;I believe that the type should instead be `application/octet-stream` (or the equivalent).&lt;/p&gt;

&lt;p&gt;I originally reported this bug downstream (at &lt;a href=&quot;https://github.com/laurilehmijoki/s3_website/issues/232&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/laurilehmijoki/s3_website/issues/232&lt;/a&gt;), and was advised to report upstream to Tika. In addition to the one I downloaded using `curl` in my example, a variety of reference Stata datasets are posted here: &lt;a href=&quot;http://www.stata-press.com/data/r14/dmain.html&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.stata-press.com/data/r14/dmain.html&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
			<file>/tika-parsers/src/test/java/org/apache/tika/mime/TestMimeTypes.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 22 Sep 2016 01:14:06 +0000" id="854" opendate="Thu, 8 Sep 2016 21:45:01 +0000">
		<buginformation>
			<summary>Extract Macro text from Microsoft Office documents</summary>
			<description>&lt;p&gt;Tika supports macro-enabled Microsoft Office documents by extracting metadata and contents, however, macros within the document are not in the metadata or content output.&lt;br/&gt;
Desire is to have the macro text extracted also.&lt;/p&gt;

&lt;p&gt;Info regarding macro extraction: &lt;a href=&quot;http://www.decalage.info/vba_tools&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://www.decalage.info/vba_tools&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/test/java/org/apache/tika/TikaTest.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 23 Sep 2016 14:02:36 +0000" id="855" opendate="Sat, 17 Sep 2016 07:47:20 +0000">
		<buginformation>
			<summary>Add back 'fileUrl' functionality to TikaJAXRS Server subject to security controls</summary>
			<description>&lt;p&gt;Add back 'fileUrl' functionality from version 1.9 to TikaJAXRS Server subject to additional security controls:&lt;/p&gt;

&lt;p&gt;disable by default&lt;br/&gt;
only enable if appropriate configuration flags are specified&lt;br/&gt;
when enabled print warning displaying at least CVE ID: CVE-2015-3271.&lt;/p&gt;

&lt;p&gt;as discussed on dev@tika.apache.org mailing list under title &quot;Query on correct use of 'fileUrl' in TikaJAXRS Server to extract document at remote url - my request is not working&quot;.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-server/src/main/java/org/apache/tika/server/DefaultInputStreamFactory.java</file>
			<file>/tika-server/src/main/java/org/apache/tika/server/TikaServerCli.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 30 Sep 2016 11:42:35 +0000" id="856" opendate="Thu, 22 Sep 2016 17:19:14 +0000">
		<buginformation>
			<summary>Add hOCR output type to the TesseractOCRParser</summary>
			<description>&lt;p&gt;I've tweaked the TesseractOCRParser and TesseractOCRConfig to add the &quot;txt&quot; or &quot;hocr&quot; parameters that allows you to get specific outputs.  There are also &quot;pdf&quot; and in the next version of Tesseract a &quot;tsv&quot; outputs, but didn't add support for those.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 26 Sep 2016 20:07:57 +0000" id="857" opendate="Mon, 26 Sep 2016 18:42:15 +0000">
		<buginformation>
			<summary>Tika.parseToString() with maxLength doesn't work correctly for PDF files</summary>
			<description>&lt;p&gt;When parsing PDF file with Tika.parseToString(InputStream stream, Metadata metadata, int maxLength) and maxLength &amp;lt; content size it throws Exception.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;org.apache.tika.exception.TikaException: Unable to extract all PDF content

	at org.apache.tika.parser.pdf.PDF2XHTML.process(PDF2XHTML.java:135)
	at org.apache.tika.parser.pdf.PDFParser.parse(PDFParser.java:150)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:280)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:280)
	at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:120)
	at org.apache.tika.Tika.parseToString(Tika.java:568)
Caused by: org.apache.commons.io.IOExceptionWithCause: Unable to write a string: Tika - Content Analysis Toolkit
	at org.apache.tika.parser.pdf.PDF2XHTML.writeString(PDF2XHTML.java:302)
	at org.apache.pdfbox.text.PDFTextStripper.writeString(PDFTextStripper.java:779)
	at org.apache.pdfbox.text.PDFTextStripper.writeLine(PDFTextStripper.java:1738)
	at org.apache.pdfbox.text.PDFTextStripper.writePage(PDFTextStripper.java:672)
	at org.apache.pdfbox.text.PDFTextStripper.processPage(PDFTextStripper.java:392)
	at org.apache.tika.parser.pdf.PDF2XHTML.processPage(PDF2XHTML.java:143)
	at org.apache.pdfbox.text.PDFTextStripper.processPages(PDFTextStripper.java:319)
	at org.apache.pdfbox.text.PDFTextStripper.writeText(PDFTextStripper.java:266)
	at org.apache.tika.parser.pdf.PDF2XHTML.process(PDF2XHTML.java:111)
	... 35 more
Caused by: org.apache.tika.sax.TaggedSAXException: Your document contained more than 100 characters, and so your requested limit has been reached. To receive the full text of the document, increase your limit. (Text up to the limit is however available).
org.apache.tika.sax.TaggedSAXException: Your document contained more than 100 characters, and so your requested limit has been reached. To receive the full text of the document, increase your limit. (Text up to the limit is however available).
org.apache.tika.sax.WriteOutContentHandler$WriteLimitReachedException: Your document contained more than 100 characters, and so your requested limit has been reached. To receive the full text of the document, increase your limit. (Text up to the limit is however available).
	at org.apache.tika.sax.TaggedContentHandler.handleException(TaggedContentHandler.java:113)
	at org.apache.tika.sax.ContentHandlerDecorator.characters(ContentHandlerDecorator.java:148)
	at org.apache.tika.sax.ContentHandlerDecorator.characters(ContentHandlerDecorator.java:146)
	at org.apache.tika.sax.SafeContentHandler.access$001(SafeContentHandler.java:46)
	at org.apache.tika.sax.SafeContentHandler$1.write(SafeContentHandler.java:82)
	at org.apache.tika.sax.SafeContentHandler.filter(SafeContentHandler.java:140)
	at org.apache.tika.sax.SafeContentHandler.characters(SafeContentHandler.java:287)
	at org.apache.tika.sax.XHTMLContentHandler.characters(XHTMLContentHandler.java:279)
	at org.apache.tika.sax.XHTMLContentHandler.characters(XHTMLContentHandler.java:306)
	at org.apache.tika.parser.pdf.PDF2XHTML.writeString(PDF2XHTML.java:300)
	... 43 more
Caused by: org.apache.tika.sax.TaggedSAXException: Your document contained more than 100 characters, and so your requested limit has been reached. To receive the full text of the document, increase your limit. (Text up to the limit is however available).
org.apache.tika.sax.WriteOutContentHandler$WriteLimitReachedException: Your document contained more than 100 characters, and so your requested limit has been reached. To receive the full text of the document, increase your limit. (Text up to the limit is however available).
	at org.apache.tika.sax.TaggedContentHandler.handleException(TaggedContentHandler.java:113)
	at org.apache.tika.sax.ContentHandlerDecorator.characters(ContentHandlerDecorator.java:148)
	at org.apache.tika.sax.ContentHandlerDecorator.characters(ContentHandlerDecorator.java:146)
	... 51 more
Caused by: org.apache.tika.sax.WriteOutContentHandler$WriteLimitReachedException: Your document contained more than 100 characters, and so your requested limit has been reached. To receive the full text of the document, increase your limit. (Text up to the limit is however available).
	at org.apache.tika.sax.WriteOutContentHandler.characters(WriteOutContentHandler.java:141)
	at org.apache.tika.sax.ContentHandlerDecorator.characters(ContentHandlerDecorator.java:146)
	at org.apache.tika.sax.xpath.MatchingContentHandler.characters(MatchingContentHandler.java:85)
	at org.apache.tika.sax.ContentHandlerDecorator.characters(ContentHandlerDecorator.java:146)
	at org.apache.tika.sax.ContentHandlerDecorator.characters(ContentHandlerDecorator.java:146)
	at org.apache.tika.sax.SecureContentHandler.characters(SecureContentHandler.java:270)
	at org.apache.tika.sax.ContentHandlerDecorator.characters(ContentHandlerDecorator.java:146)
	... 52 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/AbstractPDF2XHTML.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 29 May 2018 18:28:13 +0000" id="858" opendate="Wed, 28 Sep 2016 13:21:46 +0000">
		<buginformation>
			<summary>Html Parser does not keep the html tag attributes</summary>
			<description>&lt;p&gt;Parsing a very simple html like &lt;br/&gt;
 &amp;lt;!DOCTYPE html&amp;gt;&lt;br/&gt;
&amp;lt;html lang=&quot;en&quot;&amp;gt;&lt;br/&gt;
&amp;lt;head&amp;gt;&lt;br/&gt;
&amp;lt;title&amp;gt;Page Title&amp;lt;/title&amp;gt;&lt;br/&gt;
&amp;lt;/head&amp;gt;&lt;br/&gt;
&amp;lt;body&amp;gt;&lt;/p&gt;

&lt;p&gt;&amp;lt;h1 align=&quot;left&quot;&amp;gt;My First Heading&amp;lt;/h1&amp;gt;&lt;br/&gt;
&amp;lt;p&amp;gt;My first paragraph.&amp;lt;/p&amp;gt;&lt;/p&gt;

&lt;p&gt;&amp;lt;/body&amp;gt;&lt;br/&gt;
&amp;lt;/html&amp;gt; &lt;/p&gt;

&lt;p&gt;you won't be able to access the html tag's attributes (here lang=&quot;en&quot;) in the ContentHandler : &lt;br/&gt;
*in the method startElement(String ns, String localName, String name,&lt;br/&gt;
      Attributes atts), atts is empty.&lt;br/&gt;
*Moreover it seems that the html tag's attributes are not passed trough the HtmlMapper.mapSafeAttribute method too.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/parser/html/HtmlParserTest.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/sax/XHTMLContentHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 28 Sep 2016 17:40:44 +0000" id="859" opendate="Wed, 28 Sep 2016 17:31:18 +0000">
		<buginformation>
			<summary>Don't use MAPIMessage.close()</summary>
			<description>&lt;p&gt;Regression found during comparison of 1.14 trunk and 1.13.  I added a call to the new MAPIMessage.close() after the parse.  This causes problems if an MSG file has multiple MSG files attached.  They may all be in the same directory.  When the first child MSG is parsed, MAPIMessage.close() closes the directory and the other children can't be read:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Caused by: java.lang.RuntimeException: java.nio.channels.ClosedChannelException
	at org.apache.poi.poifs.filesystem.NPOIFSStream$StreamBlockByteBufferIterator.&amp;lt;init&amp;gt;(NPOIFSStream.java:151)
	at org.apache.poi.poifs.filesystem.NPOIFSStream.getBlockIterator(NPOIFSStream.java:95)
	at org.apache.poi.poifs.filesystem.NPOIFSMiniStore.getBlockAt(NPOIFSMiniStore.java:67)
	at org.apache.poi.poifs.filesystem.NPOIFSStream$StreamBlockByteBufferIterator.next(NPOIFSStream.java:169)
	at org.apache.poi.poifs.filesystem.NPOIFSStream$StreamBlockByteBufferIterator.next(NPOIFSStream.java:142)
	at org.apache.poi.poifs.filesystem.NDocumentInputStream.readFully(NDocumentInputStream.java:248)
	at org.apache.poi.poifs.filesystem.NDocumentInputStream.read(NDocumentInputStream.java:150)
	at org.apache.poi.poifs.filesystem.DocumentInputStream.read(DocumentInputStream.java:125)
	at org.apache.poi.util.IOUtils.toByteArray(IOUtils.java:84)
	at org.apache.poi.util.IOUtils.toByteArray(IOUtils.java:72)
	at org.apache.poi.hsmf.datatypes.ByteChunk.readValue(ByteChunk.java:51)
	at org.apache.poi.hsmf.parsers.POIFSChunkParser.process(POIFSChunkParser.java:211)
	at org.apache.poi.hsmf.parsers.POIFSChunkParser.processChunks(POIFSChunkParser.java:112)
	at org.apache.poi.hsmf.parsers.POIFSChunkParser.parse(POIFSChunkParser.java:84)
	at org.apache.poi.hsmf.MAPIMessage.&amp;lt;init&amp;gt;(MAPIMessage.java:142)
	at org.apache.tika.parser.microsoft.OutlookExtractor.&amp;lt;init&amp;gt;(OutlookExtractor.java:82)
	at org.apache.tika.parser.microsoft.OfficeParser.parse(OfficeParser.java:190)
	at org.apache.tika.parser.microsoft.OfficeParser.parse(OfficeParser.java:130)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:280)
	... 30 more
Caused by: java.nio.channels.ClosedChannelException
	at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:110)
	at sun.nio.ch.FileChannelImpl.size(FileChannelImpl.java:300)
	at org.apache.poi.poifs.nio.FileBackedDataSource.size(FileBackedDataSource.java:133)
	at org.apache.poi.poifs.filesystem.NPOIFSFileSystem.getChainLoopDetector(NPOIFSFileSystem.java:634)
	at org.apache.poi.poifs.filesystem.NPOIFSStream$StreamBlockByteBufferIterator.&amp;lt;init&amp;gt;(NPOIFSStream.java:149)
	... 48 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/OutlookExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 30 Sep 2016 23:43:54 +0000" id="860" opendate="Fri, 30 Sep 2016 19:56:33 +0000">
		<buginformation>
			<summary>&quot;hocr&quot; case on Linux fails, but works on OSX.  Related to TIKA-2093</summary>
			<description>&lt;p&gt;We pass a output type, either TXT or HOCR to the Tesseract command line.   When we call the command line we lowercase it to &quot;txt&quot; or &quot;hocr&quot;.  However, when we read back in the output, we don't lower case it.  on OSX the constructed file path &quot;output.HOCR&quot; is actually found, but in Linux it doesn't.  This patch lower cases the HOCR to hocr and TXT to txt in the constructed file path.&lt;/p&gt;

&lt;p&gt;I didn't write a unit test as I don't have a good linux env to test it in, but I was able to put a patched version of the Tika Parser Jar into my Docker Build to test it works.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/ocr/TesseractOCRParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 7 Oct 2016 17:29:26 +0000" id="861" opendate="Fri, 7 Oct 2016 17:06:01 +0000">
		<buginformation>
			<summary>Log full exception, not just message in tika-batch</summary>
			<description>&lt;p&gt;In a number of places in tika-batch, I include &lt;tt&gt;e.getMessage()&lt;/tt&gt; without including the &lt;tt&gt;e&lt;/tt&gt;.  For better logging, let's include the exception.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-batch/src/main/java/org/apache/tika/batch/BatchNoRestartError.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 17 Oct 2016 18:57:23 +0000" id="862" opendate="Sat, 15 Oct 2016 21:11:27 +0000">
		<buginformation>
			<summary>Extract all email headers from Outlook .msg files into Metadata</summary>
			<description>&lt;p&gt;Currently most email headers are not added to the Metadata when extracting Outlook .msg files.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://svn.apache.org/repos/asf/tika/trunk/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/OutlookExtractor.java&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://svn.apache.org/repos/asf/tika/trunk/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/OutlookExtractor.java&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The headers - &lt;tt&gt;msg.getHeaders()&lt;/tt&gt; - are already being looped through as a way to estimate the date.&lt;/p&gt;

&lt;p&gt;All headers should be added to Metadata, using the name of the header with a prefix such as &lt;tt&gt;&quot;raw-header:&quot;&lt;/tt&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/metadata/Message.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 19 Oct 2016 15:55:29 +0000" id="863" opendate="Wed, 19 Oct 2016 11:55:53 +0000">
		<buginformation>
			<summary>CommonsDigester calculates wrong hashes on large files</summary>
			<description>&lt;p&gt;When passing more than one algorithm to CommonsDigester constructor and&lt;br/&gt;
then trying to digest a file which is larger than 7.5 MB, results wrong&lt;br/&gt;
hashe calculation for all the algorithms except the first.&lt;/p&gt;

&lt;p&gt;The next code will reproduce the bug:&lt;/p&gt;

&lt;p&gt;// The file that was used w as a simple plain text file with size &amp;gt; 7.5 MB &lt;br/&gt;
File file = new File(&quot;testLargeFile.txt&quot;);&lt;/p&gt;

&lt;p&gt;BufferedInputStream bufferedInputStream = new BufferedInputStream(new FileInputStream(file));&lt;/p&gt;

&lt;p&gt;Metadata metadata = new Metadata();&lt;/p&gt;

&lt;p&gt;CommonsDigester digester = new CommonsDigester(20000000,&lt;br/&gt;
                CommonsDigester.DigestAlgorithm.MD5,&lt;br/&gt;
                CommonsDigester.DigestAlgorithm.SHA1,&lt;br/&gt;
                CommonsDigester.DigestAlgorithm.SHA256);&lt;/p&gt;

&lt;p&gt;digester.digest(bufferedInputStream, metadata, null);&lt;/p&gt;

&lt;p&gt;// Will print correct MD5 but wrong SHA1 and wrong SHA256&lt;br/&gt;
System.out.println(metadata);&lt;/p&gt;

&lt;p&gt;Initial direction: it seems that the inner buffered stream that is being used doesn't reset to 0 position after the first algorithm.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/utils/CommonsDigester.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 19 Oct 2016 23:25:00 +0000" id="864" opendate="Wed, 19 Oct 2016 21:12:24 +0000">
		<buginformation>
			<summary>NullPointerException on a valid PPTX</summary>
			<description>&lt;p&gt;On the attached PPTX file, which opens fine with Powerpoint, the Tika parser throws the following error:&lt;/p&gt;

&lt;p&gt;java.lang.NullPointerException&lt;br/&gt;
	at org.apache.tika.parser.microsoft.ooxml.XSLFPowerPointExtractorDecorator.buildXHTML(XSLFPowerPointExtractorDecorator.java:110)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.ooxml.AbstractOOXMLExtractor.getXHTML(AbstractOOXMLExtractor.java:109)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.ooxml.OOXMLExtractorFactory.parse(OOXMLExtractorFactory.java:112)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.ooxml.OOXMLParser.parse(OOXMLParser.java:87)&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/XSLFPowerPointExtractorDecorator.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 21 Oct 2016 14:38:54 +0000" id="865" opendate="Thu, 20 Oct 2016 14:12:12 +0000">
		<buginformation>
			<summary>TaggedIOException from ZipException on a valid PowerPoint file</summary>
			<description>&lt;p&gt;On the following PPT file, which PowerPoint opens and displays fine:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://dl.dropboxusercontent.com/u/92341073/ny2004b.ppt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://dl.dropboxusercontent.com/u/92341073/ny2004b.ppt&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The Tika parser throws the following exception:&lt;/p&gt;

&lt;p&gt;org.apache.tika.io.TaggedIOException: incorrect data check&lt;br/&gt;
	at org.apache.tika.io.TaggedInputStream.handleIOException(TaggedInputStream.java:133)&lt;br/&gt;
	at org.apache.tika.io.ProxyInputStream.read(ProxyInputStream.java:103)&lt;br/&gt;
	at org.apache.tika.io.ProxyInputStream.read(ProxyInputStream.java:99)&lt;br/&gt;
	at java.io.BufferedInputStream.read1(Unknown Source)&lt;br/&gt;
	at java.io.BufferedInputStream.read(Unknown Source)&lt;br/&gt;
	at java.io.FilterInputStream.read(Unknown Source)&lt;br/&gt;
	at java.nio.file.Files.copy(Unknown Source)&lt;br/&gt;
	at java.nio.file.Files.copy(Unknown Source)&lt;br/&gt;
	at org.apache.tika.io.TikaInputStream.getPath(TikaInputStream.java:587)&lt;br/&gt;
	at org.apache.tika.io.TikaInputStream.getFile(TikaInputStream.java:615)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.POIFSContainerDetector.getTopLevelNames(POIFSContainerDetector.java:377)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.POIFSContainerDetector.detect(POIFSContainerDetector.java:443)&lt;br/&gt;
	at org.apache.tika.detect.CompositeDetector.detect(CompositeDetector.java:77)&lt;br/&gt;
	at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:112)&lt;br/&gt;
	at org.apache.tika.parser.DelegatingParser.parse(DelegatingParser.java:72)&lt;br/&gt;
	at org.apache.tika.extractor.ParsingEmbeddedDocumentExtractor.parseEmbedded(ParsingEmbeddedDocumentExtractor.java:102)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.AbstractPOIFSExtractor.handleEmbeddedResource(AbstractPOIFSExtractor.java:140)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.AbstractPOIFSExtractor.handleEmbeddedResource(AbstractPOIFSExtractor.java:116)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.HSLFExtractor.handleSlideEmbeddedResources(HSLFExtractor.java:368)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.HSLFExtractor.parse(HSLFExtractor.java:138)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.OfficeParser.parse(OfficeParser.java:149)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.OfficeParser.parse(OfficeParser.java:117)&lt;br/&gt;
	at gov.nih.niaid.fscanner.Extract.ExtractContents(Extract.java:62)&lt;br/&gt;
	at gov.nih.niaid.temp.Main.main(Main.java:60)&lt;br/&gt;
Caused by: java.util.zip.ZipException: incorrect data check&lt;br/&gt;
	at java.util.zip.InflaterInputStream.read(Unknown Source)&lt;br/&gt;
	at org.apache.poi.util.BoundedInputStream.read(BoundedInputStream.java:121)&lt;br/&gt;
	at java.io.BufferedInputStream.fill(Unknown Source)&lt;br/&gt;
	at java.io.BufferedInputStream.read1(Unknown Source)&lt;br/&gt;
	at java.io.BufferedInputStream.read(Unknown Source)&lt;br/&gt;
	at org.apache.tika.io.ProxyInputStream.read(ProxyInputStream.java:99)&lt;br/&gt;
	... 22 more&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/HSLFExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 28 Oct 2016 13:48:33 +0000" id="866" opendate="Wed, 26 Oct 2016 20:03:32 +0000">
		<buginformation>
			<summary>NullPointerException on a valid Word file</summary>
			<description>&lt;p&gt;On the attached Word file, which opens fine in Word, the Tika parser throws the following error:&lt;/p&gt;

&lt;p&gt;java.lang.NullPointerException&lt;br/&gt;
	at org.apache.tika.parser.microsoft.ooxml.XWPFWordExtractorDecorator.extractParagraph(XWPFWordExtractorDecorator.java:149)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.ooxml.XWPFWordExtractorDecorator.extractIBodyText(XWPFWordExtractorDecorator.java:107)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.ooxml.XWPFWordExtractorDecorator.buildXHTML(XWPFWordExtractorDecorator.java:93)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.ooxml.AbstractOOXMLExtractor.getXHTML(AbstractOOXMLExtractor.java:109)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.ooxml.OOXMLExtractorFactory.parse(OOXMLExtractorFactory.java:112)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.ooxml.OOXMLParser.parse(OOXMLParser.java:87)&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/ooxml/XWPFWordExtractorDecorator.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 4 Nov 2016 15:41:43 +0000" id="867" opendate="Thu, 3 Nov 2016 20:45:20 +0000">
		<buginformation>
			<summary>HSLFException on a valid Powerpoint file</summary>
			<description>&lt;p&gt;On the attached Powerpoint file, which opens fine with Powerpoint, the Tika parser throws the following error:&lt;/p&gt;

&lt;p&gt;org.apache.poi.hslf.exceptions.HSLFException: java.util.zip.ZipException: incorrect data check&lt;br/&gt;
	at org.apache.poi.hslf.blip.PICT.getData(PICT.java:120)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.HSLFExtractor.handleSlideEmbeddedPictures(HSLFExtractor.java:324)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.HSLFExtractor.parse(HSLFExtractor.java:193)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.OfficeParser.parse(OfficeParser.java:149)&lt;br/&gt;
	at org.apache.tika.parser.microsoft.OfficeParser.parse(OfficeParser.java:117)&lt;br/&gt;
Caused by: java.util.zip.ZipException: incorrect data check&lt;br/&gt;
	at java.util.zip.InflaterInputStream.read(InflaterInputStream.java:164)&lt;br/&gt;
	at java.io.FilterInputStream.read(FilterInputStream.java:107)&lt;br/&gt;
	at org.apache.poi.hslf.blip.PICT.read(PICT.java:133)&lt;br/&gt;
	at org.apache.poi.hslf.blip.PICT.getData(PICT.java:116)&lt;br/&gt;
	... 6 more&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/HSLFExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 10 Nov 2016 14:30:56 +0000" id="868" opendate="Mon, 7 Nov 2016 15:46:14 +0000">
		<buginformation>
			<summary>Tika 1.13 ForkParser fails intermittently with very large MS Word docx</summary>
			<description>&lt;p&gt;If the ForkParser is run in a for-loop over and over against a single large Microsoft Word DOCX file, it fails intermittently. Sometimes it will fail on the very first iteration. Sometimes it will run through several iterations before failing. Results are inconsistent. &lt;/p&gt;

&lt;p&gt;A small test application is enclosed. For the test, I use a Word docx with the full text of &quot;War and Peace&quot;. 2.8MB, 1141 pages of text.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/test/java/org/apache/tika/fork/ForkParserTest.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 2 Feb 2017 19:35:40 +0000" id="869" opendate="Thu, 2 Feb 2017 17:05:02 +0000">
		<buginformation>
			<summary>Include hyperlinks from widget annotations</summary>
			<description>&lt;p&gt;On &lt;a href=&quot;https://issues.apache.org/jira/browse/PDFBOX-3644&quot; title=&quot;PrintURLs example doesn&amp;#39;t detect all URLs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PDFBOX-3644&quot;&gt;&lt;del&gt;PDFBOX-3644&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tilman&quot; class=&quot;user-hover&quot; rel=&quot;tilman&quot;&gt;Tilman Hausherr&lt;/a&gt; recently improved PDFBox's &lt;tt&gt;printURLs&lt;/tt&gt; to extract urls embedded in widget annotations.  We should follow his lead.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/AbstractPDF2XHTML.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 1 Mar 2017 16:20:30 +0000" id="870" opendate="Wed, 1 Mar 2017 10:06:53 +0000">
		<buginformation>
			<summary>Caused by: java.lang.StringIndexOutOfBoundsException - org.apache.tika.parser.microsoft.WordExtractor.buildParagraphTagAndStyle</summary>
			<description>&lt;p&gt;Getting the below error when parsing word DOC&lt;/p&gt;

&lt;p&gt;Caused by: java.lang.StringIndexOutOfBoundsException: String index out of range: 1&lt;br/&gt;
 at java.lang.String.substring(String.java:1963)&lt;br/&gt;
 at org.apache.tika.parser.microsoft.WordExtractor.buildParagraphTagAndStyle(WordExtractor.java:126)&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/WordExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 9 Jun 2017 18:47:21 +0000" id="871" opendate="Thu, 1 Jun 2017 12:54:50 +0000">
		<buginformation>
			<summary>Upgrade forbiddenapis to 2.3</summary>
			<description></description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parent/pom.xml</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 8 Jun 2017 11:43:31 +0000" id="872" opendate="Thu, 8 Jun 2017 11:24:02 +0000">
		<buginformation>
			<summary>Problem in Tika().detect for ODB (Open Office database) files</summary>
			<description>&lt;p&gt;If I submit a .odb file (OpenOffice Database File), Tika detects an &quot;application/vnd.oasis.opendocument.base&quot; mimetype but no extension.&lt;br/&gt;
Valid: true&lt;br/&gt;
Mime: application/vnd.oasis.opendocument.base&lt;br/&gt;
Extensions: []&lt;br/&gt;
Version: null&lt;br/&gt;
Description: &lt;br/&gt;
Detection Method: unknown&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 10 Oct 2011 22:26:45 +0000" id="873" opendate="Thu, 14 Jul 2011 20:15:15 +0000">
		<buginformation>
			<summary>Creative Suite formats support</summary>
			<description>&lt;p&gt;Is it possible to support Creative Suite formats, such as PSD, InDesign, etc.? &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/image/ImageParser.java</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/image/PSDParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 25 May 2015 19:33:20 +0000" id="874" opendate="Sun, 11 Jan 2015 18:24:43 +0000">
		<buginformation>
			<summary>FFMpeg installed but not parsing video files</summary>
			<description>&lt;p&gt;I have FFMPEG installed with homebrew:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;# brew install ffmpeg
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I've got some AVI files and have tried to parse them with Tika:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;[chipotle:~/Desktop/drone-vids] mattmann% tika -m SPOT11_000001\ 17.AVI
Content-Length: 334917340
Content-Type: video/x-msvideo
X-Parsed-By: org.apache.tika.parser.EmptyParser
resourceName: SPOT11_000001 17.AVI
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I took a look at the ExternalParser, which is configured for using ffmpeg if it's installed. It seems it only works on:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-xml&quot;&gt;   &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;mime-types&amp;gt;&lt;/span&gt;
       &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;mime-type&amp;gt;&lt;/span&gt;video/avi&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/mime-type&amp;gt;&lt;/span&gt;
       &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;mime-type&amp;gt;&lt;/span&gt;video/mpeg&lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/mime-type&amp;gt;&lt;/span&gt;
     &lt;span class=&quot;code-tag&quot;&gt;&amp;lt;/mime-types&amp;gt;&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I'll add video/x-msvideo and see if that fixes it. I also stumbled upon the work by &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rgauss&quot; class=&quot;user-hover&quot; rel=&quot;rgauss&quot;&gt;Ray Gauss II&lt;/a&gt; at Github - Ray I noticed there is no parser in that work:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/AlfrescoLabs/tika-ffmpeg&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://github.com/AlfrescoLabs/tika-ffmpeg&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;But there seems to be metadata extraction code, etc. Ray should I do something with this?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/resources/org/apache/tika/parser/external/tika-external-parsers.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 4 May 2015 21:54:45 +0000" id="875" opendate="Thu, 26 Feb 2015 05:20:02 +0000">
		<buginformation>
			<summary>Add examples from the Tika in Action book</summary>
			<description>&lt;p&gt;Manning publications has granted permission for me to contribute the Tika in Action code to Apache TIka. Yay! I'll put it in the examples module and update it if needed.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-example/pom.xml</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 13 Apr 2015 16:25:33 +0000" id="876" opendate="Fri, 10 Apr 2015 09:12:30 +0000">
		<buginformation>
			<summary>Unable to parse ODT files because of failed to close temporary resources</summary>
			<description>&lt;p&gt;Many ODT files are failed to parse causing of this exception. A sample file in attachment&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Apache Tika was unable to parse the document
at C:\Users\hong-thai.nguyen\Downloads\Manuel_koha.odt.

The full exception stack trace is included below:

org.apache.tika.exception.TikaException: Failed to close temporary resources
	at org.apache.tika.io.TemporaryResources.dispose(TemporaryResources.java:152)
	at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:127)
	at org.apache.tika.gui.TikaGUI.handleStream(TikaGUI.java:342)
	at org.apache.tika.gui.TikaGUI.openFile(TikaGUI.java:299)
	at org.apache.tika.gui.TikaGUI.actionPerformed(TikaGUI.java:256)
	at javax.swing.AbstractButton.fireActionPerformed(Unknown Source)
	at javax.swing.AbstractButton$Handler.actionPerformed(Unknown Source)
	at javax.swing.DefaultButtonModel.fireActionPerformed(Unknown Source)
	at javax.swing.DefaultButtonModel.setPressed(Unknown Source)
	at javax.swing.AbstractButton.doClick(Unknown Source)
	at javax.swing.plaf.basic.BasicMenuItemUI.doClick(Unknown Source)
	at javax.swing.plaf.basic.BasicMenuItemUI$Handler.mouseReleased(Unknown Source)
	at java.awt.Component.processMouseEvent(Unknown Source)
	at javax.swing.JComponent.processMouseEvent(Unknown Source)
	at java.awt.Component.processEvent(Unknown Source)
	at java.awt.Container.processEvent(Unknown Source)
	at java.awt.Component.dispatchEventImpl(Unknown Source)
	at java.awt.Container.dispatchEventImpl(Unknown Source)
	at java.awt.Component.dispatchEvent(Unknown Source)
	at java.awt.LightweightDispatcher.retargetMouseEvent(Unknown Source)
	at java.awt.LightweightDispatcher.processMouseEvent(Unknown Source)
	at java.awt.LightweightDispatcher.dispatchEvent(Unknown Source)
	at java.awt.Container.dispatchEventImpl(Unknown Source)
	at java.awt.Window.dispatchEventImpl(Unknown Source)
	at java.awt.Component.dispatchEvent(Unknown Source)
	at java.awt.EventQueue.dispatchEventImpl(Unknown Source)
	at java.awt.EventQueue.access$400(Unknown Source)
	at java.awt.EventQueue$3.run(Unknown Source)
	at java.awt.EventQueue$3.run(Unknown Source)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.security.ProtectionDomain$1.doIntersectionPrivilege(Unknown Source)
	at java.security.ProtectionDomain$1.doIntersectionPrivilege(Unknown Source)
	at java.awt.EventQueue$4.run(Unknown Source)
	at java.awt.EventQueue$4.run(Unknown Source)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.security.ProtectionDomain$1.doIntersectionPrivilege(Unknown Source)
	at java.awt.EventQueue.dispatchEvent(Unknown Source)
	at java.awt.EventDispatchThread.pumpOneEventForFilters(Unknown Source)
	at java.awt.EventDispatchThread.pumpEventsForFilter(Unknown Source)
	at java.awt.EventDispatchThread.pumpEventsForHierarchy(Unknown Source)
	at java.awt.EventDispatchThread.pumpEvents(Unknown Source)
	at java.awt.EventDispatchThread.pumpEvents(Unknown Source)
	at java.awt.EventDispatchThread.run(Unknown Source)
Caused by: java.io.IOException: Could not delete temporary file C:\Users\HONG-T~1.NGU\AppData\Local\Temp\apache-tika-2891340188156641845.tmp
	at org.apache.tika.io.TemporaryResources$1.close(TemporaryResources.java:70)
	at org.apache.tika.io.TemporaryResources.close(TemporaryResources.java:121)
	at org.apache.tika.io.TemporaryResources.dispose(TemporaryResources.java:150)
	... 42 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/odf/NSNormalizerContentHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 30 Jun 2015 00:53:25 +0000" id="877" opendate="Fri, 10 Apr 2015 12:26:59 +0000">
		<buginformation>
			<summary>Integrate Jackcess to handle MSAccess files</summary>
			<description>&lt;p&gt;Recently, James Ahlborn, the current maintainer of &lt;a href=&quot;http://jackcess.sourceforge.net/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;Jackcess&lt;/a&gt;, kindly agreed to relicense Jackcess to Apache 2.0.  &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=boneill&quot; class=&quot;user-hover&quot; rel=&quot;boneill&quot;&gt;Brian ONeill&lt;/a&gt;, the CTO at &lt;a href=&quot;https://www.healthmarketscience.com/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;Health Market Science, a LexisNexis® Company&lt;/a&gt;, also agreed with this relicensing and led the charge to obtain all necessary corporate approval to deliver a &lt;a href=&quot;https://www.apache.org/licenses/cla-corporate.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;CCLA&lt;/a&gt; for Jackcess to Apache.  As anyone who has tried to get corporate approval for anything knows, this can sometimes require not a small bit of effort.&lt;/p&gt;

&lt;p&gt;If I may speak on behalf of Tika and the larger Apache community, I offer a sincere thanks to James, Brian and the other developers and contributors to Jackcess!!!&lt;/p&gt;

&lt;p&gt;Once the licensing info has been changed in Jackcess and the new release is available in maven, we can integrate Jackcess into Tika and add a capability to process MSAccess.&lt;/p&gt;

&lt;p&gt;As a side note, I reached out to the developers and contributors to determine if there were any objections.  I couldn't find addresses for everyone, and not everyone replied, but those who did offered their support to this move. &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 14 Apr 2015 10:57:58 +0000" id="878" opendate="Tue, 14 Apr 2015 10:53:41 +0000">
		<buginformation>
			<summary>Fix potential NPEs in Throwable.getMessage().XYZ()</summary>
			<description>&lt;p&gt;There are a handful of places in our codebase where we assume that getMessage() will return a non-null value.  We need to fix these to check for nulls.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/parser/RecursiveParserWrapper.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 21 Apr 2015 17:30:41 +0000" id="879" opendate="Tue, 21 Apr 2015 14:11:04 +0000">
		<buginformation>
			<summary>Allow RecursiveParserWrapper to catch exceptions from embedded documents</summary>
			<description>&lt;p&gt;While parsing embedded documents, currently, if a parser hits an EncryptedDocumentException or anything wrapped in a TikaException, the Exception is swallowed by &lt;tt&gt;ParsingEmbeddedDocumentExtractor&lt;/tt&gt;:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;            DELEGATING_PARSER.parse(
                                    newStream,
                                    new EmbeddedContentHandler(new BodyContentHandler(handler)),
                                    metadata, context);
        } catch (EncryptedDocumentException ede) {
            // TODO: can we log a warning that we lack the password?
            // For now, just skip the content
        } catch (TikaException e) {
            // TODO: can we log a warning somehow?
            // Could not parse the entry, just skip the content
        } finally {
            tmp.close();
        }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;For some applications, it might be better to store the stack trace of the attachment that caused an exception.&lt;/p&gt;

&lt;p&gt;The proposal would be to include the stack trace in the metadata object for that particular attachment.&lt;/p&gt;

&lt;p&gt;The user will be able to specify whether or not to store stack traces, and the default will be to store stack traces.  This will be a small change to the legacy behavior.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-batch/src/main/java/org/apache/tika/batch/fs/RecursiveParserWrapperFSConsumer.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 7 May 2015 06:48:15 +0000" id="880" opendate="Tue, 5 May 2015 07:47:07 +0000">
		<buginformation>
			<summary>Expose Translation Interface from Tika Server</summary>
			<description>&lt;p&gt;Tika Server should expose the Tika Translate API. This will allow for easy use in downstream clients such as &lt;a href=&quot;http://github.com/chrismattmann/tika-python&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;Tika-Python&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-server/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 12 May 2015 21:04:31 +0000" id="881" opendate="Tue, 12 May 2015 12:51:48 +0000">
		<buginformation>
			<summary>ExternalParser.check should return false if it hits SecurityException</summary>
			<description>&lt;p&gt;If you run Tika with a Java security manager that blocks execution of external processes, ExternalParser.check throws SecurityException, but I think it should just return false?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 13 May 2015 13:50:08 +0000" id="882" opendate="Wed, 13 May 2015 09:53:06 +0000">
		<buginformation>
			<summary>Chaotic eol-style in Java source files</summary>
			<description>&lt;p&gt;Some Java source files contain mixed CRLF and LF line breaks.&lt;/p&gt;

&lt;p&gt;This causes trouble in our tool chain, but may also introduce unnecessary noise when committers use editors that normalize line breaks.&lt;/p&gt;

&lt;p&gt;Possible solution: Require a specific eol-style for Java source files.&lt;/p&gt;

&lt;p&gt;Example:&lt;br/&gt;
&lt;a href=&quot;http://svn.apache.org/repos/asf/tika/tags/1.8/tika-batch/src/main/java/org/apache/tika/batch/ParallelFileProcessingResult.java&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://svn.apache.org/repos/asf/tika/tags/1.8/tika-batch/src/main/java/org/apache/tika/batch/ParallelFileProcessingResult.java&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/main/java/org/apache/tika/cli/BatchCommandLineBuilder.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 23 May 2015 12:22:37 +0000" id="883" opendate="Wed, 20 May 2015 07:51:30 +0000">
		<buginformation>
			<summary>ZLIB magic detection support</summary>
			<description>&lt;p&gt;In our environment we encounter many compressed streams, one of them (which is currently not supported by Tika 1.8) is ZLIB. According to my sources and experience the magics that cover majority of ZLIB archives are these:&lt;/p&gt;

&lt;p&gt;    &amp;lt;mime-type type=&quot;application/zlib&quot;&amp;gt;&lt;br/&gt;
        &amp;lt;_comment&amp;gt;Zlib Compressed Archive&amp;lt;/_comment&amp;gt;&lt;br/&gt;
        &amp;lt;magic priority=&quot;45&quot;&amp;gt;&lt;br/&gt;
            &amp;lt;match value=&quot;\x78\x01&quot; type=&quot;string&quot; offset=&quot;0&quot; /&amp;gt;&lt;br/&gt;
            &amp;lt;match value=&quot;\x78\x9c&quot; type=&quot;string&quot; offset=&quot;0&quot; /&amp;gt;&lt;br/&gt;
            &amp;lt;match value=&quot;\x78\xda&quot; type=&quot;string&quot; offset=&quot;0&quot; /&amp;gt;&lt;br/&gt;
        &amp;lt;/magic&amp;gt;&lt;br/&gt;
    &amp;lt;/mime-type&amp;gt;&lt;/p&gt;

&lt;p&gt;Well described here:&lt;br/&gt;
&lt;a href=&quot;http://stackoverflow.com/questions/9050260/what-does-a-zlib-header-look-like&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://stackoverflow.com/questions/9050260/what-does-a-zlib-header-look-like&lt;/a&gt;&lt;br/&gt;
Original RFC here:&lt;br/&gt;
&lt;a href=&quot;http://tools.ietf.org/html/rfc1950&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://tools.ietf.org/html/rfc1950&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 4 Jun 2015 05:31:07 +0000" id="884" opendate="Thu, 21 May 2015 22:39:11 +0000">
		<buginformation>
			<summary>Detecting problem with Matlab source code</summary>
			<description>&lt;p&gt;Both Matlab source code and Objective-C source code have the same suffix, which is .m. Therefore, Matlab has additional match value in mime types.xml. &lt;/p&gt;

&lt;p&gt;In tika-mimetypes.xml Matlab is defined as:&lt;/p&gt;

&lt;p&gt;  &amp;lt;mime-type type=&quot;text/x-matlab&quot;&amp;gt;&lt;br/&gt;
    &amp;lt;_comment&amp;gt;Matlab source code&amp;lt;/_comment&amp;gt;&lt;br/&gt;
    &amp;lt;magic priority=&quot;50&quot;&amp;gt;&lt;br/&gt;
      &amp;lt;match value=&quot;function [&quot; type=&quot;string&quot; offset=&quot;0&quot;/&amp;gt;&lt;br/&gt;
    &amp;lt;/magic&amp;gt;&lt;br/&gt;
    &amp;lt;!-- &amp;lt;glob pattern=&quot;*.m&quot;/&amp;gt; - conflicts with text/x-objcsrc --&amp;gt;&lt;br/&gt;
    &amp;lt;sub-class-of type=&quot;text/plain&quot;/&amp;gt;&lt;br/&gt;
  &amp;lt;/mime-type&amp;gt;&lt;/p&gt;

&lt;p&gt;However, Matlab codes does not always start with &quot;function [“. Therefore, some Matlab codes are detected as text/x-bojcsrc. Based on the source codes collected from NOAA Paleoclimatology Software Resources, many Matlab codes have match value like these (problematic files are attached as an example):&lt;/p&gt;

&lt;p&gt;&amp;lt;mime-type type=&quot;text/x-matlab&quot;&amp;gt;&lt;br/&gt;
    &amp;lt;_comment&amp;gt;Matlab source code&amp;lt;/_comment&amp;gt;&lt;br/&gt;
    &amp;lt;magic priority=&quot;50&quot;&amp;gt;&lt;br/&gt;
      &amp;lt;match value=&quot;function&quot; type=&quot;string&quot; offset=&quot;0&quot;/&amp;gt;&lt;br/&gt;
      &amp;lt;match value=&quot;%&quot; type=&quot;string&quot; offset=&quot;0&quot;/&amp;gt;&lt;br/&gt;
    &amp;lt;/magic&amp;gt;&lt;br/&gt;
    &amp;lt;!-- &amp;lt;glob pattern=&quot;*.m&quot;/&amp;gt; - conflicts with text/x-objcsrc --&amp;gt;&lt;br/&gt;
    &amp;lt;sub-class-of type=&quot;text/plain&quot;/&amp;gt;&lt;br/&gt;
  &amp;lt;/mime-type&amp;gt;&lt;/p&gt;

&lt;p&gt;Conducted several detecting tests by using different Matlab packages obtained from NOAA Paleoclimatology Software Resources, with/without Custom-mimtypes.xml. Results are attached. As a results, total 103 Matlab files are detected correctly with custom-mimetypes.xml, while  42 Matlab files are detected as Matlab files without custom-mimetypes.xml (= only with current match value). However, this match value for Matlab source code could be only common in Paleoclimatology community. &lt;/p&gt;
			</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/resources/test-documents/testMATLAB.m</file>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 4 Jun 2015 05:31:07 +0000" id="885" opendate="Sat, 23 May 2015 12:19:27 +0000">
		<buginformation>
			<summary>Update CompressorParser to handle zlib/deflate</summary>
			<description>&lt;p&gt;Thanks to &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1632&quot; title=&quot;ZLIB magic detection support&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1632&quot;&gt;&lt;del&gt;TIKA-1632&lt;/del&gt;&lt;/a&gt;, we can now detect zlib / deflate files which have a zlib header&lt;/p&gt;

&lt;p&gt;Commons Compress also supports these files, so we should add support to CompressorParser to call out to this for zlib files&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pkg/CompressorParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 29 May 2015 14:38:16 +0000" id="886" opendate="Fri, 29 May 2015 14:34:48 +0000">
		<buginformation>
			<summary>Clean up code in tika-parsers</summary>
			<description>&lt;p&gt;I'd like to clean up some code in tika-parsers, esp my spacing/indent issues. Many apologies for the noise.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-server/pom.xml</file>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/html/BoilerpipeContentHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 3 Jun 2015 17:56:23 +0000" id="887" opendate="Wed, 3 Jun 2015 17:19:24 +0000">
		<buginformation>
			<summary>RecursiveParserWrapper should add container metadata to list even if exception is hit</summary>
			<description>&lt;p&gt;Thanks to &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gagravarr&quot; class=&quot;user-hover&quot; rel=&quot;gagravarr&quot;&gt;Nick Burch&lt;/a&gt; for finding this on &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1644&quot; title=&quot;Mime type diffs between 1.8 and 1.9-rc1&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1644&quot;&gt;&lt;del&gt;TIKA-1644&lt;/del&gt;&lt;/a&gt;.  If a runtime exception is hit while parsing the container file, that metadata object is not added to the list. Will fix shortly.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/test/java/org/apache/tika/parser/mock/MockParser.java</file>
			<file>/tika-batch/src/main/java/org/apache/tika/batch/fs/AbstractFSConsumer.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 28 Jun 2015 01:58:16 +0000" id="888" opendate="Wed, 24 Jun 2015 20:33:28 +0000">
		<buginformation>
			<summary>Add a DigestingParser to add MD5/SHA-X hashes as fields in Metadata</summary>
			<description>&lt;p&gt;It might be useful to integrate commons' DigestUtils and allow users to easily add the MD5 or other supported hashes to the Metadata object.&lt;/p&gt;

&lt;p&gt;Anyone else find this of use?&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 3 Feb 2016 14:14:37 +0000" id="889" opendate="Wed, 30 Dec 2015 09:51:46 +0000">
		<buginformation>
			<summary>Problem in Tika().detect for xml file signed in CADES</summary>
			<description>&lt;p&gt;We have a xml file with base64 attachment signed with CADES signature. &lt;br/&gt;
In this case TIKA recognize the resulted file mime type as &quot;text/plain&quot; and not &quot;application/pkcs7-signature&quot; as we expected.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/resources/org/apache/tika/mime/tika-mimetypes.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 1 Aug 2015 10:20:03 +0000" id="890" opendate="Wed, 12 Feb 2014 15:59:20 +0000">
		<buginformation>
			<summary>Update OutlookExtractor to handle codepage identification more rigorously</summary>
			<description>&lt;p&gt;Since OutlookExtractor's codepage detection chunk was written, POI's HSMF has added more robutst capabilities for identifying codepages in Outlook .msg files.  As a first step to integrating those improvements, I'll copy and paste some of POI's code into OutlookExtractor.  As a second step, I'll expose more of HSMF's capabilities within POI and then factor out the duplicate code in Tika.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 23 Jul 2015 01:09:20 +0000" id="891" opendate="Tue, 31 Mar 2015 01:22:22 +0000">
		<buginformation>
			<summary>Upgrade to PDFBox 1.8.10 when available</summary>
			<description>&lt;p&gt;Let's use this ticket to discuss/prepare for the release and integration of PDFBox 1.8.10 when it is available.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Mon, 8 Jun 2015 14:43:14 +0000" id="892" opendate="Mon, 8 Jun 2015 14:12:45 +0000">
		<buginformation>
			<summary>Tika config xml shouldn't read nested parser definitions as top level</summary>
			<description>&lt;p&gt;Spotted while looking at &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1642&quot; title=&quot;Integrate cTAKES into Tika&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1642&quot;&gt;&lt;del&gt;TIKA-1642&lt;/del&gt;&lt;/a&gt;, if you have some Tika config xml like:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&amp;lt;properties&amp;gt;
  &amp;lt;parsers&amp;gt;
    &amp;lt;parser class=&lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.tika.parser.ctakes.CTAKESParser&quot;&lt;/span&gt;&amp;gt;
       &amp;lt;parser class=&lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.tika.parser.DefaultParser&quot;&lt;/span&gt;/&amp;gt;
    &amp;lt;/parser&amp;gt;
  &amp;lt;/parsers&amp;gt;
&amp;lt;/properties&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then because of the way that TikaConfig is fetching the elements, it will process the DefaultParser once as a child of CTakes, then a second time at the top level&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/config/TikaConfig.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sun, 21 Jun 2015 23:31:02 +0000" id="893" opendate="Wed, 10 Jun 2015 23:26:29 +0000">
		<buginformation>
			<summary>Reset cTAKES CAS into CTAKESParser</summary>
			<description>&lt;p&gt;Using &lt;a href=&quot;https://wiki.apache.org/tika/cTAKESParser&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;CTAKESParser from Tika Server&lt;/a&gt;, I noticed that an exception occurs when the CTAKESParser is used multiple times:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.uima.cas.CASRuntimeException: Data for Sofa feature setLocalSofaData() has already been set.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is due to the CAS (Common Analysis System) used by CTAKESParser. The CAS, as the AE (AnalysisEngine), is a static field into CTAKESParser to make a sort of singleton.&lt;/p&gt;

&lt;p&gt;By the way, An Analysis Engine is a cTAKES/UIMA component responsible for analyzing unstructured information, discovering and representing semantic content. An AnalysisEngine operates on an &quot;analysis structure&quot; (implemented by CAS).&lt;/p&gt;

&lt;p&gt;It is highly recommended to reuse the CAS, but it has to be reset before the next run. The CTAKESUtils class (&lt;tt&gt;org.apache.tika.parser.ctakes&lt;/tt&gt;) provides the reset method to release all resources held by both AnalysisEngine and CAS and then &quot;destroy&quot; them. This method prevents the CASRuntimeException error.&lt;/p&gt;

&lt;p&gt;You can find in attachment the patch including two new methods (resetCAS and resetAE) to reset, but not to destroy, the CAS and the AnalysisEngine respectively.&lt;br/&gt;
By using only resetCAS, CTAKESParser can reuse both CAS and AE instead of building them again for each run.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/ctakes/CTAKESContentHandler.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 18 Jun 2015 20:13:35 +0000" id="894" opendate="Thu, 18 Jun 2015 19:57:04 +0000">
		<buginformation>
			<summary>Mimetypes entry for Java .properties files</summary>
			<description>&lt;p&gt;As pointed out on Stack Overflow today, Tika doesn't have a dedicated mime type entry for Java Properties files, and the glob is taken by the generic text one. We should add an entry, along with a test&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/java/org/apache/tika/mime/TestMimeTypes.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 23 Jul 2015 17:27:47 +0000" id="895" opendate="Fri, 26 Jun 2015 11:59:33 +0000">
		<buginformation>
			<summary>Upgrade to POI 3.13-beta1 when available</summary>
			<description>&lt;p&gt;Upgrade to POI 3.13-beta1 when available.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 9 Jul 2015 13:02:58 +0000" id="896" opendate="Thu, 9 Jul 2015 13:00:44 +0000">
		<buginformation>
			<summary>Fix logic error in batch driver that prevents correct restarting of child process</summary>
			<description>&lt;p&gt;Thanks to work on &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1285&quot; title=&quot;Upgrade to PDFBox 2.0.0 when available&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1285&quot;&gt;&lt;del&gt;TIKA-1285&lt;/del&gt;&lt;/a&gt;, I discovered a logic bug in the driver process that prevents correct restarting of the child process.  This should only happen under very heavy load, but this needs to be fixed asap.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-batch/src/main/java/org/apache/tika/batch/BatchProcessDriverCLI.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 24 Jul 2015 14:08:19 +0000" id="897" opendate="Tue, 14 Jul 2015 15:26:31 +0000">
		<buginformation>
			<summary>PDF metadata extraction fails to spot UTF-16 encoded title</summary>
			<description>&lt;p&gt;When extracting metadata from PDFs, we see some odd behaviour for a minority of the documents. The PDF metadata can be encoded as UTF-18 octets, but is not always being decoded as such.&lt;/p&gt;

&lt;p&gt;A specific example is here: &lt;a href=&quot;http://mqug.org.uk/downloads/201207/201207%20-%20TEC02%20-%20Introduction%20to%20Worklight.pdf&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://mqug.org.uk/downloads/201207/201207%20-%20TEC02%20-%20Introduction%20to%20Worklight.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Which contains this (literal file content):&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;443 0 obj
&amp;lt;&amp;lt;/Type/Metadata
/Subtype/XML/Length 1978&amp;gt;&amp;gt;stream
&amp;lt;?xpacket begin='ï»¿' id='W5M0MpCehiHzreSzNTczkc9d'?&amp;gt;
&amp;lt;?adobe-xap-filters esc=&quot;CRLF&quot;?&amp;gt;
&amp;lt;x:xmpmeta xmlns:x='adobe:ns:meta/' x:xmptk='XMP toolkit 2.9.1-13, framework 1.6'&amp;gt;
&amp;lt;rdf:RDF xmlns:rdf='http://www.w3.org/1999/02/22-rdf-syntax-ns#' xmlns:iX='http://ns.adobe.com/iX/1.0/'&amp;gt;
&amp;lt;rdf:Description rdf:about='ac9f232e-d341-11e1-0000-ba905bfc4694' xmlns:pdf='http://ns.adobe.com/pdf/1.3/' pdf:Producer='\376\377\000B\000u\000l\000l\000z\000i\000p\000 \000P\000D\000F\000 \000P\000r\000i\000n\000t\000e\000r\000 \000/\000 \000w\000w\000w\000.\000b\000u\000l\000l\000z\000i\000p\000.\000c\000o\000m\000 \000/\000 \000F\000r\000e\000e\000w\000a\000r\000e\000 \000E\000d\000i\000t\000i\000o\000n'/&amp;gt;
&amp;lt;rdf:Description rdf:about='ac9f232e-d341-11e1-0000-ba905bfc4694' xmlns:xmp='http://ns.adobe.com/xap/1.0/'&amp;gt;&amp;lt;xmp:ModifyDate&amp;gt;2012-07-18T15:38:01+01:00&amp;lt;/xmp:ModifyDate&amp;gt;
&amp;lt;xmp:CreateDate&amp;gt;2012-07-18T15:38:01+01:00&amp;lt;/xmp:CreateDate&amp;gt;
&amp;lt;xmp:CreatorTool&amp;gt;UnknownApplication&amp;lt;/xmp:CreatorTool&amp;gt;&amp;lt;/rdf:Description&amp;gt;
&amp;lt;rdf:Description rdf:about='ac9f232e-d341-11e1-0000-ba905bfc4694' xmlns:xapMM='http://ns.adobe.com/xap/1.0/mm/' xapMM:DocumentID='ac9f232e-d341-11e1-0000-ba905bfc4694'/&amp;gt;
&amp;lt;rdf:Description rdf:about='ac9f232e-d341-11e1-0000-ba905bfc4694' xmlns:dc='http://purl.org/dc/elements/1.1/' dc:format='application/pdf'&amp;gt;&amp;lt;dc:title&amp;gt;&amp;lt;rdf:Alt&amp;gt;&amp;lt;rdf:li xml:lang='x-default'&amp;gt;\376\377\000M\000i\000c\000r\000o\000s\000o\000f\000t\000 \000P\000o\000w\000e\000r\000P\000o\000i\000n\000t\000 \000-\000 \000I\000n\000t\000r\000o\000d\000u\000c\000t\000i\000o\000n\000 \000t\000o\000 \000W\000o\000r\000k\000l\000i\000g\000h\000t\000 \000\(\000S\000R\000D\000\)\000.\000p\000p\000t\000x&amp;lt;/rdf:li&amp;gt;&amp;lt;/rdf:Alt&amp;gt;&amp;lt;/dc:title&amp;gt;&amp;lt;dc:creator&amp;gt;&amp;lt;rdf:Seq&amp;gt;&amp;lt;rdf:li&amp;gt;\376\377\000T\000e\000t\000t\000i&amp;lt;/rdf:li&amp;gt;&amp;lt;/rdf:Seq&amp;gt;&amp;lt;/dc:creator&amp;gt;&amp;lt;/rdf:Description&amp;gt;
&amp;lt;/rdf:RDF&amp;gt;
&amp;lt;/x:xmpmeta&amp;gt;


&amp;lt;?xpacket end='w'?&amp;gt;
endstream
endobj
2 0 obj
&amp;lt;&amp;lt;/Producer(\376\377\000B\000u\000l\000l\000z\000i\000p\000 \000P\000D\000F\000 \000P\000r\000i\000n\000t\000e\000r\000 \000/\000 \000w\000w\000w\000.\000b\000u\000l\000l\000z\000i\000p\000.\000c\000o\000m\000 \000/\000 \000F\000r\000e\000e\000w\000a\000r\000e\000 \000E\000d\000i\000t\000i\000o\000n)
/CreationDate(D:20120718153801+01'00')
/ModDate(D:20120718153801+01'00')
/Title(\376\377\000M\000i\000c\000r\000o\000s\000o\000f\000t\000 \000P\000o\000w\000e\000r\000P\000o\000i\000n\000t\000 \000-\000 \000I\000n\000t\000r\000o\000d\000u\000c\000t\000i\000o\000n\000 \000t\000o\000 \000W\000o\000r\000k\000l\000i\000g\000h\000t\000 \000\(\000S\000R\000D\000\)\000.\000p\000p\000t\000x)
/Author(\376\377\000T\000e\000t\000t\000i)&amp;gt;&amp;gt;endobj
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt; 

&lt;p&gt;Presumably, embedding these UTF-16 octet sequences in the XMP RDF is an error, but the ones encoded in the actual PDF metadata fields should be extracted accurately.&lt;/p&gt;

&lt;p&gt;When extracted, we get:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;...
dc:title: \376\377\000M\000i\000c\000r\000o\000s\000o\000f\000t\000 \000P\000o\000w\000e\000r\000P\000o\000i\000n\000t\000 \000-\000 \000I\000n\000t\000r\000o\000d\000u\000c\000t\000i\000o\000n\000 \000t\000o\000 \000W\000o\000r\000k\000l\000i\000g\000h\000t\000 \000\(\000S\000R\000D\000\)\000.\000p\000p\000t\000x
title: \376\377\000M\000i\000c\000r\000o\000s\000o\000f\000t\000 \000P\000o\000w\000e\000r\000P\000o\000i\000n\000t\000 \000-\000 \000I\000n\000t\000r\000o\000d\000u\000c\000t\000i\000o\000n\000 \000t\000o\000 \000W\000o\000r\000k\000l\000i\000g\000h\000t\000 \000\(\000S\000R\000D\000\)\000.\000p\000p\000t\000x
meta:author: \376\377\000T\000e\000t\000t\000i
meta:author: Tetti
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So, the author appears to be decoded correctly once, but the title is not. Is the XML dc:title being used to override the PDF title field? Or is one of the title fields being decoded incorrectly?&lt;/p&gt;

&lt;p&gt;(I accept that although this is a real PDF document from the web, it is also a malformed one, so maybe there is not much to be done here.)&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/pdf/PDFEncodedStringDecoder.java</file>
			<file>/tika-parsers/src/main/java/org/apache/pdfbox/pdfparser/PDFOctalUnicodeDecoder.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 16 Jul 2015 00:27:35 +0000" id="898" opendate="Wed, 15 Jul 2015 15:08:57 +0000">
		<buginformation>
			<summary>Fix file opening in Jackcess to enable read only for v1997 files</summary>
			<description>&lt;p&gt;We need to make a small modification in how we're opening mdb files with Jackcess to set readonly correctly.  Once we do this, we'll be able to read v1997 files without an exception.&lt;/p&gt;

&lt;p&gt;See: &lt;a href=&quot;https://sourceforge.net/p/jackcess/discussion/456474/thread/038878e6/#d233&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://sourceforge.net/p/jackcess/discussion/456474/thread/038878e6/#d233&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/JackcessParser.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Tue, 21 Jul 2015 12:46:30 +0000" id="899" opendate="Wed, 15 Jul 2015 19:28:28 +0000">
		<buginformation>
			<summary>Add encryption support to Jackcess parser</summary>
			<description>&lt;p&gt;The initial Jackcess parser integration did not include handling of password protected files.  This issue will track adding that into our wrapper.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/test/resources/test-documents/testAccess2_encrypted.accdb</file>
			<file>/tika-parsers/pom.xml</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 16 Jul 2015 00:55:52 +0000" id="900" opendate="Thu, 16 Jul 2015 00:28:38 +0000">
		<buginformation>
			<summary>Clean up metadata properties in Jackcess parser</summary>
			<description>&lt;p&gt;Add common dc properties: author, title, etc as dc properties.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-parsers/src/main/java/org/apache/tika/parser/microsoft/JackcessExtractor.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 16 Jul 2015 01:17:32 +0000" id="901" opendate="Thu, 16 Jul 2015 01:08:00 +0000">
		<buginformation>
			<summary>Clean up some deprecated components</summary>
			<description>&lt;p&gt;gson's JsonNull and junit.framework are easy.  Let's get rid of these.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-app/src/test/java/org/apache/tika/cli/TikaCLIBatchCommandLineTest.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 16 Jul 2015 01:53:57 +0000" id="902" opendate="Thu, 16 Jul 2015 01:51:14 +0000">
		<buginformation>
			<summary>Upgrade xerial.org's sqlite-jdbc to 3.8.10.1</summary>
			<description>&lt;p&gt;Same as title&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Fri, 24 Jul 2015 18:25:29 +0000" id="903" opendate="Thu, 16 Jul 2015 20:35:05 +0000">
		<buginformation>
			<summary>Parser sort order change in TIKA-1517 breaks parser override capability</summary>
			<description>&lt;p&gt;In Tika 1.9, the comparator used to sort parsers (in ServiceLoaderUtils) now returns them in the reverse order from how they were returned in prior versions, when the comparator was in DefaultParser.  This work was done under &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1517&quot; title=&quot;MIME type selection with probability&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1517&quot;&gt;&lt;del&gt;TIKA-1517&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This change broke one of our customizations in which we use our own parser instead of Tika's HtmlParser to process html.  We use the service loader logic (creating our own META-INF/services/org.apache.tika.parser.Parser file) and rely on the order in which the list returned by DefaultParser.getDefaultParsers() is evaluated.    Expecting that when Tika builds the map of mime types to parsers it first puts in entries for HtmlParser, then overwrites these with our custom parser.  &lt;/p&gt;

&lt;p&gt;I realize relying on this is brittle.  And I found a valid workaround to the problem in Tika 1.9 is to blacklist HtmlParser.  However, in case this parser ordering change was not intentional, I figured I'd mention it.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/utils/ServiceLoaderUtils.java</file>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Thu, 23 Jul 2015 13:02:26 +0000" id="904" opendate="Fri, 17 Jul 2015 00:11:42 +0000">
		<buginformation>
			<summary>Inconsistent (buggy) behavior when using tika-server </summary>
			<description>&lt;p&gt;I am using Tika trunk (1.10-SNAPSHOT) and posting documents there. An example would be the following:&lt;/p&gt;

&lt;p&gt;curl -T MOD09GA.A2014010.h30v12.005.2014012183944.vegetation_fraction.tif  &lt;a href=&quot;http://localhost:9998/meta&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://localhost:9998/meta&lt;/a&gt; --header &quot;Accept: application/json”&lt;br/&gt;
…&lt;br/&gt;
curl -T MOD09GA.A2014010.h30v12.005.2014012183944.vegetation_fraction.tif  &lt;a href=&quot;http://localhost:9998/meta&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://localhost:9998/meta&lt;/a&gt; --header &quot;Accept: application/rdf+xml”&lt;br/&gt;
…&lt;br/&gt;
curl -T MOD09GA.A2014010.h30v12.005.2014012183944.vegetation_fraction.tif  &lt;a href=&quot;http://localhost:9998/meta&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://localhost:9998/meta&lt;/a&gt; --header &quot;Accept: text/csv”&lt;/p&gt;

&lt;p&gt;I am using a python script to iterate through all the files in a folder. It works for about 50% to 80% of the files. For the rest it gives an error 500. When I post a file individually for which it previously failed (using the python script) it sometimes works. When done in an ad hoc manner, it works most of the time but fails sometimes. At times it is successful for application/rdf+xml format but fails for application/json format. The behavior is inconsistent.&lt;/p&gt;

&lt;p&gt;Here is an example trace of when it does not work as expected &lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;&lt;br/&gt;
A sample of the data being used can be found here &lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;&lt;br/&gt;
Any help would be appreciated. &lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt; &lt;a href=&quot;https://paste.apache.org/lbAm&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://paste.apache.org/lbAm&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; &lt;a href=&quot;https://drive.google.com/file/d/0B6wmo4_-H0P2eWJjdTdtYS1HRGs/view?usp=sharing&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://drive.google.com/file/d/0B6wmo4_-H0P2eWJjdTdtYS1HRGs/view?usp=sharing&lt;/a&gt;&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-server/src/main/java/org/apache/tika/server/resource/MetadataResource.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 22 Jul 2015 14:31:01 +0000" id="905" opendate="Tue, 21 Jul 2015 18:54:31 +0000">
		<buginformation>
			<summary>Enable getExtension() for mime strings with parameters</summary>
			<description>&lt;p&gt;&lt;tt&gt;getExtension()&lt;/tt&gt; offers a handy way to add a &quot;detected&quot; extension from a &lt;tt&gt;MimeType&lt;/tt&gt; for a file that didn't come with an extension.  However, this functionality doesn't work with texty files: html, xml, css, csv, etc.  &lt;/p&gt;

&lt;p&gt;Let's add a static helper class (or build it into &lt;tt&gt;MimeType&lt;/tt&gt;?) that will output an extension for all mime types including texty mime types. &lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Sat, 1 Aug 2015 17:55:47 +0000" id="906" opendate="Sat, 1 Aug 2015 12:51:26 +0000">
		<buginformation>
			<summary>Tika config xml support for detectors</summary>
			<description>&lt;p&gt;Currently, you can use the Tika Config XML to have very fine-grained control over what parsers to use, in what order, for what mimetypes etc.&lt;/p&gt;

&lt;p&gt;While the same decoration needs won't apply/be appropriate for detectors, the ordering part / composites part / excluding part does. We should therefore add similar support for detectors for those areas&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/config/TikaConfig.java</file>
			<file>/tika-core/src/main/java/org/apache/tika/detect/CompositeDetector.java</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 30 Sep 2015 16:53:07 +0000" id="907" opendate="Fri, 14 Aug 2015 00:22:56 +0000">
		<buginformation>
			<summary>Upgrade to Apache POI 3.13 Beta 2</summary>
			<description>&lt;p&gt;In the not so far future, POI 3.13 Beta 2 will be available.&lt;br/&gt;
This contains a quite big change to the Powerpoint modules XSLF/HSLF, but thankfully TIKA isn't much affected.&lt;br/&gt;
Please try the patch on our trunk and post side-effects.&lt;/p&gt;

&lt;p&gt;As the work on the common_sl api hasn't been finished yet, there might be another patch for the next POI beta version.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/CHANGES.txt</file>
		</fixedFiles>
	</bug>
	<bug fixdate="Wed, 23 Mar 2016 20:03:01 +0000" id="908" opendate="Mon, 21 Mar 2016 18:33:49 +0000">
		<buginformation>
			<summary>ExternalParser No Longer Supports Commands in Array Format</summary>
			<description>&lt;p&gt;After the changes in &lt;a href=&quot;https://issues.apache.org/jira/browse/TIKA-1638&quot; title=&quot;Make ExternalParser actually work&quot; class=&quot;issue-link&quot; data-issue-key=&quot;TIKA-1638&quot;&gt;&lt;del&gt;TIKA-1638&lt;/del&gt;&lt;/a&gt; the ExternalParser now ignores commands specified as a string array and assumes commands will be in a single string with a space delimiter.&lt;/p&gt;

&lt;p&gt;Both formats should be supported.&lt;/p&gt;</description>
		</buginformation>
		<fixedFiles>
			<file>/tika-core/src/main/java/org/apache/tika/parser/external/ExternalParser.java</file>
		</fixedFiles>
	</bug>
</bugrepository>
